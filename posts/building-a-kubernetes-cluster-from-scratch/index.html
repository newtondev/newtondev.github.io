<!DOCTYPE html><html lang="en" data-mode="dark" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="google-adsense-account" content="ca-pub-5206656295129943"><meta name="generator" content="Jekyll v4.2.2" /><meta property="og:title" content="Building a Kubernetes cluster on AWS from scratch" /><meta property="og:locale" content="en" /><meta name="description" content="A place where I can share my experiences with all things Kubernetes and Technology related." /><meta property="og:description" content="A place where I can share my experiences with all things Kubernetes and Technology related." /><link rel="canonical" href="https://newtondev.github.io/posts/building-a-kubernetes-cluster-from-scratch/" /><meta property="og:url" content="https://newtondev.github.io/posts/building-a-kubernetes-cluster-from-scratch/" /><meta property="og:site_name" content="Craig Newton" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2022-07-09T14:00:00+02:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Building a Kubernetes cluster on AWS from scratch" /><meta name="twitter:site" content="@craignewtondev" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2022-07-09T14:00:00+02:00","datePublished":"2022-07-09T14:00:00+02:00","description":"A place where I can share my experiences with all things Kubernetes and Technology related.","headline":"Building a Kubernetes cluster on AWS from scratch","mainEntityOfPage":{"@type":"WebPage","@id":"https://newtondev.github.io/posts/building-a-kubernetes-cluster-from-scratch/"},"url":"https://newtondev.github.io/posts/building-a-kubernetes-cluster-from-scratch/"}</script><title>Building a Kubernetes cluster on AWS from scratch | Craig Newton</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="Craig Newton"><meta name="application-name" content="Craig Newton"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link rel="dns-prefetch" href="https://fonts.gstatic.com" crossorigin><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://cdn.jsdelivr.net" ><link rel="dns-prefetch" href="https://cdn.jsdelivr.net" ><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato&family=Source+Sans+Pro:wght@400;600;700;900&display=swap"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><body data-spy="scroll" data-target="#toc" data-topbar-visible="true"><div id="sidebar" class="d-flex flex-column align-items-end"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" class="mx-auto"> <img src="https://pbs.twimg.com/profile_images/1573218316889595904/hncWRiAJ_400x400.jpg" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">Craig Newton</a></div><div class="site-subtitle font-italic">DevOps Engineer and Technology Evangelist</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a><li class="nav-item"> <a href="/books/" class="nav-link"> <i class="fa-fw fas fa-book ml-xl-3 mr-xl-3 unloaded"></i> <span>BOOKS</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <a href="https://github.com/newtondev" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href="https://twitter.com/craignewtondev" aria-label="twitter" target="_blank" rel="noopener"> <i class="fab fa-twitter"></i> </a> <a href="https://www.linkedin.com/in/craig-newton-18a31923/" aria-label="linkedin" target="_blank" rel="noopener"> <i class="fab fa-linkedin"></i> </a> <a href="https://craignewtondev.medium.com/" aria-label="medium" target="_blank" rel="noopener"> <i class="fab fa-medium"></i> </a> <a href="/feed.xml" aria-label="rss" > <i class="fas fa-rss"></i> </a></div></div><div id="topbar-wrapper"><div id="topbar" class="container d-flex align-items-center justify-content-between h-100 pl-3 pr-3 pl-md-4 pr-md-4"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>Building a Kubernetes cluster on AWS from scratch</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper" class="d-flex justify-content-center"><div id="main" class="container pl-xl-4 pr-xl-4"><div class="row"><div id="core-wrapper" class="col-12 col-lg-11 col-xl-9 pr-xl-4"><div class="post pl-1 pr-1 pl-md-2 pr-md-2"><h1 data-toc-skip>Building a Kubernetes cluster on AWS from scratch</h1><div class="post-meta text-muted"> <span> Posted <em class="" data-ts="1657368000" data-df="ll" data-toggle="tooltip" data-placement="bottom"> Jul 9, 2022 </em> </span><div class="d-flex justify-content-between"> <span> By <em> <a href="https://twitter.com/craignewtondev">Craig Newton</a> </em> </span><div> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="8708 words"> <em>48 min</em> read</span></div></div></div><div class="post-content"><p><img data-src="/assets/img/2022/07/container_ship.jpg" alt="Building a Kubernetes cluster on AWS from scratch" data-proofer-ignore></p><p>These days rolling out a Kubernetes cluster on any cloud provider is quite simple and easy, but not always cost effective. Us as DevOps/Ops Engineers are spoilt with the ease of configuration and not being able to see “behind the curtain”, especially on the master control plane. But there is always that nagging question as to how that works. Let’s peel away that curtain and go on a journey to build our own HA(Highly Available) Kubernetes cluster on AWS.</p><p>This walkthrough is essential for people who support Kubernetes clusters in production and want to understand how it all fits together; and how all this runs on AWS. This work is based off of <a href="https://github.com/kelseyhightower/kubernetes-the-hard-way">Kelsey Hightower’s Kubernetes The Hard Way Guide</a> which was deployed to GCP (Google Cloud Platform).</p><p>Below is a simple representation of what the infrastructure architecture, of what we want to achieve, would look like:</p><p><img data-src="/assets/img/2022/07/kubernetes_architecture_aws.png" alt="" data-proofer-ignore></p><p>We will need to provision the following compute resources in AWS:</p><ul><li>Networking — VPC, Subnet, Internet Gateway, Route Tables, Security Groups, Network Load Balancer<li>Compute Instances — EC2 nodes for controllers and workers, SSH Key Pair</ul><p>Before we begin, we first need to ensure we have some prerequisites:</p><ul><li><p>Have an AWS account setup (hopefully with some free credit, but this cluster will not cost us much to provision as we will tear it down afterwards).</p><li><p>Install the AWS CLI for interacting with your AWS account to provision resources; or you can use the <a href="https://aws.amazon.com/cloudshell/">AWS Cloud Shell</a> which is a browser based alternative to setting all this up on your machine.</p><li><p>Pick an AWS zone you want to deploy in, preferably closer to where you are; for me I am choosing <strong>eu-central-1</strong> as that is my closest AWS region.</p></ul><p><img data-src="/assets/img/2022/07/cloudshell_region.png" alt="AWS CloudShell" data-proofer-ignore></p><ul><li><p>You can also install and use <a href="https://github.com/tmux/tmux/wiki">tmux</a> to simplify running the same commands on multiple instances.</p><li><p>We will be generating quite a few <a href="https://en.wikipedia.org/wiki/Public_key_infrastructure">PKI Infrastructure</a> keys and generate TLS certificates as we want everything to be secure. Ensure that you have <a href="https://github.com/cloudflare/cfssl">cfssl</a> and <a href="https://github.com/cloudflare/cfssl">cfssljon</a> command line utilties installed.</p><li><p>Because we will be using Kubernetes, we also need to make sure that we have our trusty <a href="https://kubernetes.io/docs/tasks/tools/">kubectl</a> client installed so we can perform actions on our Kubernetes cluster.</p></ul><h2 id="set-a-default-compute-region-and-zone"><span class="mr-2">Set a Default Compute Region and Zone</span><a href="#set-a-default-compute-region-and-zone" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>In your terminal window or in your AWS CloudShell window run:</p><div class="language-shell highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre><td class="rouge-code"><pre><span class="nv">AWS_REGION</span><span class="o">=</span>eu-central-1
aws configure <span class="nb">set </span>default.region <span class="nv">$AWS_REGION</span>
</pre></table></code></div></div><p><img data-src="/assets/img/2022/07/cloudshell_set_region.png" alt="CloudShell set region" data-proofer-ignore></p><h2 id="installing-some-client-tools-we-need"><span class="mr-2">Installing some client tools we need</span><a href="#installing-some-client-tools-we-need" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>I am going to be using examples using linux here, but if you are using some other OS like Mac OS X, then <a href="https://github.com/kelseyhightower/kubernetes-the-hard-way/blob/master/docs/02-client-tools.md">refer to this page here</a>.</p><div class="language-shell highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
</pre><td class="rouge-code"><pre>wget <span class="nt">-q</span> <span class="nt">--timestamping</span> <span class="se">\</span>
  https://storage.googleapis.com/kubernetes-the-hard-way/cfssl/1.4.1/linux/cfssl <span class="se">\</span>
  https://storage.googleapis.com/kubernetes-the-hard-way/cfssl/1.4.1/linux/cfssljson
<span class="nb">chmod</span> +x cfssl cfssljson
<span class="nb">sudo mv </span>cfssl cfssljson /usr/local/bin/
cfssl version
cfssljson <span class="nt">--version</span>
wget https://storage.googleapis.com/kubernetes-release/release/v1.21.0/bin/linux/amd64/kubectl
<span class="nb">chmod</span> +x kubectl
<span class="nb">sudo mv </span>kubectl /usr/local/bin/
kubectl version <span class="nt">--client</span>
</pre></table></code></div></div><p><img data-src="/assets/img/2022/07/tools_setup.png" alt="Tools Setup" data-proofer-ignore></p><h2 id="provisioning-compute-infrastructure"><span class="mr-2">Provisioning Compute Infrastructure</span><a href="#provisioning-compute-infrastructure" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>Best practise with AWS dictates that we wrap our “project”/”product” into it’s own VPC (Virtual Private Cloud) with Subnets, Routing tables, Load Balancers and an Internet Gateway (of which only 1 Internet Gateway is allowed per VPC). This is not only a grouping mechanism, but also a layer of security.</p><h3 id="vpc"><span class="mr-2">VPC</span><a href="#vpc" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>Let’s set up a VPC called <strong>kubernetes-from-scratch</strong> that has DNS support and DNS hostname support enabled. Execute the following in your terminal session:</p><div class="language-bash highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre><td class="rouge-code"><pre><span class="nv">VPC_ID</span><span class="o">=</span><span class="si">$(</span>aws ec2 create-vpc <span class="nt">--cidr-block</span> 10.0.0.0/16 <span class="nt">--output</span> text <span class="nt">--query</span> <span class="s1">'Vpc.VpcId'</span><span class="si">)</span>
aws ec2 create-tags <span class="nt">--resources</span> <span class="k">${</span><span class="nv">VPC_ID</span><span class="k">}</span> <span class="nt">--tags</span> <span class="nv">Key</span><span class="o">=</span>Name,Value<span class="o">=</span>kubernetes-from-scratch
aws ec2 modify-vpc-attribute <span class="nt">--vpc-id</span> <span class="k">${</span><span class="nv">VPC_ID</span><span class="k">}</span> <span class="nt">--enable-dns-support</span> <span class="s1">'{"Value": true}'</span>
aws ec2 modify-vpc-attribute <span class="nt">--vpc-id</span> <span class="k">${</span><span class="nv">VPC_ID</span><span class="k">}</span> <span class="nt">--enable-dns-hostnames</span> <span class="s1">'{"Value": true}'</span>
</pre></table></code></div></div><p><img data-src="/assets/img/2022/07/vpc_creation.png" alt="VPC create" data-proofer-ignore></p><p><img data-src="/assets/img/2022/07/aws_vpc.png" alt="AWS VPC" data-proofer-ignore></p><p><img data-src="/assets/img/2022/07/cidr.png" alt="CIDR Range" data-proofer-ignore></p><h3 id="private-subnet"><span class="mr-2">Private Subnet</span><a href="#private-subnet" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>We need to be able to assign private IP addresses to our compute instances for both the control plane controllers, as well as our worker instances. Subnets in our VPC allow us to create a range of IP addresses that we can allocate to our instances which do not allow external access (unless through a proxy or load balancer):</p><div class="language-shell highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre><td class="rouge-code"><pre><span class="nv">SUBNET_ID</span><span class="o">=</span><span class="si">$(</span>aws ec2 create-subnet <span class="se">\</span>
  <span class="nt">--vpc-id</span> <span class="k">${</span><span class="nv">VPC_ID</span><span class="k">}</span> <span class="se">\</span>
  <span class="nt">--cidr-block</span> 10.0.1.0/24 <span class="se">\</span>
  <span class="nt">--output</span> text <span class="nt">--query</span> <span class="s1">'Subnet.SubnetId'</span><span class="si">)</span>
aws ec2 create-tags <span class="nt">--resources</span> <span class="k">${</span><span class="nv">SUBNET_ID</span><span class="k">}</span> <span class="nt">--tags</span> <span class="nv">Key</span><span class="o">=</span>Name,Value<span class="o">=</span>kubernetes-pvt
</pre></table></code></div></div><p><img data-src="/assets/img/2022/07/private_subnet_shell.png" alt="CloudShell Private Subnet Creation" data-proofer-ignore></p><p><img data-src="/assets/img/2022/07/aws_private_subnet.png" alt="AWS Console Private Subnet" data-proofer-ignore></p><p><img data-src="/assets/img/2022/07/private_cidr.png" alt="Private Subnet CIDR" data-proofer-ignore></p><p>By using this CIDR range 10.0.1.0/24 we have up to 256 hosts (actually less due to AWS reserving some of the IPs, the first 4 and the last 1, <a href="https://docs.aws.amazon.com/vpc/latest/userguide/configure-subnets.html">read more here</a>).</p><h3 id="internet-gateway"><span class="mr-2">Internet Gateway</span><a href="#internet-gateway" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>Our instances need some way to connect and communicate with the internet since we are on a private network. This means we need to provision a gateway we can use to proxy our traffic through. Let’s setup one by running the following commands:</p><div class="language-shell highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre><td class="rouge-code"><pre><span class="nv">INTERNET_GATEWAY_ID</span><span class="o">=</span><span class="si">$(</span>aws ec2 create-internet-gateway <span class="nt">--output</span> text <span class="nt">--query</span> <span class="s1">'InternetGateway.InternetGatewayId'</span><span class="si">)</span>
aws ec2 create-tags <span class="nt">--resources</span> <span class="k">${</span><span class="nv">INTERNET_GATEWAY_ID</span><span class="k">}</span> <span class="nt">--tags</span> <span class="nv">Key</span><span class="o">=</span>Name,Value<span class="o">=</span>kubernetes-igw
aws ec2 attach-internet-gateway <span class="nt">--internet-gateway-id</span> <span class="k">${</span><span class="nv">INTERNET_GATEWAY_ID</span><span class="k">}</span> <span class="nt">--vpc-id</span> <span class="k">${</span><span class="nv">VPC_ID</span><span class="k">}</span>
</pre></table></code></div></div><p><img data-src="/assets/img/2022/07/shell_internet_gateway.png" alt="CloudShell Internet Gateway" data-proofer-ignore></p><p><img data-src="/assets/img/2022/07/aws_internet_gateway.png" alt="AWS Internet Gateway" data-proofer-ignore></p><h3 id="route-table"><span class="mr-2">Route Table</span><a href="#route-table" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>We need to now define how we want to route our traffic from our instances in our network through our Internet Gateway. To do that we define a routing table for the traffic:</p><div class="language-shell highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre><td class="rouge-code"><pre><span class="nv">ROUTE_TABLE_ID</span><span class="o">=</span><span class="si">$(</span>aws ec2 create-route-table <span class="nt">--vpc-id</span> <span class="k">${</span><span class="nv">VPC_ID</span><span class="k">}</span> <span class="nt">--output</span> text <span class="nt">--query</span> <span class="s1">'RouteTable.RouteTableId'</span><span class="si">)</span>
aws ec2 create-tags <span class="nt">--resources</span> <span class="k">${</span><span class="nv">ROUTE_TABLE_ID</span><span class="k">}</span> <span class="nt">--tags</span> <span class="nv">Key</span><span class="o">=</span>Name,Value<span class="o">=</span>kubernetes-rt
aws ec2 associate-route-table <span class="nt">--route-table-id</span> <span class="k">${</span><span class="nv">ROUTE_TABLE_ID</span><span class="k">}</span> <span class="nt">--subnet-id</span> <span class="k">${</span><span class="nv">SUBNET_ID</span><span class="k">}</span>
aws ec2 create-route <span class="nt">--route-table-id</span> <span class="k">${</span><span class="nv">ROUTE_TABLE_ID</span><span class="k">}</span> <span class="nt">--destination-cidr-block</span> 0.0.0.0/0 <span class="nt">--gateway-id</span> <span class="k">${</span><span class="nv">INTERNET_GATEWAY_ID</span><span class="k">}</span>
</pre></table></code></div></div><p><img data-src="/assets/img/2022/07/cloudshell_route_table.png" alt="CloudShell Route Table" data-proofer-ignore></p><p><img data-src="/assets/img/2022/07/aws_route_tables.png" alt="AWS Route Tables" data-proofer-ignore></p><p><img data-src="/assets/img/2022/07/aws_route_table_detail.png" alt="AWS Route Table Detail" data-proofer-ignore></p><p><img data-src="/assets/img/2022/07/aws_route_table_assoc.png" alt="AWS Route Table Subnet Associations" data-proofer-ignore></p><p>Our private subnet has now been associated with the Route Table and our routes have been setup for our Internet Gateway.</p><h3 id="security-group"><span class="mr-2">Security Group</span><a href="#security-group" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>We need a security group so that we can allow traffic between our instances, as well as access from our client software. We define rules for communication betweeen our controllers and our workers; SSH, Kubernetes API server, HTTPS and ICMP (for pings):</p><div class="language-shell highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
</pre><td class="rouge-code"><pre><span class="nv">SECURITY_GROUP_ID</span><span class="o">=</span><span class="si">$(</span>aws ec2 create-security-group <span class="se">\</span>
  <span class="nt">--group-name</span> kubernetes-from-scratch <span class="se">\</span>
  <span class="nt">--description</span> <span class="s2">"Kubernetes from scratch - security group"</span> <span class="se">\</span>
  <span class="nt">--vpc-id</span> <span class="k">${</span><span class="nv">VPC_ID</span><span class="k">}</span> <span class="se">\</span>
  <span class="nt">--output</span> text <span class="nt">--query</span> <span class="s1">'GroupId'</span><span class="si">)</span>
aws ec2 create-tags <span class="nt">--resources</span> <span class="k">${</span><span class="nv">SECURITY_GROUP_ID</span><span class="k">}</span> <span class="nt">--tags</span> <span class="nv">Key</span><span class="o">=</span>Name,Value<span class="o">=</span>kubernetes-sg
aws ec2 authorize-security-group-ingress <span class="nt">--group-id</span> <span class="k">${</span><span class="nv">SECURITY_GROUP_ID</span><span class="k">}</span> <span class="nt">--protocol</span> all <span class="nt">--cidr</span> 10.0.0.0/16
aws ec2 authorize-security-group-ingress <span class="nt">--group-id</span> <span class="k">${</span><span class="nv">SECURITY_GROUP_ID</span><span class="k">}</span> <span class="nt">--protocol</span> all <span class="nt">--cidr</span> 10.200.0.0/16
aws ec2 authorize-security-group-ingress <span class="nt">--group-id</span> <span class="k">${</span><span class="nv">SECURITY_GROUP_ID</span><span class="k">}</span> <span class="nt">--protocol</span> tcp <span class="nt">--port</span> 22 <span class="nt">--cidr</span> 0.0.0.0/0
aws ec2 authorize-security-group-ingress <span class="nt">--group-id</span> <span class="k">${</span><span class="nv">SECURITY_GROUP_ID</span><span class="k">}</span> <span class="nt">--protocol</span> tcp <span class="nt">--port</span> 6443 <span class="nt">--cidr</span> 0.0.0.0/0
aws ec2 authorize-security-group-ingress <span class="nt">--group-id</span> <span class="k">${</span><span class="nv">SECURITY_GROUP_ID</span><span class="k">}</span> <span class="nt">--protocol</span> tcp <span class="nt">--port</span> 443 <span class="nt">--cidr</span> 0.0.0.0/0
aws ec2 authorize-security-group-ingress <span class="nt">--group-id</span> <span class="k">${</span><span class="nv">SECURITY_GROUP_ID</span><span class="k">}</span> <span class="nt">--protocol</span> icmp <span class="nt">--port</span> <span class="nt">-1</span> <span class="nt">--cidr</span> 0.0.0.0/0
</pre></table></code></div></div><p><img data-src="/assets/img/2022/07/cloudshell_create_security_group.png" alt="CloudShell Create Security Group" data-proofer-ignore></p><p><img data-src="/assets/img/2022/07/cloudshell_sc_define_ingress_ssh.png" alt="Security Group SSH Access" data-proofer-ignore></p><p><img data-src="/assets/img/2022/07/cloudshell_sc_define_ingress_https.png" alt="Security Group HTTPS Access" data-proofer-ignore></p><p><img data-src="/assets/img/2022/07/aws_security_group.png" alt="AWS Security Group" data-proofer-ignore></p><p><img data-src="/assets/img/2022/07/aws_security_group_rules.png" alt="AWS Security Group Rules" data-proofer-ignore></p><h3 id="network-load-balancer"><span class="mr-2">Network Load Balancer</span><a href="#network-load-balancer" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>We need some way for us to access our Kubernetes API from the outside world . We create an <strong>Internet-facing Network Load Balancer</strong> and register our control plane controllers. This way we can have an HA (Highly Available) control plane. To create our load balancer execute the following:</p><div class="language-shell highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
</pre><td class="rouge-code"><pre><span class="nv">LOAD_BALANCER_ARN</span><span class="o">=</span><span class="si">$(</span>aws elbv2 create-load-balancer <span class="se">\</span>
    <span class="nt">--name</span> kubernetes-nlb <span class="se">\</span>
    <span class="nt">--subnets</span> <span class="k">${</span><span class="nv">SUBNET_ID</span><span class="k">}</span> <span class="se">\</span>
    <span class="nt">--scheme</span> internet-facing <span class="se">\</span>
    <span class="nt">--type</span> network <span class="se">\</span>
    <span class="nt">--output</span> text <span class="nt">--query</span> <span class="s1">'LoadBalancers[].LoadBalancerArn'</span><span class="si">)</span>
<span class="nv">TARGET_GROUP_ARN</span><span class="o">=</span><span class="si">$(</span>aws elbv2 create-target-group <span class="se">\</span>
    <span class="nt">--name</span> kubernetes-tg <span class="se">\</span>
    <span class="nt">--protocol</span> TCP <span class="se">\</span>
    <span class="nt">--port</span> 6443 <span class="se">\</span>
    <span class="nt">--vpc-id</span> <span class="k">${</span><span class="nv">VPC_ID</span><span class="k">}</span> <span class="se">\</span>
    <span class="nt">--target-type</span> ip <span class="se">\</span>
    <span class="nt">--output</span> text <span class="nt">--query</span> <span class="s1">'TargetGroups[].TargetGroupArn'</span><span class="si">)</span>
aws elbv2 register-targets <span class="nt">--target-group-arn</span> <span class="k">${</span><span class="nv">TARGET_GROUP_ARN</span><span class="k">}</span> <span class="nt">--targets</span> <span class="nv">Id</span><span class="o">=</span>10.0.1.1<span class="o">{</span>0,1,2<span class="o">}</span>
aws elbv2 create-listener <span class="se">\</span>
    <span class="nt">--load-balancer-arn</span> <span class="k">${</span><span class="nv">LOAD_BALANCER_ARN</span><span class="k">}</span> <span class="se">\</span>
    <span class="nt">--protocol</span> TCP <span class="se">\</span>
    <span class="nt">--port</span> 443 <span class="se">\</span>
    <span class="nt">--default-actions</span> <span class="nv">Type</span><span class="o">=</span>forward,TargetGroupArn<span class="o">=</span><span class="k">${</span><span class="nv">TARGET_GROUP_ARN</span><span class="k">}</span> <span class="se">\</span>
    <span class="nt">--output</span> text <span class="nt">--query</span> <span class="s1">'Listeners[].ListenerArn'</span>
</pre></table></code></div></div><p><img data-src="/assets/img/2022/07/cloudshell_create_lb.png" alt="CloudShell Create Network Load Balancer" data-proofer-ignore></p><p>We can get our public DNS address of our load balancer for use later on:</p><div class="language-shell highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre><td class="rouge-code"><pre><span class="nv">KUBERNETES_PUBLIC_ADDRESS</span><span class="o">=</span><span class="si">$(</span>aws elbv2 describe-load-balancers <span class="se">\</span>
  <span class="nt">--load-balancer-arns</span> <span class="k">${</span><span class="nv">LOAD_BALANCER_ARN</span><span class="k">}</span> <span class="se">\</span>
  <span class="nt">--output</span> text <span class="nt">--query</span> <span class="s1">'LoadBalancers[].DNSName'</span><span class="si">)</span>
</pre></table></code></div></div><p><img data-src="/assets/img/2022/07/cloudshell_export_lb_dns.png" alt="CloudShell export NLB DNS" data-proofer-ignore></p><h3 id="compute-instances"><span class="mr-2">Compute Instances</span><a href="#compute-instances" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>Now that we have setup all of our routing and supporting infrastructure, it has now come time to define our work horses (controllers and workers). We are going to be using Ubuntu 20.04 for our compute instances, so we will need to select that first:</p><div class="language-shell highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre><td class="rouge-code"><pre><span class="nv">IMAGE_ID</span><span class="o">=</span><span class="si">$(</span>aws ec2 describe-images <span class="nt">--owners</span> 099720109477 <span class="se">\</span>
  <span class="nt">--output</span> json <span class="se">\</span>
  <span class="nt">--filters</span> <span class="se">\</span>
  <span class="s1">'Name=root-device-type,Values=ebs'</span> <span class="se">\</span>
  <span class="s1">'Name=architecture,Values=x86_64'</span> <span class="se">\</span>
  <span class="s1">'Name=name,Values=ubuntu/images/hvm-ssd/ubuntu-focal-20.04-amd64-server-*'</span> <span class="se">\</span>
  | jq <span class="nt">-r</span> <span class="s1">'.Images|sort_by(.Name)[-1]|.ImageId'</span><span class="si">)</span>
</pre></table></code></div></div><p><img data-src="/assets/img/2022/07/cloudshell_image_id.png" alt="CloudShell Image ID" data-proofer-ignore></p><p>This will get the <strong>IMAGE_ID</strong> in our region for the Ubuntu 20.04 OS that we will be running on our nodes.</p><p>We need to connect to our nodes so we can install software and manage the systems, so we need to create a key-pair so we can use it to securely connect to our instances. Let’s create our new key-pair:</p><div class="language-shell highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre><td class="rouge-code"><pre>aws ec2 create-key-pair <span class="nt">--key-name</span> kubernetes <span class="nt">--output</span> text <span class="nt">--query</span> <span class="s1">'KeyMaterial'</span> <span class="o">&gt;</span> kubernetes.id_rsa
<span class="nb">chmod </span>600 kubernetes.id_rsa
</pre></table></code></div></div><p><img data-src="/assets/img/2022/07/cloudshell_generate_key_pair.png" alt="CloudShell Generate Key Pair" data-proofer-ignore></p><p>Remember to save this file somewhere safe like in a keychain or something like Bitwarden.</p><p>Next, let’s provision our Kubernetes controllers. We going to require 3 for HA (High Availability), and we will be using <strong>t3.micro</strong> instances in this step (feel free to experiment with different instance types, but bear in mind they do cost you money):</p><div class="language-plaintext highlighter-rouge"><div class="code-header"> <span data-label-text="Plaintext"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
</pre><td class="rouge-code"><pre>for i in 0 1 2; do
  instance_id=$(aws ec2 run-instances \
    --associate-public-ip-address \
    --image-id ${IMAGE_ID} \
    --count 1 \
    --key-name kubernetes \
    --security-group-ids ${SECURITY_GROUP_ID} \
    --instance-type t3.micro \
    --private-ip-address 10.0.1.1${i} \
    --user-data "name=controller-${i}" \
    --subnet-id ${SUBNET_ID} \
    --block-device-mappings='{"DeviceName": "/dev/sda1", "Ebs": { "VolumeSize": 50 }, "NoDevice": "" }' \
    --output text --query 'Instances[].InstanceId')
  aws ec2 modify-instance-attribute --instance-id ${instance_id} --no-source-dest-check
  aws ec2 create-tags --resources ${instance_id} --tags "Key=Name,Value=controller-${i}"
  echo "controller-${i} created "
done
</pre></table></code></div></div><p>Take special note of the <strong>key-name</strong> if you have named it different to what we have in the previous steps. We associate a public ip address as well to the instances.</p><p><img data-src="/assets/img/2022/07/cloudshell_create_controllers.png" alt="CloudShell Create Controllers" data-proofer-ignore></p><p><img data-src="/assets/img/2022/07/aws_ec2_controllers.png" alt="AWS EC2 Controller Instances" data-proofer-ignore></p><p>Now it is time to create the worker nodes:</p><div class="language-shell highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
</pre><td class="rouge-code"><pre><span class="k">for </span>i <span class="k">in </span>0 1 2<span class="p">;</span> <span class="k">do
  </span><span class="nv">instance_id</span><span class="o">=</span><span class="si">$(</span>aws ec2 run-instances <span class="se">\</span>
    <span class="nt">--associate-public-ip-address</span> <span class="se">\</span>
    <span class="nt">--image-id</span> <span class="k">${</span><span class="nv">IMAGE_ID</span><span class="k">}</span> <span class="se">\</span>
    <span class="nt">--count</span> 1 <span class="se">\</span>
    <span class="nt">--key-name</span> kubernetes <span class="se">\</span>
    <span class="nt">--security-group-ids</span> <span class="k">${</span><span class="nv">SECURITY_GROUP_ID</span><span class="k">}</span> <span class="se">\</span>
    <span class="nt">--instance-type</span> t3.micro <span class="se">\</span>
    <span class="nt">--private-ip-address</span> 10.0.1.2<span class="k">${</span><span class="nv">i</span><span class="k">}</span> <span class="se">\</span>
    <span class="nt">--user-data</span> <span class="s2">"name=worker-</span><span class="k">${</span><span class="nv">i</span><span class="k">}</span><span class="s2">|pod-cidr=10.200.</span><span class="k">${</span><span class="nv">i</span><span class="k">}</span><span class="s2">.0/24"</span> <span class="se">\</span>
    <span class="nt">--subnet-id</span> <span class="k">${</span><span class="nv">SUBNET_ID</span><span class="k">}</span> <span class="se">\</span>
    <span class="nt">--block-device-mappings</span><span class="o">=</span><span class="s1">'{"DeviceName": "/dev/sda1", "Ebs": { "VolumeSize": 50 }, "NoDevice": "" }'</span> <span class="se">\</span>
    <span class="nt">--output</span> text <span class="nt">--query</span> <span class="s1">'Instances[].InstanceId'</span><span class="si">)</span>
  aws ec2 modify-instance-attribute <span class="nt">--instance-id</span> <span class="k">${</span><span class="nv">instance_id</span><span class="k">}</span> <span class="nt">--no-source-dest-check</span>
  aws ec2 create-tags <span class="nt">--resources</span> <span class="k">${</span><span class="nv">instance_id</span><span class="k">}</span> <span class="nt">--tags</span> <span class="s2">"Key=Name,Value=worker-</span><span class="k">${</span><span class="nv">i</span><span class="k">}</span><span class="s2">"</span>
  <span class="nb">echo</span> <span class="s2">"worker-</span><span class="k">${</span><span class="nv">i</span><span class="k">}</span><span class="s2"> created"</span>
<span class="k">done</span>
</pre></table></code></div></div><p><img data-src="/assets/img/2022/07/cloudshell_create_workers.png" alt="CloudShell Create Workers" data-proofer-ignore></p><p><img data-src="/assets/img/2022/07/aws_ec2_workers.png" alt="AWS EC2 Workers" data-proofer-ignore></p><p>And just like that we have our infrastructure in place, we now need to move on and install the necessary software and configuration on each node.</p><h2 id="provisioning-a-ca-and-generating-tls-certificates"><span class="mr-2">Provisioning a CA and Generating TLS Certificates</span><a href="#provisioning-a-ca-and-generating-tls-certificates" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>Security is extremely important, and by using TLS Certificates we ensure that communication between the nodes in the system is secure.</p><p>Let us proceed to create a folder called certs and then enter the folder:</p><div class="language-shell highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre><td class="rouge-code"><pre><span class="nb">mkdir</span> <span class="nt">-p</span> certs
<span class="nb">cd </span>certs/
</pre></table></code></div></div><h3 id="certificate-authority"><span class="mr-2">Certificate Authority</span><a href="#certificate-authority" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><div class="language-shell highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
</pre><td class="rouge-code"><pre><span class="nb">cat</span> <span class="o">&gt;</span> ca-config.json <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh">
{
  "signing": {
    "default": {
      "expiry": "8760h"
    },
    "profiles": {
      "kubernetes": {
        "usages": ["signing", "key encipherment", "server auth", "client auth"],
        "expiry": "8760h"
      }
    }
  }
}
</span><span class="no">EOF
</span><span class="nb">cat</span> <span class="o">&gt;</span> ca-csr.json <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh">
{
  "CN": "Kubernetes",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "NL",
      "L": "Netherlands",
      "O": "Kubernetes",
      "OU": "AMS",
      "ST": "Amsterdam"
    }
  ]
}
</span><span class="no">EOF
</span>cfssl gencert <span class="nt">-initca</span> ca-csr.json | cfssljson <span class="nt">-bare</span> ca
</pre></table></code></div></div><p><img data-src="/assets/img/2022/07/cloudshell_ca.png" alt="CloudShell Certificate Authority" data-proofer-ignore></p><h2 id="client-and-server-certificates"><span class="mr-2">Client and Server Certificates</span><a href="#client-and-server-certificates" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>We need to generate client and server certificates for each Kubernetes component and a client certificate for the Kubernetes admin user.</p><h3 id="the-admin-client-certificate"><span class="mr-2">The Admin Client Certificate</span><a href="#the-admin-client-certificate" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>Generate the admin client certificate and private key:</p><div class="language-shell highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
</pre><td class="rouge-code"><pre><span class="nb">cat</span> <span class="o">&gt;</span> admin-csr.json <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh">
{
  "CN": "admin",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "NL",
      "L": "Netherlands",
      "O": "system:masters",
      "OU": "Kubernetes from scratch",
      "ST": "Amsterdam"
    }
  ]
}
</span><span class="no">EOF
</span>cfssl gencert <span class="se">\</span>
  <span class="nt">-ca</span><span class="o">=</span>ca.pem <span class="se">\</span>
  <span class="nt">-ca-key</span><span class="o">=</span>ca-key.pem <span class="se">\</span>
  <span class="nt">-config</span><span class="o">=</span>ca-config.json <span class="se">\</span>
  <span class="nt">-profile</span><span class="o">=</span>kubernetes <span class="se">\</span>
  admin-csr.json | cfssljson <span class="nt">-bare</span> admin
</pre></table></code></div></div><p><img data-src="/assets/img/2022/07/cloudshell_create_admin_cert.png" alt="CloudShell Create Admin Certificate" data-proofer-ignore></p><h3 id="the-kubelet-client-certificates"><span class="mr-2">The Kubelet Client Certificates</span><a href="#the-kubelet-client-certificates" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>Kubernetes uses a <a href="https://kubernetes.io/docs/admin/authorization/node/">special-purpose authorization mode</a> called Node Authorizer, that specifically authorizes API requests made by Kubelets. In order to be authorized by the Node Authorizer, <a href="https://kubernetes.io/docs/concepts/overview/components/#kubelet">Kubelets</a> must use a credential that identifies them as being in the <code class="language-plaintext highlighter-rouge">system:nodes</code> group, with a username of <code class="language-plaintext highlighter-rouge">system:node:&lt;nodeName&gt;</code>. In this section you will create a certificate for each Kubernetes worker node that meets the Node Authorizer requirements.</p><p>Generate a certificate and private key for each Kubernetes worker node:</p><div class="language-bash highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
</pre><td class="rouge-code"><pre><span class="k">for </span>i <span class="k">in </span>0 1 2<span class="p">;</span> <span class="k">do
  </span><span class="nv">instance</span><span class="o">=</span><span class="s2">"worker-</span><span class="k">${</span><span class="nv">i</span><span class="k">}</span><span class="s2">"</span>
  <span class="nv">instance_hostname</span><span class="o">=</span><span class="s2">"ip-10-0-1-2</span><span class="k">${</span><span class="nv">i</span><span class="k">}</span><span class="s2">"</span>
  <span class="nb">cat</span> <span class="o">&gt;</span> <span class="k">${</span><span class="nv">instance</span><span class="k">}</span><span class="nt">-csr</span>.json <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh">
{
  "CN": "system:node:</span><span class="k">${</span><span class="nv">instance_hostname</span><span class="k">}</span><span class="sh">",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "NL",
      "L": "Netherlands",
      "O": "system:nodes",
      "OU": "Kubernetes from scratch",
      "ST": "Amsterdam"
    }
  ]
}
</span><span class="no">EOF
</span><span class="nv">external_ip</span><span class="o">=</span><span class="si">$(</span>aws ec2 describe-instances <span class="nt">--filters</span> <span class="se">\</span>
    <span class="s2">"Name=tag:Name,Values=</span><span class="k">${</span><span class="nv">instance</span><span class="k">}</span><span class="s2">"</span> <span class="se">\</span>
    <span class="s2">"Name=instance-state-name,Values=running"</span> <span class="se">\</span>
    <span class="nt">--output</span> text <span class="nt">--query</span> <span class="s1">'Reservations[].Instances[].PublicIpAddress'</span><span class="si">)</span>
<span class="nv">internal_ip</span><span class="o">=</span><span class="si">$(</span>aws ec2 describe-instances <span class="nt">--filters</span> <span class="se">\</span>
    <span class="s2">"Name=tag:Name,Values=</span><span class="k">${</span><span class="nv">instance</span><span class="k">}</span><span class="s2">"</span> <span class="se">\</span>
    <span class="s2">"Name=instance-state-name,Values=running"</span> <span class="se">\</span>
    <span class="nt">--output</span> text <span class="nt">--query</span> <span class="s1">'Reservations[].Instances[].PrivateIpAddress'</span><span class="si">)</span>
cfssl gencert <span class="se">\</span>
    <span class="nt">-ca</span><span class="o">=</span>ca.pem <span class="se">\</span>
    <span class="nt">-ca-key</span><span class="o">=</span>ca-key.pem <span class="se">\</span>
    <span class="nt">-config</span><span class="o">=</span>ca-config.json <span class="se">\</span>
    <span class="nt">-hostname</span><span class="o">=</span><span class="k">${</span><span class="nv">instance_hostname</span><span class="k">}</span>,<span class="k">${</span><span class="nv">external_ip</span><span class="k">}</span>,<span class="k">${</span><span class="nv">internal_ip</span><span class="k">}</span> <span class="se">\</span>
    <span class="nt">-profile</span><span class="o">=</span>kubernetes <span class="se">\</span>
    worker-<span class="k">${</span><span class="nv">i</span><span class="k">}</span><span class="nt">-csr</span>.json | cfssljson <span class="nt">-bare</span> worker-<span class="k">${</span><span class="nv">i</span><span class="k">}</span>
<span class="k">done</span>
</pre></table></code></div></div><p><img data-src="/assets/img/2022/07/cloudshell_create_worker_certs_1.png" alt="CloudShell Create Worker Certificates" data-proofer-ignore> <img data-src="/assets/img/2022/07/cloudshell_create_worker_certs_2.png" alt="Cloud Shell Create Worker Certificates" data-proofer-ignore></p><h2 id="the-controller-manager-client-certificate"><span class="mr-2">The Controller Manager Client Certificate</span><a href="#the-controller-manager-client-certificate" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>Generate the <strong>kube-controller-manager</strong> client certificate and private key:</p><div class="language-shell highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
</pre><td class="rouge-code"><pre><span class="nb">cat</span> <span class="o">&gt;</span> kube-controller-manager-csr.json <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh">
{
  "CN": "system:kube-controller-manager",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "NL",
      "L": "Netherlands",
      "O": "system:kube-controller-manager",
      "OU": "Kubernetes from scratch",
      "ST": "Amsterdam"
    }
  ]
}
</span><span class="no">EOF
</span>cfssl gencert <span class="se">\</span>
  <span class="nt">-ca</span><span class="o">=</span>ca.pem <span class="se">\</span>
  <span class="nt">-ca-key</span><span class="o">=</span>ca-key.pem <span class="se">\</span>
  <span class="nt">-config</span><span class="o">=</span>ca-config.json <span class="se">\</span>
  <span class="nt">-profile</span><span class="o">=</span>kubernetes <span class="se">\</span>
  kube-controller-manager-csr.json | cfssljson <span class="nt">-bare</span> kube-controller-manager
</pre></table></code></div></div><p><img data-src="/assets/img/2022/07/cloudshell_create_cm_cert.png" alt="CloudShell Create Controller Manager Certificate" data-proofer-ignore></p><h2 id="the-kube-proxy-client-certificate"><span class="mr-2">The Kube Proxy Client Certificate</span><a href="#the-kube-proxy-client-certificate" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>Generate the <strong>kube-proxy</strong> client certificate and private key:</p><div class="language-shell highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
</pre><td class="rouge-code"><pre><span class="nb">cat</span> <span class="o">&gt;</span> kube-proxy-csr.json <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh">
{
  "CN": "system:kube-proxy",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "NL",
      "L": "Netherlands",
      "O": "system:node-proxier",
      "OU": "Kubernetes from scratch",
      "ST": "Amsterdam"
    }
  ]
}
</span><span class="no">EOF
</span>cfssl gencert <span class="se">\</span>
  <span class="nt">-ca</span><span class="o">=</span>ca.pem <span class="se">\</span>
  <span class="nt">-ca-key</span><span class="o">=</span>ca-key.pem <span class="se">\</span>
  <span class="nt">-config</span><span class="o">=</span>ca-config.json <span class="se">\</span>
  <span class="nt">-profile</span><span class="o">=</span>kubernetes <span class="se">\</span>
  kube-proxy-csr.json | cfssljson <span class="nt">-bare</span> kube-proxy
</pre></table></code></div></div><p><img data-src="/assets/img/2022/07/cloudshell_create_cloud_proxy_cert.png" alt="CloudShell Create Cloud Proxy Certificate" data-proofer-ignore></p><h2 id="the-scheduler-client-certificate"><span class="mr-2">The Scheduler Client Certificate</span><a href="#the-scheduler-client-certificate" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>Generate the <strong>kube-scheduler</strong> client certificate and private key:</p><div class="language-shell highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
</pre><td class="rouge-code"><pre><span class="nb">cat</span> <span class="o">&gt;</span> kube-scheduler-csr.json <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh">
{
  "CN": "system:kube-scheduler",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "NL",
      "L": "Netherlands",
      "O": "system:kube-scheduler",
      "OU": "Kubernetes from scratch",
      "ST": "Amsterdam"
    }
  ]
}
</span><span class="no">EOF
</span>cfssl gencert <span class="se">\</span>
  <span class="nt">-ca</span><span class="o">=</span>ca.pem <span class="se">\</span>
  <span class="nt">-ca-key</span><span class="o">=</span>ca-key.pem <span class="se">\</span>
  <span class="nt">-config</span><span class="o">=</span>ca-config.json <span class="se">\</span>
  <span class="nt">-profile</span><span class="o">=</span>kubernetes <span class="se">\</span>
  kube-scheduler-csr.json | cfssljson <span class="nt">-bare</span> kube-scheduler
</pre></table></code></div></div><p><img data-src="/assets/img/2022/07/cloudshell_create_cloud_proxy_cert.png" alt="CloudShell Create Kube Scheduler Certificate" data-proofer-ignore></p><h2 id="the-kubernetes-api-server-certificate"><span class="mr-2">The Kubernetes API Server Certificate</span><a href="#the-kubernetes-api-server-certificate" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>The Kubernetes Public Address will be included in the list of subject alternative names for the Kubernetes API Server certificate. This will encure the certificate can be validated by remote clients.</p><p>Generate the Kubernetes API Server certificate and private key:</p><div class="language-shell highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
</pre><td class="rouge-code"><pre><span class="nv">KUBERNETES_HOSTNAMES</span><span class="o">=</span>kubernetes,kubernetes.default,kubernetes.default.svc,kubernetes.default.svc.cluster,kubernetes.svc.cluster.local
<span class="nb">cat</span> <span class="o">&gt;</span> kubernetes-csr.json <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh">
{
  "CN": "kubernetes",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "NL",
      "L": "Netherlands",
      "O": "Kubernetes",
      "OU": "Kubernetes from scratch",
      "ST": "Amsterdam"
    }
  ]
}
</span><span class="no">EOF
</span>cfssl gencert <span class="se">\</span>
  <span class="nt">-ca</span><span class="o">=</span>ca.pem <span class="se">\</span>
  <span class="nt">-ca-key</span><span class="o">=</span>ca-key.pem <span class="se">\</span>
  <span class="nt">-config</span><span class="o">=</span>ca-config.json <span class="se">\</span>
  <span class="nt">-hostname</span><span class="o">=</span>10.32.0.1,10.0.1.10,10.0.1.11,10.0.1.12,<span class="k">${</span><span class="nv">KUBERNETES_PUBLIC_ADDRESS</span><span class="k">}</span>,127.0.0.1,<span class="k">${</span><span class="nv">KUBERNETES_HOSTNAMES</span><span class="k">}</span> <span class="se">\</span>
  <span class="nt">-profile</span><span class="o">=</span>kubernetes <span class="se">\</span>
  kubernetes-csr.json | cfssljson <span class="nt">-bare</span> kubernetes
</pre></table></code></div></div><p><img data-src="/assets/img/2022/07/cloudshell_create_api_server_cert.png" alt="CloudShell Create Kubernetes API Server Certificate" data-proofer-ignore></p><h2 id="the-service-account-key-pair"><span class="mr-2">The Service Account Key Pair</span><a href="#the-service-account-key-pair" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>The Kubernetes Controller Manager leverages a key pair to generate and sign service account tokens as described in the <a href="https://kubernetes.io/docs/admin/service-accounts-admin/">managing service accounts</a> documentation.</p><p>Generate the <strong>service-account</strong> certificate and private key:</p><div class="language-shell highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
</pre><td class="rouge-code"><pre><span class="nb">cat</span> <span class="o">&gt;</span> service-account-csr.json <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh">
{
  "CN": "service-accounts",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "NL",
      "L": "Netherlands",
      "O": "Kubernetes",
      "OU": "Kubernetes from scratch",
      "ST": "Amsterdam"
    }
  ]
}
</span><span class="no">EOF
</span>cfssl gencert <span class="se">\</span>
  <span class="nt">-ca</span><span class="o">=</span>ca.pem <span class="se">\</span>
  <span class="nt">-ca-key</span><span class="o">=</span>ca-key.pem <span class="se">\</span>
  <span class="nt">-config</span><span class="o">=</span>ca-config.json <span class="se">\</span>
  <span class="nt">-profile</span><span class="o">=</span>kubernetes <span class="se">\</span>
  service-account-csr.json | cfssljson <span class="nt">-bare</span> service-account
</pre></table></code></div></div><p><img data-src="/assets/img/2022/07/cloudshell_service_account_cert.png" alt="CloudShell Service Account Certificate" data-proofer-ignore></p><h2 id="distribute-the-client-and-server-certificates"><span class="mr-2">Distribute the Client and Server Certificates</span><a href="#distribute-the-client-and-server-certificates" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>Now that we have this directory full of certificates, its time to send them to the nodes.</p><p>First we will copy the certificates and private keys to each worker instance:</p><div class="language-shell highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre><td class="rouge-code"><pre><span class="k">for </span>instance <span class="k">in </span>worker-0 worker-1 worker-2<span class="p">;</span> <span class="k">do
  </span><span class="nv">external_ip</span><span class="o">=</span><span class="si">$(</span>aws ec2 describe-instances <span class="nt">--filters</span> <span class="se">\</span>
    <span class="s2">"Name=tag:Name,Values=</span><span class="k">${</span><span class="nv">instance</span><span class="k">}</span><span class="s2">"</span> <span class="se">\</span>
    <span class="s2">"Name=instance-state-name,Values=running"</span> <span class="se">\</span>
    <span class="nt">--output</span> text <span class="nt">--query</span> <span class="s1">'Reservations[].Instances[].PublicIpAddress'</span><span class="si">)</span>
scp <span class="nt">-i</span> ../kubernetes.id_rsa ca.pem <span class="k">${</span><span class="nv">instance</span><span class="k">}</span><span class="nt">-key</span>.pem <span class="k">${</span><span class="nv">instance</span><span class="k">}</span>.pem ubuntu@<span class="k">${</span><span class="nv">external_ip</span><span class="k">}</span>:~/
<span class="k">done</span>
</pre></table></code></div></div><p>Take note, we are in the <strong>certs/</strong> folder, so to I added the <strong>../kubernetes.id_rsa</strong> — which is one directory up from where we were in case you are getting permission denied errors.</p><p><img data-src="/assets/img/2022/07/cloudshell_copy_certs.png" alt="CloudShell Copy Certificates" data-proofer-ignore></p><p>Next we copy the certificates and private keys to each controller instance:</p><div class="language-shell highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre><td class="rouge-code"><pre><span class="k">for </span>instance <span class="k">in </span>controller-0 controller-1 controller-2<span class="p">;</span> <span class="k">do
  </span><span class="nv">external_ip</span><span class="o">=</span><span class="si">$(</span>aws ec2 describe-instances <span class="nt">--filters</span> <span class="se">\</span>
    <span class="s2">"Name=tag:Name,Values=</span><span class="k">${</span><span class="nv">instance</span><span class="k">}</span><span class="s2">"</span> <span class="se">\</span>
    <span class="s2">"Name=instance-state-name,Values=running"</span> <span class="se">\</span>
    <span class="nt">--output</span> text <span class="nt">--query</span> <span class="s1">'Reservations[].Instances[].PublicIpAddress'</span><span class="si">)</span>
scp <span class="nt">-i</span> ../kubernetes.id_rsa <span class="se">\</span>
    ca.pem ca-key.pem kubernetes-key.pem kubernetes.pem <span class="se">\</span>
    service-account-key.pem service-account.pem ubuntu@<span class="k">${</span><span class="nv">external_ip</span><span class="k">}</span>:~/
<span class="k">done</span>
</pre></table></code></div></div><p><img data-src="/assets/img/2022/07/cloudshell_copy_private_keys.png" alt="CloudShell Copy Private Keys" data-proofer-ignore></p><h2 id="generating-kubernetes-configuration-files-for-authentication"><span class="mr-2">Generating Kubernetes Configuration Files for Authentication</span><a href="#generating-kubernetes-configuration-files-for-authentication" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>We need to generate some <a href="https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/">kubeconfigs</a> — Kubernetes configuration files — which enable Kubernetes clients to locate and authenticate to the Kubernetes API Servers.</p><p>For this we will go back to our home folder and create a folder called configs and then change into that folder:</p><div class="language-shell highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre><td class="rouge-code"><pre><span class="nb">cd</span> ~/
<span class="nb">mkdir</span> <span class="nt">-p</span> configs
<span class="nb">cd </span>configs/
</pre></table></code></div></div><p><img data-src="/assets/img/2022/07/cloudshell_configs.png" alt="CloudShell Kubernetes Configs" data-proofer-ignore></p><h3 id="the-kubelet-configuration-file"><span class="mr-2">The kubelet Configuration File</span><a href="#the-kubelet-configuration-file" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>Generate a kubeconfig for each worker node:</p><div class="language-shell highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
</pre><td class="rouge-code"><pre><span class="k">for </span>instance <span class="k">in </span>worker-0 worker-1 worker-2<span class="p">;</span> <span class="k">do
  </span>kubectl config set-cluster kubernetes-from-scratch <span class="se">\</span>
    <span class="nt">--certificate-authority</span><span class="o">=</span>../certs/ca.pem <span class="se">\</span>
    <span class="nt">--embed-certs</span><span class="o">=</span><span class="nb">true</span> <span class="se">\</span>
    <span class="nt">--server</span><span class="o">=</span>https://<span class="k">${</span><span class="nv">KUBERNETES_PUBLIC_ADDRESS</span><span class="k">}</span>:443 <span class="se">\</span>
    <span class="nt">--kubeconfig</span><span class="o">=</span><span class="k">${</span><span class="nv">instance</span><span class="k">}</span>.kubeconfig
kubectl config set-credentials system:node:<span class="k">${</span><span class="nv">instance</span><span class="k">}</span> <span class="se">\</span>
    <span class="nt">--client-certificate</span><span class="o">=</span>../certs/<span class="k">${</span><span class="nv">instance</span><span class="k">}</span>.pem <span class="se">\</span>
    <span class="nt">--client-key</span><span class="o">=</span>../certs/<span class="k">${</span><span class="nv">instance</span><span class="k">}</span><span class="nt">-key</span>.pem <span class="se">\</span>
    <span class="nt">--embed-certs</span><span class="o">=</span><span class="nb">true</span> <span class="se">\</span>
    <span class="nt">--kubeconfig</span><span class="o">=</span><span class="k">${</span><span class="nv">instance</span><span class="k">}</span>.kubeconfig
kubectl config set-context default <span class="se">\</span>
    <span class="nt">--cluster</span><span class="o">=</span>kubernetes-from-scratch <span class="se">\</span>
    <span class="nt">--user</span><span class="o">=</span>system:node:<span class="k">${</span><span class="nv">instance</span><span class="k">}</span> <span class="se">\</span>
    <span class="nt">--kubeconfig</span><span class="o">=</span><span class="k">${</span><span class="nv">instance</span><span class="k">}</span>.kubeconfig
kubectl config use-context default <span class="nt">--kubeconfig</span><span class="o">=</span><span class="k">${</span><span class="nv">instance</span><span class="k">}</span>.kubeconfig
<span class="k">done</span>
</pre></table></code></div></div><p><img data-src="/assets/img/2022/07/cloudshell_config_kubelet.png" alt="CloudShell Kubelet Config" data-proofer-ignore></p><h3 id="the-kube-proxy-kubernets-configuration-file"><span class="mr-2">The kube-proxy Kubernets Configuration File</span><a href="#the-kube-proxy-kubernets-configuration-file" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>Generate a kubeconfig file for the <strong>kube-proxy</strong> service:</p><div class="language-shell highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
</pre><td class="rouge-code"><pre>kubectl config set-cluster kubernetes-from-scratch <span class="se">\</span>
  <span class="nt">--certificate-authority</span><span class="o">=</span>../certs/ca.pem <span class="se">\</span>
  <span class="nt">--embed-certs</span><span class="o">=</span><span class="nb">true</span> <span class="se">\</span>
  <span class="nt">--server</span><span class="o">=</span>https://<span class="k">${</span><span class="nv">KUBERNETES_PUBLIC_ADDRESS</span><span class="k">}</span>:443 <span class="se">\</span>
  <span class="nt">--kubeconfig</span><span class="o">=</span>kube-proxy.kubeconfig
kubectl config set-credentials system:kube-proxy <span class="se">\</span>
  <span class="nt">--client-certificate</span><span class="o">=</span>../certs/kube-proxy.pem <span class="se">\</span>
  <span class="nt">--client-key</span><span class="o">=</span>../certs/kube-proxy-key.pem <span class="se">\</span>
  <span class="nt">--embed-certs</span><span class="o">=</span><span class="nb">true</span> <span class="se">\</span>
  <span class="nt">--kubeconfig</span><span class="o">=</span>kube-proxy.kubeconfig
kubectl config set-context default <span class="se">\</span>
  <span class="nt">--cluster</span><span class="o">=</span>kubernetes-from-scratch <span class="se">\</span>
  <span class="nt">--user</span><span class="o">=</span>system:kube-proxy <span class="se">\</span>
  <span class="nt">--kubeconfig</span><span class="o">=</span>kube-proxy.kubeconfig
kubectl config use-context default <span class="nt">--kubeconfig</span><span class="o">=</span>kube-proxy.kubeconfig
</pre></table></code></div></div><p><img data-src="/assets/img/2022/07/cloudshell_config_kube_proxy.png" alt="CloudShell Kubernetes Proxy Configuration" data-proofer-ignore></p><h3 id="the-kube-controller-manager-kubernetes-configuration-file"><span class="mr-2">The kube-controller-manager Kubernetes Configuration File</span><a href="#the-kube-controller-manager-kubernetes-configuration-file" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>Generate a kubeconfig file for the <strong>kube-controller-manager</strong> service:</p><div class="language-shell highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
</pre><td class="rouge-code"><pre>kubectl config set-cluster kubernetes-from-scratch <span class="se">\</span>
  <span class="nt">--certificate-authority</span><span class="o">=</span>../certs/ca.pem <span class="se">\</span>
  <span class="nt">--embed-certs</span><span class="o">=</span><span class="nb">true</span> <span class="se">\</span>
  <span class="nt">--server</span><span class="o">=</span>https://127.0.0.1:6443 <span class="se">\</span>
  <span class="nt">--kubeconfig</span><span class="o">=</span>kube-controller-manager.kubeconfig
kubectl config set-credentials system:kube-controller-manager <span class="se">\</span>
  <span class="nt">--client-certificate</span><span class="o">=</span>../certs/kube-controller-manager.pem <span class="se">\</span>
  <span class="nt">--client-key</span><span class="o">=</span>../certs/kube-controller-manager-key.pem <span class="se">\</span>
  <span class="nt">--embed-certs</span><span class="o">=</span><span class="nb">true</span> <span class="se">\</span>
  <span class="nt">--kubeconfig</span><span class="o">=</span>kube-controller-manager.kubeconfig
kubectl config set-context default <span class="se">\</span>
  <span class="nt">--cluster</span><span class="o">=</span>kubernetes-from-scratch <span class="se">\</span>
  <span class="nt">--user</span><span class="o">=</span>system:kube-controller-manager <span class="se">\</span>
  <span class="nt">--kubeconfig</span><span class="o">=</span>kube-controller-manager.kubeconfig
kubectl config use-context default <span class="nt">--kubeconfig</span><span class="o">=</span>kube-controller-manager.kubeconfig
</pre></table></code></div></div><p><img data-src="/assets/img/2022/07/cloudshell_config_cm.png" alt="CloudShell Kubernetes Config Manager Configuration" data-proofer-ignore></p><h3 id="the-kube-scheduler-kubernetes-configuration-file"><span class="mr-2">The kube-scheduler Kubernetes Configuration File</span><a href="#the-kube-scheduler-kubernetes-configuration-file" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>Generate a kubeconfig file for the kube-scheduler service:</p><div class="language-shell highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
</pre><td class="rouge-code"><pre>kubectl config set-cluster kubernetes-from-scratch <span class="se">\</span>
  <span class="nt">--certificate-authority</span><span class="o">=</span>../certs/ca.pem <span class="se">\</span>
  <span class="nt">--embed-certs</span><span class="o">=</span><span class="nb">true</span> <span class="se">\</span>
  <span class="nt">--server</span><span class="o">=</span>https://127.0.0.1:6443 <span class="se">\</span>
  <span class="nt">--kubeconfig</span><span class="o">=</span>kube-scheduler.kubeconfig
kubectl config set-credentials system:kube-scheduler <span class="se">\</span>
  <span class="nt">--client-certificate</span><span class="o">=</span>../certs/kube-scheduler.pem <span class="se">\</span>
  <span class="nt">--client-key</span><span class="o">=</span>../certs/kube-scheduler-key.pem <span class="se">\</span>
  <span class="nt">--embed-certs</span><span class="o">=</span><span class="nb">true</span> <span class="se">\</span>
  <span class="nt">--kubeconfig</span><span class="o">=</span>kube-scheduler.kubeconfig
kubectl config set-context default <span class="se">\</span>
  <span class="nt">--cluster</span><span class="o">=</span>kubernetes-from-scratch <span class="se">\</span>
  <span class="nt">--user</span><span class="o">=</span>system:kube-scheduler <span class="se">\</span>
  <span class="nt">--kubeconfig</span><span class="o">=</span>kube-scheduler.kubeconfig
kubectl config use-context default <span class="nt">--kubeconfig</span><span class="o">=</span>kube-scheduler.kubeconfig
</pre></table></code></div></div><p><img data-src="/assets/img/2022/07/cloudshell_config_kube_scheduler.png" alt="CloudShell Kubernetes Scheduler Configuration" data-proofer-ignore></p><h3 id="the-admin-kubernetes-configuration-file"><span class="mr-2">The admin Kubernetes Configuration File</span><a href="#the-admin-kubernetes-configuration-file" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>Generate a kubeconfig file for the <strong>admin</strong> user:</p><div class="language-shell highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
</pre><td class="rouge-code"><pre>kubectl config set-cluster kubernetes-from-scratch <span class="se">\</span>
  <span class="nt">--certificate-authority</span><span class="o">=</span>../certs/ca.pem <span class="se">\</span>
  <span class="nt">--embed-certs</span><span class="o">=</span><span class="nb">true</span> <span class="se">\</span>
  <span class="nt">--server</span><span class="o">=</span>https://127.0.0.1:6443 <span class="se">\</span>
  <span class="nt">--kubeconfig</span><span class="o">=</span>admin.kubeconfig
kubectl config set-credentials admin <span class="se">\</span>
  <span class="nt">--client-certificate</span><span class="o">=</span>../certs/admin.pem <span class="se">\</span>
  <span class="nt">--client-key</span><span class="o">=</span>../certs/admin-key.pem <span class="se">\</span>
  <span class="nt">--embed-certs</span><span class="o">=</span><span class="nb">true</span> <span class="se">\</span>
  <span class="nt">--kubeconfig</span><span class="o">=</span>admin.kubeconfig
kubectl config set-context default <span class="se">\</span>
  <span class="nt">--cluster</span><span class="o">=</span>kubernetes-from-scratch <span class="se">\</span>
  <span class="nt">--user</span><span class="o">=</span>admin <span class="se">\</span>
  <span class="nt">--kubeconfig</span><span class="o">=</span>admin.kubeconfig
kubectl config use-context default <span class="nt">--kubeconfig</span><span class="o">=</span>admin.kubeconfig
</pre></table></code></div></div><p><img data-src="/assets/img/2022/07/cloudshell_config_admin.png" alt="CloudShell Kubernetes Admin User Configuration" data-proofer-ignore></p><h2 id="distribute-the-kubernetes-configuration-files"><span class="mr-2">Distribute the Kubernetes Configuration Files</span><a href="#distribute-the-kubernetes-configuration-files" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>Copy the <strong>kubelet</strong> and <strong>kube-proxy</strong> kubeconfig files to each worker instance:</p><div class="language-shell highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
</pre><td class="rouge-code"><pre><span class="k">for </span>instance <span class="k">in </span>worker-0 worker-1 worker-2<span class="p">;</span> <span class="k">do
  </span><span class="nv">external_ip</span><span class="o">=</span><span class="si">$(</span>aws ec2 describe-instances <span class="nt">--filters</span> <span class="se">\</span>
    <span class="s2">"Name=tag:Name,Values=</span><span class="k">${</span><span class="nv">instance</span><span class="k">}</span><span class="s2">"</span> <span class="se">\</span>
    <span class="s2">"Name=instance-state-name,Values=running"</span> <span class="se">\</span>
    <span class="nt">--output</span> text <span class="nt">--query</span> <span class="s1">'Reservations[].Instances[].PublicIpAddress'</span><span class="si">)</span>
scp <span class="nt">-i</span> ../kubernetes.id_rsa <span class="se">\</span>
    <span class="k">${</span><span class="nv">instance</span><span class="k">}</span>.kubeconfig kube-proxy.kubeconfig ubuntu@<span class="k">${</span><span class="nv">external_ip</span><span class="k">}</span>:~/
<span class="k">done</span>
</pre></table></code></div></div><p><img data-src="/assets/img/2022/07/cloudshell_distribute_configs_worker.png" alt="CloudShell Distribute Configurations to Worker Instances" data-proofer-ignore></p><p>Take note, we are in the <strong>configs/</strong> folder, so to I added the <strong>../kubernetes.id_rsa</strong> — which is one directory up from where we were in case you are getting permission denied errors.</p><p>Copy the <strong>kube-controller-manager</strong> and <strong>kube-scheduler</strong> kubeconfig files to each controller instance:</p><div class="language-shell highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre><td class="rouge-code"><pre><span class="k">for </span>instance <span class="k">in </span>controller-0 controller-1 controller-2<span class="p">;</span> <span class="k">do
  </span><span class="nv">external_ip</span><span class="o">=</span><span class="si">$(</span>aws ec2 describe-instances <span class="nt">--filters</span> <span class="se">\</span>
    <span class="s2">"Name=tag:Name,Values=</span><span class="k">${</span><span class="nv">instance</span><span class="k">}</span><span class="s2">"</span> <span class="se">\</span>
    <span class="s2">"Name=instance-state-name,Values=running"</span> <span class="se">\</span>
    <span class="nt">--output</span> text <span class="nt">--query</span> <span class="s1">'Reservations[].Instances[].PublicIpAddress'</span><span class="si">)</span>
  
  scp <span class="nt">-i</span> ../kubernetes.id_rsa <span class="se">\</span>
    admin.kubeconfig kube-controller-manager.kubeconfig kube-scheduler.kubeconfig ubuntu@<span class="k">${</span><span class="nv">external_ip</span><span class="k">}</span>:~/
<span class="k">done</span>
</pre></table></code></div></div><p><img data-src="/assets/img/2022/07/cloudshell_distribute_configs_controllers.png" alt="CloudShell Distribute Configurations to Controller Instances" data-proofer-ignore></p><p>Take note, we are in the <strong>configs/</strong> folder, so to I added the <strong>../kubernetes.id_rsa</strong> — which is one directory up from where we were in case you are getting permission denied errors.</p><h2 id="generating-the-data-encryption-config-and-key"><span class="mr-2">Generating the Data Encryption Config and Key</span><a href="#generating-the-data-encryption-config-and-key" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>Kubernetes stores a variety of data including cluster state, application configurations, and secrets. Kubernetes supports the ability to <a href="https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data">encrypt</a> cluster data at rest.</p><p>For this we need to generate an encryption key and an <a href="https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/#understanding-the-encryption-at-rest-configuration">encryption config</a> suitable for encrypting Kubernetes Secrets.</p><p>For this we will go back to our home folder and create a folder called <strong>encryption</strong> and then change into that folder:</p><div class="language-shell highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre><td class="rouge-code"><pre><span class="nb">cd</span> ~/
<span class="nb">mkdir</span> <span class="nt">-p</span> encryption
<span class="nb">cd </span>encryption/
</pre></table></code></div></div><p><img data-src="/assets/img/2022/07/cloudshell_create_encryption_folder.png" alt="CloudShell Create Encryption Folder" data-proofer-ignore></p><h3 id="the-encryption-key"><span class="mr-2">The Encryption Key</span><a href="#the-encryption-key" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>Generate an encryption key:</p><div class="language-shell highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre><span class="nv">ENCRYPTION_KEY</span><span class="o">=</span><span class="si">$(</span><span class="nb">head</span> <span class="nt">-c</span> 32 /dev/urandom | <span class="nb">base64</span><span class="si">)</span>
</pre></table></code></div></div><h3 id="the-encryption-config"><span class="mr-2">The Encryption Config</span><a href="#the-encryption-config" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>Create the <strong>encryption-config.yaml</strong> encryption config file:</p><div class="language-shell highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
</pre><td class="rouge-code"><pre><span class="nb">cat</span> <span class="o">&gt;</span> encryption-config.yaml <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh">
kind: EncryptionConfig
apiVersion: v1
resources:
  - resources:
      - secrets
    providers:
      - aescbc:
          keys:
            - name: key1
              secret: </span><span class="k">${</span><span class="nv">ENCRYPTION_KEY</span><span class="k">}</span><span class="sh">
      - identity: {}
</span><span class="no">EOF
</span></pre></table></code></div></div><p><img data-src="/assets/img/2022/07/cloudshell_encryption_config.png" alt="CloudShell Encryption Configuration" data-proofer-ignore></p><p>Copy the <strong>encryption-config.yaml</strong> encryption config file to each controller instance:</p><div class="language-shell highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
</pre><td class="rouge-code"><pre><span class="k">for </span>instance <span class="k">in </span>controller-0 controller-1 controller-2<span class="p">;</span> <span class="k">do
  </span><span class="nv">external_ip</span><span class="o">=</span><span class="si">$(</span>aws ec2 describe-instances <span class="nt">--filters</span> <span class="se">\</span>
    <span class="s2">"Name=tag:Name,Values=</span><span class="k">${</span><span class="nv">instance</span><span class="k">}</span><span class="s2">"</span> <span class="se">\</span>
    <span class="s2">"Name=instance-state-name,Values=running"</span> <span class="se">\</span>
    <span class="nt">--output</span> text <span class="nt">--query</span> <span class="s1">'Reservations[].Instances[].PublicIpAddress'</span><span class="si">)</span>
  
  scp <span class="nt">-i</span> ../kubernetes.id_rsa encryption-config.yaml ubuntu@<span class="k">${</span><span class="nv">external_ip</span><span class="k">}</span>:~/
<span class="k">done</span>
</pre></table></code></div></div><p><img data-src="/assets/img/2022/07/cloudshell_copy_encryption_config_to_controllers.png" alt="CloudShell Copy Encryption Configurations to Controllers" data-proofer-ignore></p><h2 id="bootstrapping-the-etcd-cluster"><span class="mr-2">Bootstrapping the etcd Cluster</span><a href="#bootstrapping-the-etcd-cluster" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>Let’s first return to our home directory:</p><div class="language-shell highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre><span class="nb">cd</span> ~/
</pre></table></code></div></div><p>Kubernetes components are stateless and store cluster state in <a href="https://github.com/etcd-io/etcd">etcd</a>. We are going to provision a three node etcd cluster and configure it for HA (High Availability) and secure remote access.</p><p>The next command will generate our SSH command line arguments to be able to connect to our controller instances:</p><div class="language-shell highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre><td class="rouge-code"><pre><span class="k">for </span>instance <span class="k">in </span>controller-0 controller-1 controller-2<span class="p">;</span> <span class="k">do
  </span><span class="nv">external_ip</span><span class="o">=</span><span class="si">$(</span>aws ec2 describe-instances <span class="nt">--filters</span> <span class="se">\</span>
    <span class="s2">"Name=tag:Name,Values=</span><span class="k">${</span><span class="nv">instance</span><span class="k">}</span><span class="s2">"</span> <span class="se">\</span>
    <span class="s2">"Name=instance-state-name,Values=running"</span> <span class="se">\</span>
    <span class="nt">--output</span> text <span class="nt">--query</span> <span class="s1">'Reservations[].Instances[].PublicIpAddress'</span><span class="si">)</span>
<span class="nb">echo </span>ssh <span class="nt">-i</span> kubernetes.id_rsa ubuntu@<span class="nv">$external_ip</span>
<span class="k">done</span>
</pre></table></code></div></div><p><img data-src="/assets/img/2022/07/cloudshell_instance_ssh.png" alt="CloudShell Instance SSH" data-proofer-ignore></p><p>You can now use tmux to create multiple panes and connect to each instance. I am using AWS CloudShell, so I will be creating separate rows:</p><p><img data-src="/assets/img/2022/07/cloudshell_tmux_1.png" alt="CloudShell TMUX" data-proofer-ignore></p><p><img data-src="/assets/img/2022/07/cloudshell_tmux_2.png" alt="CloudShell TMUX" data-proofer-ignore></p><p><img data-src="/assets/img/2022/07/cloudshell_tmux_3.png" alt="CloudShell TMUX" data-proofer-ignore></p><h2 id="bootstrapping-and-etcd-cluster-member"><span class="mr-2">Bootstrapping and etcd Cluster Member</span><a href="#bootstrapping-and-etcd-cluster-member" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>After logging in to each controller node, we now need to download the official etcd release binaries from the <a href="https://github.com/etcd-io/etcd">etcd GitHub</a> project (run this on each node):</p><div class="language-shell highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre><td class="rouge-code"><pre>wget <span class="nt">-q</span> <span class="nt">--timestamping</span> <span class="se">\</span>
  <span class="s2">"https://github.com/etcd-io/etcd/releases/download/v3.4.15/etcd-v3.4.15-linux-amd64.tar.gz"</span>
<span class="nb">tar</span> <span class="nt">-xvf</span> etcd-v3.4.15-linux-amd64.tar.gz
<span class="nb">sudo mv </span>etcd-v3.4.15-linux-amd64/etcd<span class="k">*</span> /usr/local/bin/
</pre></table></code></div></div><p><img data-src="/assets/img/2022/07/cloudshell_etcd_bootstrapping.png" alt="CloudShell etcd Bootstrapping" data-proofer-ignore></p><h2 id="configure-the-etcd-server"><span class="mr-2">Configure the etcd Server</span><a href="#configure-the-etcd-server" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>Next we create the necessary configuration folders and copy over the certificates and keys. We also get the internal IP address of the node to use in the configuration files. We also need to set a unique name within an etcd cluster. Remember to run this one each controller instance:</p><div class="language-shell highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre><td class="rouge-code"><pre><span class="nb">sudo mkdir</span> <span class="nt">-p</span> /etc/etcd /var/lib/etcd
<span class="nb">sudo chmod </span>700 /var/lib/etcd
<span class="nb">sudo cp </span>ca.pem kubernetes-key.pem kubernetes.pem /etc/etcd/
<span class="nv">INTERNAL_IP</span><span class="o">=</span><span class="si">$(</span>curl <span class="nt">-s</span> http://169.254.169.254/latest/meta-data/local-ipv4<span class="si">)</span>
<span class="nv">ETCD_NAME</span><span class="o">=</span><span class="si">$(</span>curl <span class="nt">-s</span> http://169.254.169.254/latest/user-data/ <span class="se">\</span>
  | <span class="nb">tr</span> <span class="s2">"|"</span> <span class="s2">"</span><span class="se">\n</span><span class="s2">"</span> | <span class="nb">grep</span> <span class="s2">"^name"</span> | <span class="nb">cut</span> <span class="nt">-d</span><span class="s2">"="</span> <span class="nt">-f2</span><span class="si">)</span>
<span class="nb">echo</span> <span class="s2">"</span><span class="k">${</span><span class="nv">ETCD_NAME</span><span class="k">}</span><span class="s2">"</span>
</pre></table></code></div></div><p><img data-src="/assets/img/2022/07/cloudshell_configure_etcd_server.png" alt="CloudShell Configure etcd Server" data-proofer-ignore></p><p>We want to encure etcd is started on each controller at boot time, so we need to create an etcd.service systemd unit file, as well as enable and start the etcd service (remember to run this on each controller node):</p><div class="language-shell highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
</pre><td class="rouge-code"><pre><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh"> | sudo tee /etc/systemd/system/etcd.service
[Unit]
Description=etcd
Documentation=https://github.com/coreos
[Service]
Type=notify
ExecStart=/usr/local/bin/etcd </span><span class="se">\\</span><span class="sh">
  --name </span><span class="k">${</span><span class="nv">ETCD_NAME</span><span class="k">}</span><span class="sh"> </span><span class="se">\\</span><span class="sh">
  --cert-file=/etc/etcd/kubernetes.pem </span><span class="se">\\</span><span class="sh">
  --key-file=/etc/etcd/kubernetes-key.pem </span><span class="se">\\</span><span class="sh">
  --peer-cert-file=/etc/etcd/kubernetes.pem </span><span class="se">\\</span><span class="sh">
  --peer-key-file=/etc/etcd/kubernetes-key.pem </span><span class="se">\\</span><span class="sh">
  --trusted-ca-file=/etc/etcd/ca.pem </span><span class="se">\\</span><span class="sh">
  --peer-trusted-ca-file=/etc/etcd/ca.pem </span><span class="se">\\</span><span class="sh">
  --peer-client-cert-auth </span><span class="se">\\</span><span class="sh">
  --client-cert-auth </span><span class="se">\\</span><span class="sh">
  --initial-advertise-peer-urls https://</span><span class="k">${</span><span class="nv">INTERNAL_IP</span><span class="k">}</span><span class="sh">:2380 </span><span class="se">\\</span><span class="sh">
  --listen-peer-urls https://</span><span class="k">${</span><span class="nv">INTERNAL_IP</span><span class="k">}</span><span class="sh">:2380 </span><span class="se">\\</span><span class="sh">
  --listen-client-urls https://</span><span class="k">${</span><span class="nv">INTERNAL_IP</span><span class="k">}</span><span class="sh">:2379,https://127.0.0.1:2379 </span><span class="se">\\</span><span class="sh">
  --advertise-client-urls https://</span><span class="k">${</span><span class="nv">INTERNAL_IP</span><span class="k">}</span><span class="sh">:2379 </span><span class="se">\\</span><span class="sh">
  --initial-cluster-token etcd-cluster-0 </span><span class="se">\\</span><span class="sh">
  --initial-cluster controller-0=https://10.0.1.10:2380,controller-1=https://10.0.1.11:2380,controller-2=https://10.0.1.12:2380 </span><span class="se">\\</span><span class="sh">
  --initial-cluster-state new </span><span class="se">\\</span><span class="sh">
  --data-dir=/var/lib/etcd
Restart=on-failure
RestartSec=5
[Install]
WantedBy=multi-user.target
</span><span class="no">EOF
</span><span class="nb">sudo </span>systemctl daemon-reload
<span class="nb">sudo </span>systemctl <span class="nb">enable </span>etcd
<span class="nb">sudo </span>systemctl start etcd
</pre></table></code></div></div><p><img data-src="/assets/img/2022/07/cloudshell_start_etcd.png" alt="CloudShell Start etcd Server" data-proofer-ignore></p><p>You can check the status of the <strong>etcd</strong> service by running:</p><div class="language-shell highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre><span class="nb">sudo </span>service etcd status
</pre></table></code></div></div><p>We can also verify the etcd cluster members by running:</p><div class="language-shell highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre><td class="rouge-code"><pre><span class="nb">sudo </span><span class="nv">ETCDCTL_API</span><span class="o">=</span>3 etcdctl member list <span class="se">\</span>
  <span class="nt">--endpoints</span><span class="o">=</span>https://127.0.0.1:2379 <span class="se">\</span>
  <span class="nt">--cacert</span><span class="o">=</span>/etc/etcd/ca.pem <span class="se">\</span>
  <span class="nt">--cert</span><span class="o">=</span>/etc/etcd/kubernetes.pem <span class="se">\</span>
  <span class="nt">--key</span><span class="o">=</span>/etc/etcd/kubernetes-key.pem
</pre></table></code></div></div><p><img data-src="/assets/img/2022/07/cloudshell_etcd_members.png" alt="CloudShell etcd Members" data-proofer-ignore></p><h2 id="bootstrapping-the-kubernetes-control-plane"><span class="mr-2">Bootstrapping the Kubernetes Control Plane</span><a href="#bootstrapping-the-kubernetes-control-plane" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>We will be installing the following components on each of our controller nodes: <a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/">Kubernetes API Server</a>, <a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kube-scheduler/">Scheduler</a>, and <a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kube-controller-manager/">Controller Manager</a>.</p><p>Ensure that you have logged into each of the controller nodes via SSH, you can get the SSH command list by running:</p><div class="language-shell highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre><td class="rouge-code"><pre><span class="k">for </span>instance <span class="k">in </span>controller-0 controller-1 controller-2<span class="p">;</span> <span class="k">do
  </span><span class="nv">external_ip</span><span class="o">=</span><span class="si">$(</span>aws ec2 describe-instances <span class="nt">--filters</span> <span class="se">\</span>
    <span class="s2">"Name=tag:Name,Values=</span><span class="k">${</span><span class="nv">instance</span><span class="k">}</span><span class="s2">"</span> <span class="se">\</span>
    <span class="s2">"Name=instance-state-name,Values=running"</span> <span class="se">\</span>
    <span class="nt">--output</span> text <span class="nt">--query</span> <span class="s1">'Reservations[].Instances[].PublicIpAddress'</span><span class="si">)</span>
<span class="nb">echo </span>ssh <span class="nt">-i</span> kubernetes.id_rsa ubuntu@<span class="nv">$external_ip</span>
<span class="k">done</span>
</pre></table></code></div></div><h2 id="provisioning-the-kubernetes-control-plane"><span class="mr-2">Provisioning the Kubernetes Control Plane</span><a href="#provisioning-the-kubernetes-control-plane" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>We will be creating our directory structure, as well as downloading and installing the binaries and copying over our certificates and keys that we need. We also determin the internal IP address of our node so we can use it in our configurations.</p><p>Remember to run this on each one of our controller nodes:</p><div class="language-shell highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
</pre><td class="rouge-code"><pre><span class="nb">sudo mkdir</span> <span class="nt">-p</span> /etc/kubernetes/config
<span class="nb">sudo mkdir</span> <span class="nt">-p</span> /var/lib/kubernetes/
<span class="nb">sudo mv </span>ca.pem ca-key.pem kubernetes-key.pem kubernetes.pem <span class="se">\</span>
  service-account-key.pem service-account.pem <span class="se">\</span>
  encryption-config.yaml /var/lib/kubernetes/
wget <span class="nt">-q</span> <span class="nt">--show-progress</span> <span class="nt">--https-only</span> <span class="nt">--timestamping</span> <span class="se">\</span>
  <span class="s2">"https://storage.googleapis.com/kubernetes-release/release/v1.21.0/bin/linux/amd64/kube-apiserver"</span> <span class="se">\</span>
  <span class="s2">"https://storage.googleapis.com/kubernetes-release/release/v1.21.0/bin/linux/amd64/kube-controller-manager"</span> <span class="se">\</span>
  <span class="s2">"https://storage.googleapis.com/kubernetes-release/release/v1.21.0/bin/linux/amd64/kube-scheduler"</span> <span class="se">\</span>
  <span class="s2">"https://storage.googleapis.com/kubernetes-release/release/v1.21.0/bin/linux/amd64/kubectl"</span>
<span class="nb">chmod</span> +x kube-apiserver kube-controller-manager kube-scheduler kubectl
<span class="nb">sudo mv </span>kube-apiserver kube-controller-manager kube-scheduler kubectl /usr/local/bin/
<span class="nv">INTERNAL_IP</span><span class="o">=</span><span class="si">$(</span>curl <span class="nt">-s</span> http://169.254.169.254/latest/meta-data/local-ipv4<span class="si">)</span>
</pre></table></code></div></div><p><img data-src="/assets/img/2022/07/cloudshell_provision_control_plane.png" alt="CloudShell Provision Control Plane" data-proofer-ignore></p><p>We want to encure <strong>kube-apiserver</strong> service is started on each controller at boot time, so we need to create an <strong>kube-apiserver.service</strong> systemd unit file (remember to run this on each controller node):</p><div class="language-shell highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
</pre><td class="rouge-code"><pre><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh"> | sudo tee /etc/systemd/system/kube-apiserver.service
[Unit]
Description=Kubernetes API Server
Documentation=https://github.com/kubernetes/kubernetes
[Service]
ExecStart=/usr/local/bin/kube-apiserver </span><span class="se">\\</span><span class="sh">
  --advertise-address=</span><span class="k">${</span><span class="nv">INTERNAL_IP</span><span class="k">}</span><span class="sh"> </span><span class="se">\\</span><span class="sh">
  --allow-privileged=true </span><span class="se">\\</span><span class="sh">
  --apiserver-count=3 </span><span class="se">\\</span><span class="sh">
  --audit-log-maxage=30 </span><span class="se">\\</span><span class="sh">
  --audit-log-maxbackup=3 </span><span class="se">\\</span><span class="sh">
  --audit-log-maxsize=100 </span><span class="se">\\</span><span class="sh">
  --audit-log-path=/var/log/audit.log </span><span class="se">\\</span><span class="sh">
  --authorization-mode=Node,RBAC </span><span class="se">\\</span><span class="sh">
  --bind-address=0.0.0.0 </span><span class="se">\\</span><span class="sh">
  --client-ca-file=/var/lib/kubernetes/ca.pem </span><span class="se">\\</span><span class="sh">
  --enable-admission-plugins=NamespaceLifecycle,NodeRestriction,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota </span><span class="se">\\</span><span class="sh">
  --etcd-cafile=/var/lib/kubernetes/ca.pem </span><span class="se">\\</span><span class="sh">
  --etcd-certfile=/var/lib/kubernetes/kubernetes.pem </span><span class="se">\\</span><span class="sh">
  --etcd-keyfile=/var/lib/kubernetes/kubernetes-key.pem </span><span class="se">\\</span><span class="sh">
  --etcd-servers=https://10.0.1.10:2379,https://10.0.1.11:2379,https://10.0.1.12:2379 </span><span class="se">\\</span><span class="sh">
  --event-ttl=1h </span><span class="se">\\</span><span class="sh">
  --encryption-provider-config=/var/lib/kubernetes/encryption-config.yaml </span><span class="se">\\</span><span class="sh">
  --kubelet-certificate-authority=/var/lib/kubernetes/ca.pem </span><span class="se">\\</span><span class="sh">
  --kubelet-client-certificate=/var/lib/kubernetes/kubernetes.pem </span><span class="se">\\</span><span class="sh">
  --kubelet-client-key=/var/lib/kubernetes/kubernetes-key.pem </span><span class="se">\\</span><span class="sh">
  --runtime-config='api/all=true' </span><span class="se">\\</span><span class="sh">
  --service-account-key-file=/var/lib/kubernetes/service-account.pem </span><span class="se">\\</span><span class="sh">
  --service-account-signing-key-file=/var/lib/kubernetes/service-account-key.pem </span><span class="se">\\</span><span class="sh">
  --service-account-issuer=https://</span><span class="k">${</span><span class="nv">KUBERNETES_PUBLIC_ADDRESS</span><span class="k">}</span><span class="sh">:443 </span><span class="se">\\</span><span class="sh">
  --service-cluster-ip-range=10.32.0.0/24 </span><span class="se">\\</span><span class="sh">
  --service-node-port-range=30000-32767 </span><span class="se">\\</span><span class="sh">
  --tls-cert-file=/var/lib/kubernetes/kubernetes.pem </span><span class="se">\\</span><span class="sh">
  --tls-private-key-file=/var/lib/kubernetes/kubernetes-key.pem </span><span class="se">\\</span><span class="sh">
  --v=2
Restart=on-failure
RestartSec=5
[Install]
WantedBy=multi-user.target
</span><span class="no">EOF
</span></pre></table></code></div></div><h2 id="configure-the-kubernetes-controller-manager"><span class="mr-2">Configure the Kubernetes Controller Manager</span><a href="#configure-the-kubernetes-controller-manager" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>Move the <strong>kube-controller-manager</strong> kubeconfig into place (remember to run this on each controller node):</p><div class="language-shell highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre><span class="nb">sudo mv </span>kube-controller-manager.kubeconfig /var/lib/kubernetes/
</pre></table></code></div></div><p>Create the <strong>kube-controller-manager.service</strong> systemd unit file (remember to run this on each controller node):</p><div class="language-shell highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
</pre><td class="rouge-code"><pre><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh"> | sudo tee /etc/systemd/system/kube-controller-manager.service
[Unit]
Description=Kubernetes Controller Manager
Documentation=https://github.com/kubernetes/kubernetes
[Service]
ExecStart=/usr/local/bin/kube-controller-manager </span><span class="se">\\</span><span class="sh">
  --bind-address=0.0.0.0 </span><span class="se">\\</span><span class="sh">
  --cluster-cidr=10.200.0.0/16 </span><span class="se">\\</span><span class="sh">
  --cluster-name=kubernetes </span><span class="se">\\</span><span class="sh">
  --cluster-signing-cert-file=/var/lib/kubernetes/ca.pem </span><span class="se">\\</span><span class="sh">
  --cluster-signing-key-file=/var/lib/kubernetes/ca-key.pem </span><span class="se">\\</span><span class="sh">
  --kubeconfig=/var/lib/kubernetes/kube-controller-manager.kubeconfig </span><span class="se">\\</span><span class="sh">
  --leader-elect=true </span><span class="se">\\</span><span class="sh">
  --root-ca-file=/var/lib/kubernetes/ca.pem </span><span class="se">\\</span><span class="sh">
  --service-account-private-key-file=/var/lib/kubernetes/service-account-key.pem </span><span class="se">\\</span><span class="sh">
  --service-cluster-ip-range=10.32.0.0/24 </span><span class="se">\\</span><span class="sh">
  --use-service-account-credentials=true </span><span class="se">\\</span><span class="sh">
  --v=2
Restart=on-failure
RestartSec=5
[Install]
WantedBy=multi-user.target
</span><span class="no">EOF
</span></pre></table></code></div></div><h2 id="configure-the-kubernetes-scheduler"><span class="mr-2">Configure the Kubernetes Scheduler</span><a href="#configure-the-kubernetes-scheduler" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>Move the <strong>kube-scheduler</strong> kubeconfig into place (remember to run this on each controller node):</p><div class="language-shell highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre><span class="nb">sudo mv </span>kube-scheduler.kubeconfig /var/lib/kubernetes/
</pre></table></code></div></div><p>Create the <strong>kube-scheuduler.yaml</strong> configuration file:</p><div class="language-shell highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
</pre><td class="rouge-code"><pre><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh"> | sudo tee /etc/kubernetes/config/kube-scheduler.yaml
apiVersion: kubescheduler.config.k8s.io/v1beta1
kind: KubeSchedulerConfiguration
clientConnection:
  kubeconfig: "/var/lib/kubernetes/kube-scheduler.kubeconfig"
leaderElection:
  leaderElect: true
</span><span class="no">EOF
</span></pre></table></code></div></div><p>Create the <strong>kube-scheduler.service</strong> systemd unit file:</p><div class="language-shell highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
</pre><td class="rouge-code"><pre><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh"> | sudo tee /etc/systemd/system/kube-scheduler.service
[Unit]
Description=Kubernetes Scheduler
Documentation=https://github.com/kubernetes/kubernetes
[Service]
ExecStart=/usr/local/bin/kube-scheduler </span><span class="se">\\</span><span class="sh">
  --config=/etc/kubernetes/config/kube-scheduler.yaml </span><span class="se">\\</span><span class="sh">
  --v=2
Restart=on-failure
RestartSec=5
[Install]
WantedBy=multi-user.target
</span><span class="no">EOF
</span></pre></table></code></div></div><h2 id="enable-start-on-boot-and-start-the-controller-services"><span class="mr-2">Enable start-on-boot and Start the Controller Services</span><a href="#enable-start-on-boot-and-start-the-controller-services" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>Remember to run this on each controller node:</p><div class="language-shell highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre><td class="rouge-code"><pre><span class="nb">sudo </span>systemctl daemon-reload
<span class="nb">sudo </span>systemctl <span class="nb">enable </span>kube-apiserver kube-controller-manager kube-scheduler
<span class="nb">sudo </span>systemctl start kube-apiserver kube-controller-manager kube-scheduler
</pre></table></code></div></div><h2 id="add-host-file-entries"><span class="mr-2">Add Host File Entries</span><a href="#add-host-file-entries" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>In order for <strong>kubectl exec</strong> commands to work, the controller nodes must each be able to resolve the worker hostnames. This is not set up by default in AWS. The workaround is to add manual host entries on each of the controller nodes:</p><div class="language-shell highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre><td class="rouge-code"><pre><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh"> | sudo tee -a /etc/hosts
10.0.1.20 ip-10-0-1-20
10.0.1.21 ip-10-0-1-21
10.0.1.22 ip-10-0-1-22
</span><span class="no">EOF
</span></pre></table></code></div></div><p>If this step is missed, the DNS Cluster Add-on testing will fail with an error like:</p><div class="language-shell highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>Error from server: error dialing backend: dial tcp: lookup ip-10-0-1-22 on 127.0.0.53:53: server misbehaving
</pre></table></code></div></div><h2 id="verify-that-you-can-access-the-kubernetes-cluster-on-the-control-plane-nodes"><span class="mr-2">Verify that you can access the Kubernetes cluster on the control plane nodes</span><a href="#verify-that-you-can-access-the-kubernetes-cluster-on-the-control-plane-nodes" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><div class="language-shell highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>kubectl cluster-info <span class="nt">--kubeconfig</span> admin.kubeconfig
</pre></table></code></div></div><p><img data-src="/assets/img/2022/07/cloudshell_cluster_verify.png" alt="CloudShell Cluster Verify" data-proofer-ignore></p><h2 id="rbac-for-kubelet-authorization"><span class="mr-2">RBAC For Kubelet Authorization</span><a href="#rbac-for-kubelet-authorization" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>We need to setup roles and permissions for the Kubernetes API Server to access the Kubelet API on each worker node. Access to the Kubelet API is required for retrieving metrics, logs and executing commands in pods.</p><p>The commands in this section will affect the entire cluster and only need to be run once from one of the controller nodes, the command below will print out the SSH command that you should use to connect to an instance:</p><div class="language-shell highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre><td class="rouge-code"><pre><span class="nv">external_ip</span><span class="o">=</span><span class="si">$(</span>aws ec2 describe-instances <span class="nt">--filters</span> <span class="se">\</span>
    <span class="s2">"Name=tag:Name,Values=controller-0"</span> <span class="se">\</span>
    <span class="s2">"Name=instance-state-name,Values=running"</span> <span class="se">\</span>
    <span class="nt">--output</span> text <span class="nt">--query</span> <span class="s1">'Reservations[].Instances[].PublicIpAddress'</span><span class="si">)</span>
ssh <span class="nt">-i</span> kubernetes.id_rsa ubuntu@<span class="k">${</span><span class="nv">external_ip</span><span class="k">}</span>
</pre></table></code></div></div><p>Create the <strong>system:kube-apiserver-to-kubelet</strong> ClusterRole with permissions to acccess the Kubelet API and perform some common tasks associated with managing pods:</p><div class="language-shell highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
</pre><td class="rouge-code"><pre><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh"> | kubectl apply --kubeconfig admin.kubeconfig -f -
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  annotations:
    rbac.authorization.kubernetes.io/autoupdate: "true"
  labels:
    kubernetes.io/bootstrapping: rbac-defaults
  name: system:kube-apiserver-to-kubelet
rules:
  - apiGroups:
      - ""
    resources:
      - nodes/proxy
      - nodes/stats
      - nodes/log
      - nodes/spec
      - nodes/metrics
    verbs:
      - "*"
</span><span class="no">EOF
</span></pre></table></code></div></div><p><img data-src="/assets/img/2022/07/cloudshell_create_clusterrole.png" alt="CloudShell Create Cluster Roles" data-proofer-ignore></p><p>Bind the <strong>system:kube-apiserver-to-kubelet</strong> ClusterRole to the <strong>kubernetes</strong> user:</p><div class="language-shell highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
</pre><td class="rouge-code"><pre><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh"> | kubectl apply --kubeconfig admin.kubeconfig -f -
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: system:kube-apiserver
  namespace: ""
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:kube-apiserver-to-kubelet
subjects:
  - apiGroup: rbac.authorization.k8s.io
    kind: User
    name: kubernetes
</span><span class="no">EOF
</span></pre></table></code></div></div><p><img data-src="/assets/img/2022/07/cloudshell_bind_clusterrole.png" alt="CloudShell Bind Cluster Role" data-proofer-ignore></p><h2 id="verify-the-cluster-public-endpoint"><span class="mr-2">Verify the cluster public endpoint</span><a href="#verify-the-cluster-public-endpoint" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>Log out of the SSH connection and head back to the main terminal window as you need to execute AWS commands to verify the public endpoint remotely:</p><div class="language-shell highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre><td class="rouge-code"><pre><span class="nv">KUBERNETES_PUBLIC_ADDRESS</span><span class="o">=</span><span class="si">$(</span>aws elbv2 describe-load-balancers <span class="se">\</span>
  <span class="nt">--load-balancer-arns</span> <span class="k">${</span><span class="nv">LOAD_BALANCER_ARN</span><span class="k">}</span> <span class="se">\</span>
  <span class="nt">--output</span> text <span class="nt">--query</span> <span class="s1">'LoadBalancers[].DNSName'</span><span class="si">)</span>
curl <span class="nt">--cacert</span> certs/ca.pem https://<span class="k">${</span><span class="nv">KUBERNETES_PUBLIC_ADDRESS</span><span class="k">}</span>/version
</pre></table></code></div></div><p><img data-src="/assets/img/2022/07/cloudshell_verify_cluster_public_endpoints.png" alt="CloudShel Verify Cluster Public Endpoints" data-proofer-ignore></p><h2 id="bootstrapping-the-kubernetes-worker-nodes"><span class="mr-2">Bootstrapping the Kubernetes Worker Nodes</span><a href="#bootstrapping-the-kubernetes-worker-nodes" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>The following components will be installed on each node: <a href="https://github.com/opencontainers/runc">runc</a>, <a href="https://github.com/containernetworking/cni">container networking plugins</a>, <a href="https://github.com/containerd/containerd">containerd</a>, <a href="https://kubernetes.io/docs/admin/kubelet">kubelet</a>, and <a href="https://kubernetes.io/docs/concepts/cluster-administration/proxies">kube-proxy</a>.</p><p>The commands need to be run on each worker instance. Let’s generate our SSH command list:</p><div class="language-shell highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre><td class="rouge-code"><pre><span class="k">for </span>instance <span class="k">in </span>worker-0 worker-1 worker-2<span class="p">;</span> <span class="k">do
  </span><span class="nv">external_ip</span><span class="o">=</span><span class="si">$(</span>aws ec2 describe-instances <span class="nt">--filters</span> <span class="se">\</span>
    <span class="s2">"Name=tag:Name,Values=</span><span class="k">${</span><span class="nv">instance</span><span class="k">}</span><span class="s2">"</span> <span class="se">\</span>
    <span class="s2">"Name=instance-state-name,Values=running"</span> <span class="se">\</span>
    <span class="nt">--output</span> text <span class="nt">--query</span> <span class="s1">'Reservations[].Instances[].PublicIpAddress'</span><span class="si">)</span>
<span class="nb">echo </span>ssh <span class="nt">-i</span> kubernetes.id_rsa ubuntu@<span class="nv">$external_ip</span>
<span class="k">done</span>
</pre></table></code></div></div><p><img data-src="/assets/img/2022/07/cloudshell_bootstrap_workers_ssh.png" alt="CloudShell Bootstrap Workers SSH" data-proofer-ignore></p><p>SSH into each instance in a separate pane.</p><p><img data-src="/assets/img/2022/07/cloudshell_bootstrap_workers_ssh_pane.png" alt="CloudShell Bootstrap Workers SSH" data-proofer-ignore></p><h2 id="provision-a-kubernetes-worker-node"><span class="mr-2">Provision a Kubernetes Worker Node</span><a href="#provision-a-kubernetes-worker-node" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>Install the OS dependencies, disable swap and download and install the worker binaries:</p><div class="language-shell highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
</pre><td class="rouge-code"><pre><span class="nb">sudo </span>apt-get update
<span class="nb">sudo </span>apt-get <span class="nt">-y</span> <span class="nb">install </span>socat conntrack ipset
<span class="nb">sudo </span>swapon <span class="nt">--show</span>
<span class="nb">sudo </span>swapoff <span class="nt">-a</span>
wget <span class="nt">-q</span> <span class="nt">--show-progress</span> <span class="nt">--https-only</span> <span class="nt">--timestamping</span> <span class="se">\</span>
  https://github.com/kubernetes-sigs/cri-tools/releases/download/v1.21.0/crictl-v1.21.0-linux-amd64.tar.gz <span class="se">\</span>
  https://github.com/opencontainers/runc/releases/download/v1.0.0-rc93/runc.amd64 <span class="se">\</span>
  https://github.com/containernetworking/plugins/releases/download/v0.9.1/cni-plugins-linux-amd64-v0.9.1.tgz <span class="se">\</span>
  https://github.com/containerd/containerd/releases/download/v1.4.4/containerd-1.4.4-linux-amd64.tar.gz <span class="se">\</span>
  https://storage.googleapis.com/kubernetes-release/release/v1.21.0/bin/linux/amd64/kubectl <span class="se">\</span>
  https://storage.googleapis.com/kubernetes-release/release/v1.21.0/bin/linux/amd64/kube-proxy <span class="se">\</span>
  https://storage.googleapis.com/kubernetes-release/release/v1.21.0/bin/linux/amd64/kubelet
<span class="nb">sudo mkdir</span> <span class="nt">-p</span> <span class="se">\</span>
  /etc/cni/net.d <span class="se">\</span>
  /opt/cni/bin <span class="se">\</span>
  /var/lib/kubelet <span class="se">\</span>
  /var/lib/kube-proxy <span class="se">\</span>
  /var/lib/kubernetes <span class="se">\</span>
  /var/run/kubernetes
<span class="nb">mkdir </span>containerd
<span class="nb">tar</span> <span class="nt">-xvf</span> crictl-v1.21.0-linux-amd64.tar.gz
<span class="nb">tar</span> <span class="nt">-xvf</span> containerd-1.4.4-linux-amd64.tar.gz <span class="nt">-C</span> containerd
<span class="nb">sudo tar</span> <span class="nt">-xvf</span> cni-plugins-linux-amd64-v0.9.1.tgz <span class="nt">-C</span> /opt/cni/bin/
<span class="nb">sudo mv </span>runc.amd64 runc
<span class="nb">chmod</span> +x crictl kubectl kube-proxy kubelet runc 
<span class="nb">sudo mv </span>crictl kubectl kube-proxy kubelet runc /usr/local/bin/
<span class="nb">sudo mv </span>containerd/bin/<span class="k">*</span> /bin/
</pre></table></code></div></div><h2 id="configure-the-cni-networking"><span class="mr-2">Configure the CNI Networking</span><a href="#configure-the-cni-networking" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>Retrieve the Pod CIDR range for teh current compute instance. Create a network bridge and loopback configuration (remember to run on each worker node):</p><div class="language-shell highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
</pre><td class="rouge-code"><pre><span class="nv">POD_CIDR</span><span class="o">=</span><span class="si">$(</span>curl <span class="nt">-s</span> http://169.254.169.254/latest/user-data/ <span class="se">\</span>
  | <span class="nb">tr</span> <span class="s2">"|"</span> <span class="s2">"</span><span class="se">\n</span><span class="s2">"</span> | <span class="nb">grep</span> <span class="s2">"^pod-cidr"</span> | <span class="nb">cut</span> <span class="nt">-d</span><span class="s2">"="</span> <span class="nt">-f2</span><span class="si">)</span>
<span class="nb">echo</span> <span class="s2">"</span><span class="k">${</span><span class="nv">POD_CIDR</span><span class="k">}</span><span class="s2">"</span>
<span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh"> | sudo tee /etc/cni/net.d/10-bridge.conf
{
    "cniVersion": "0.4.0",
    "name": "bridge",
    "type": "bridge",
    "bridge": "cnio0",
    "isGateway": true,
    "ipMasq": true,
    "ipam": {
        "type": "host-local",
        "ranges": [
          [{"subnet": "</span><span class="k">${</span><span class="nv">POD_CIDR</span><span class="k">}</span><span class="sh">"}]
        ],
        "routes": [{"dst": "0.0.0.0/0"}]
    }
}
</span><span class="no">EOF
</span><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh"> | sudo tee /etc/cni/net.d/99-loopback.conf
{
    "cniVersion": "0.4.0",
    "name": "lo",
    "type": "loopback"
}
</span><span class="no">EOF
</span></pre></table></code></div></div><h2 id="configure-containerd"><span class="mr-2">Configure containerd</span><a href="#configure-containerd" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>Create a <strong>containerd</strong> configuration file and create the <strong>containerd.service</strong> systemd unit file (remember to run on each worker node):</p><div class="language-shell highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
</pre><td class="rouge-code"><pre><span class="nb">sudo mkdir</span> <span class="nt">-p</span> /etc/containerd/
<span class="nb">cat</span> <span class="o">&lt;&lt;</span> <span class="no">EOF</span><span class="sh"> | sudo tee /etc/containerd/config.toml
[plugins]
  [plugins.cri.containerd]
    snapshotter = "overlayfs"
    [plugins.cri.containerd.default_runtime]
      runtime_type = "io.containerd.runtime.v1.linux"
      runtime_engine = "/usr/local/bin/runc"
      runtime_root = ""
</span><span class="no">EOF
</span><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh"> | sudo tee /etc/systemd/system/containerd.service
[Unit]
Description=containerd container runtime
Documentation=https://containerd.io
After=network.target
[Service]
ExecStartPre=/sbin/modprobe overlay
ExecStart=/bin/containerd
Restart=always
RestartSec=5
Delegate=yes
KillMode=process
OOMScoreAdjust=-999
LimitNOFILE=1048576
LimitNPROC=infinity
LimitCORE=infinity
[Install]
WantedBy=multi-user.target
</span><span class="no">EOF
</span></pre></table></code></div></div><h1 id="configure-the-kubelet">Configure the Kubelet</h1><p>Move and store the certificate and keys and create the necessary folders and configuration (remember to run on each worker node):</p><div class="language-shell highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
</pre><td class="rouge-code"><pre><span class="nv">WORKER_NAME</span><span class="o">=</span><span class="si">$(</span>curl <span class="nt">-s</span> http://169.254.169.254/latest/user-data/ <span class="se">\</span>
| <span class="nb">tr</span> <span class="s2">"|"</span> <span class="s2">"</span><span class="se">\n</span><span class="s2">"</span> | <span class="nb">grep</span> <span class="s2">"^name"</span> | <span class="nb">cut</span> <span class="nt">-d</span><span class="s2">"="</span> <span class="nt">-f2</span><span class="si">)</span>
<span class="nb">echo</span> <span class="s2">"</span><span class="k">${</span><span class="nv">WORKER_NAME</span><span class="k">}</span><span class="s2">"</span>
<span class="nb">sudo mv</span> <span class="k">${</span><span class="nv">WORKER_NAME</span><span class="k">}</span><span class="nt">-key</span>.pem <span class="k">${</span><span class="nv">WORKER_NAME</span><span class="k">}</span>.pem /var/lib/kubelet/
<span class="nb">sudo mv</span> <span class="k">${</span><span class="nv">WORKER_NAME</span><span class="k">}</span>.kubeconfig /var/lib/kubelet/kubeconfig
<span class="nb">sudo mv </span>ca.pem /var/lib/kubernetes/
<span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh"> | sudo tee /var/lib/kubelet/kubelet-config.yaml
kind: KubeletConfiguration
apiVersion: kubelet.config.k8s.io/v1beta1
authentication:
  anonymous:
    enabled: false
  webhook:
    enabled: true
  x509:
    clientCAFile: "/var/lib/kubernetes/ca.pem"
authorization:
  mode: Webhook
clusterDomain: "cluster.local"
clusterDNS:
  - "10.32.0.10"
podCIDR: "</span><span class="k">${</span><span class="nv">POD_CIDR</span><span class="k">}</span><span class="sh">"
resolvConf: "/run/systemd/resolve/resolv.conf"
runtimeRequestTimeout: "15m"
tlsCertFile: "/var/lib/kubelet/</span><span class="k">${</span><span class="nv">WORKER_NAME</span><span class="k">}</span><span class="sh">.pem"
tlsPrivateKeyFile: "/var/lib/kubelet/</span><span class="k">${</span><span class="nv">WORKER_NAME</span><span class="k">}</span><span class="sh">-key.pem"
</span><span class="no">EOF
</span></pre></table></code></div></div><p>The <strong>resolveConf</strong> configuration is used to avoid loops when using CoreDNS for service discovery on systems running <strong>systemd-resolved</strong>.</p><p>Create the <strong>kubelet.service</strong> systemd unit file (remember to run on each worker node):</p><div class="language-shell highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
</pre><td class="rouge-code"><pre><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh"> | sudo tee /etc/systemd/system/kubelet.service
[Unit]
Description=Kubernetes Kubelet
Documentation=https://github.com/kubernetes/kubernetes
After=containerd.service
Requires=containerd.service
[Service]
ExecStart=/usr/local/bin/kubelet </span><span class="se">\\</span><span class="sh">
  --config=/var/lib/kubelet/kubelet-config.yaml </span><span class="se">\\</span><span class="sh">
  --container-runtime=remote </span><span class="se">\\</span><span class="sh">
  --container-runtime-endpoint=unix:///var/run/containerd/containerd.sock </span><span class="se">\\</span><span class="sh">
  --image-pull-progress-deadline=2m </span><span class="se">\\</span><span class="sh">
  --kubeconfig=/var/lib/kubelet/kubeconfig </span><span class="se">\\</span><span class="sh">
  --network-plugin=cni </span><span class="se">\\</span><span class="sh">
  --register-node=true </span><span class="se">\\</span><span class="sh">
  --v=2
Restart=on-failure
RestartSec=5
[Install]
WantedBy=multi-user.target
</span><span class="no">EOF
</span></pre></table></code></div></div><h2 id="configure-the-kubernetes-proxy"><span class="mr-2">Configure the Kubernetes Proxy</span><a href="#configure-the-kubernetes-proxy" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>Move the <strong>kube-proxy</strong> kubeconfig into place, create the kube-proxy-config and finally create the <strong>kube-proxy.service</strong> systemd unit file (remember to run on each worker node):</p><div class="language-shell highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
</pre><td class="rouge-code"><pre><span class="nb">sudo mv </span>kube-proxy.kubeconfig /var/lib/kube-proxy/kubeconfig
<span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh"> | sudo tee /var/lib/kube-proxy/kube-proxy-config.yaml
kind: KubeProxyConfiguration
apiVersion: kubeproxy.config.k8s.io/v1alpha1
clientConnection:
  kubeconfig: "/var/lib/kube-proxy/kubeconfig"
mode: "iptables"
clusterCIDR: "10.200.0.0/16"
</span><span class="no">EOF
</span><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh"> | sudo tee /etc/systemd/system/kube-proxy.service
[Unit]
Description=Kubernetes Kube Proxy
Documentation=https://github.com/kubernetes/kubernetes
[Service]
ExecStart=/usr/local/bin/kube-proxy </span><span class="se">\\</span><span class="sh">
  --config=/var/lib/kube-proxy/kube-proxy-config.yaml
Restart=on-failure
RestartSec=5
[Install]
WantedBy=multi-user.target
</span><span class="no">EOF
</span></pre></table></code></div></div><h2 id="enable-start-at-boot-and-start-the-worker-services"><span class="mr-2">Enable start-at-boot and Start the Worker Services</span><a href="#enable-start-at-boot-and-start-the-worker-services" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>Remember to run on each worker node:</p><div class="language-shell highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre><td class="rouge-code"><pre><span class="nb">sudo </span>systemctl daemon-reload
<span class="nb">sudo </span>systemctl <span class="nb">enable </span>containerd kubelet kube-proxy
<span class="nb">sudo </span>systemctl start containerd kubelet kube-proxy
</pre></table></code></div></div><h2 id="verify-all-is-working"><span class="mr-2">Verify all is working</span><a href="#verify-all-is-working" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>Switch back to you terminal that has the AWS permissions (not the logged in worker node) and run:</p><div class="language-shell highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre><td class="rouge-code"><pre><span class="nv">external_ip</span><span class="o">=</span><span class="si">$(</span>aws ec2 describe-instances <span class="nt">--filters</span> <span class="se">\</span>
    <span class="s2">"Name=tag:Name,Values=controller-0"</span> <span class="se">\</span>
    <span class="s2">"Name=instance-state-name,Values=running"</span> <span class="se">\</span>
    <span class="nt">--output</span> text <span class="nt">--query</span> <span class="s1">'Reservations[].Instances[].PublicIpAddress'</span><span class="si">)</span>
ssh <span class="nt">-i</span> kubernetes.id_rsa ubuntu@<span class="k">${</span><span class="nv">external_ip</span><span class="k">}</span> kubectl get nodes <span class="nt">--kubeconfig</span> admin.kubeconfig
</pre></table></code></div></div><p><img data-src="/assets/img/2022/07/cloudshell_verify_nodes.png" alt="CloudShell Verify Nodes" data-proofer-ignore></p><p>We can see the 3 worker nodes up and running and are <strong>Ready</strong>.</p><h2 id="configuring-kubectl-for-remote-access"><span class="mr-2">Configuring kubectl for Remote Access</span><a href="#configuring-kubectl-for-remote-access" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>We will generate a kubeconfig file for the <strong>kubectl</strong> command line utility based on the <strong>admin</strong> user credentials.</p><p>Each kubeconfig requires a Kubernetes API Server to connect to. To support HA (High Availability), the DNS name of the external load balancer fronting the Kubernetes API servers will be used.</p><p>Generate a kubeconfig file suitable for authenticating as the <strong>admin</strong> user:</p><div class="language-shell highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
</pre><td class="rouge-code"><pre><span class="nb">cd</span> ~
<span class="nv">KUBERNETES_PUBLIC_ADDRESS</span><span class="o">=</span><span class="si">$(</span>aws elbv2 describe-load-balancers <span class="se">\</span>
<span class="nt">--load-balancer-arns</span> <span class="k">${</span><span class="nv">LOAD_BALANCER_ARN</span><span class="k">}</span> <span class="se">\</span>
<span class="nt">--output</span> text <span class="nt">--query</span> <span class="s1">'LoadBalancers[].DNSName'</span><span class="si">)</span>
kubectl config set-cluster kubernetes-from-scratch <span class="se">\</span>
  <span class="nt">--certificate-authority</span><span class="o">=</span>certs/ca.pem <span class="se">\</span>
  <span class="nt">--embed-certs</span><span class="o">=</span><span class="nb">true</span> <span class="se">\</span>
  <span class="nt">--server</span><span class="o">=</span>https://<span class="k">${</span><span class="nv">KUBERNETES_PUBLIC_ADDRESS</span><span class="k">}</span>:443
kubectl config set-credentials admin <span class="se">\</span>
  <span class="nt">--client-certificate</span><span class="o">=</span>certs/admin.pem <span class="se">\</span>
  <span class="nt">--client-key</span><span class="o">=</span>certs/admin-key.pem
kubectl config set-context kubernetes-from-scratch <span class="se">\</span>
  <span class="nt">--cluster</span><span class="o">=</span>kubernetes-from-scratch <span class="se">\</span>
  <span class="nt">--user</span><span class="o">=</span>admin
kubectl config use-context kubernetes-from-scratch
</pre></table></code></div></div><h2 id="verify-all-is-good"><span class="mr-2">Verify all is good</span><a href="#verify-all-is-good" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>Check the version of teh remote Kubernetes cluster:</p><div class="language-shell highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre><td class="rouge-code"><pre>kubectl version
kubectl get nodes
kubectl config view
</pre></table></code></div></div><p><img data-src="/assets/img/2022/07/cloudshell_cluster_verify_detail.png" alt="CloudShell Cluster Verify" data-proofer-ignore></p><h2 id="provisioning-pod-network-routes"><span class="mr-2">Provisioning Pod Network Routes</span><a href="#provisioning-pod-network-routes" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>Pods scheduled to a node receive an IP address from the node’s Pod CIDR range. At this point pods can not communicate with other pods running on different nodes due to missing network <a href="https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Route_Tables.html">routes</a>.</p><p>We will create a route for each worker node that maps the node’s Pod CIDR range to the node’s internal IP address.</p><p>However in production workloads, this functionality will be provided by CNI plugins like flannel, calico, amazon-vpc-cni-k8s. Doing this by hand makes it easier to understand what those plugins do behind the scenes.</p><p>Print the internal IP address and Pod CIDR range for each worker instance and create route table entries:</p><div class="language-shell highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
</pre><td class="rouge-code"><pre><span class="k">for </span>instance <span class="k">in </span>worker-0 worker-1 worker-2<span class="p">;</span> <span class="k">do
  </span><span class="nv">instance_id_ip</span><span class="o">=</span><span class="s2">"</span><span class="si">$(</span>aws ec2 describe-instances <span class="se">\</span>
    <span class="nt">--filters</span> <span class="s2">"Name=tag:Name,Values=</span><span class="k">${</span><span class="nv">instance</span><span class="k">}</span><span class="s2">"</span> <span class="se">\</span>
    <span class="nt">--output</span> text <span class="nt">--query</span> <span class="s1">'Reservations[].Instances[].[InstanceId,PrivateIpAddress]'</span><span class="si">)</span><span class="s2">"</span>
  <span class="nv">instance_id</span><span class="o">=</span><span class="s2">"</span><span class="si">$(</span><span class="nb">echo</span> <span class="s2">"</span><span class="k">${</span><span class="nv">instance_id_ip</span><span class="k">}</span><span class="s2">"</span> | <span class="nb">cut</span> <span class="nt">-f1</span><span class="si">)</span><span class="s2">"</span>
  <span class="nv">instance_ip</span><span class="o">=</span><span class="s2">"</span><span class="si">$(</span><span class="nb">echo</span> <span class="s2">"</span><span class="k">${</span><span class="nv">instance_id_ip</span><span class="k">}</span><span class="s2">"</span> | <span class="nb">cut</span> <span class="nt">-f2</span><span class="si">)</span><span class="s2">"</span>
  <span class="nv">pod_cidr</span><span class="o">=</span><span class="s2">"</span><span class="si">$(</span>aws ec2 describe-instance-attribute <span class="se">\</span>
    <span class="nt">--instance-id</span> <span class="s2">"</span><span class="k">${</span><span class="nv">instance_id</span><span class="k">}</span><span class="s2">"</span> <span class="se">\</span>
    <span class="nt">--attribute</span> userData <span class="se">\</span>
    <span class="nt">--output</span> text <span class="nt">--query</span> <span class="s1">'UserData.Value'</span> <span class="se">\</span>
    | <span class="nb">base64</span> <span class="nt">--decode</span> | <span class="nb">tr</span> <span class="s2">"|"</span> <span class="s2">"</span><span class="se">\n</span><span class="s2">"</span> | <span class="nb">grep</span> <span class="s2">"^pod-cidr"</span> | <span class="nb">cut</span> <span class="nt">-d</span><span class="s1">'='</span> <span class="nt">-f2</span><span class="si">)</span><span class="s2">"</span>
  <span class="nb">echo</span> <span class="s2">"</span><span class="k">${</span><span class="nv">instance_ip</span><span class="k">}</span><span class="s2"> </span><span class="k">${</span><span class="nv">pod_cidr</span><span class="k">}</span><span class="s2">"</span>
aws ec2 create-route <span class="se">\</span>
    <span class="nt">--route-table-id</span> <span class="s2">"</span><span class="k">${</span><span class="nv">ROUTE_TABLE_ID</span><span class="k">}</span><span class="s2">"</span> <span class="se">\</span>
    <span class="nt">--destination-cidr-block</span> <span class="s2">"</span><span class="k">${</span><span class="nv">pod_cidr</span><span class="k">}</span><span class="s2">"</span> <span class="se">\</span>
    <span class="nt">--instance-id</span> <span class="s2">"</span><span class="k">${</span><span class="nv">instance_id</span><span class="k">}</span><span class="s2">"</span>
<span class="k">done</span>
</pre></table></code></div></div><p><img data-src="/assets/img/2022/07/cloudshell_provision_pod_network_routes.png" alt="CloudShell Provision POD Network Routes" data-proofer-ignore></p><h2 id="validate-routes"><span class="mr-2">Validate Routes</span><a href="#validate-routes" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>Validate network routes for each worker instance:</p><div class="language-shell highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre><td class="rouge-code"><pre>aws ec2 describe-route-tables <span class="se">\</span>
  <span class="nt">--route-table-ids</span> <span class="s2">"</span><span class="k">${</span><span class="nv">ROUTE_TABLE_ID</span><span class="k">}</span><span class="s2">"</span> <span class="se">\</span>
  <span class="nt">--query</span> <span class="s1">'RouteTables[].Routes'</span>
</pre></table></code></div></div><p><img data-src="/assets/img/2022/07/cloudshell_validate_network_routes.png" alt="CloudShell Validate Network Routes" data-proofer-ignore></p><h2 id="deploying-the-dns-cluster-add-on"><span class="mr-2">Deploying the DNS Cluster Add-on</span><a href="#deploying-the-dns-cluster-add-on" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>We will now deploy the DNS add-on which provides DNS based service discovery, backed by CoreDNS, to applications running inside the Kubernetes cluster.</p><p>Deploy the <strong>coredns</strong> cluster add-on:</p><div class="language-shell highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>kubectl apply <span class="nt">-f</span> https://storage.googleapis.com/kubernetes-the-hard-way/coredns-1.8.yaml
</pre></table></code></div></div><p><img data-src="/assets/img/2022/07/cloudshell_deploy_coredns.png" alt="CloudShell Deploy Core DNS" data-proofer-ignore></p><p>List pods created by the <strong>core-dns</strong> deployment:</p><div class="language-shell highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>kubectl get pods <span class="nt">-l</span> k8s-app<span class="o">=</span>kube-dns <span class="nt">-n</span> kube-system
</pre></table></code></div></div><p><img data-src="/assets/img/2022/07/cloudshell_coredns_pods.png" alt="CloudShell CoreDNS Pods" data-proofer-ignore></p><h2 id="verify-all-is-ok"><span class="mr-2">Verify all is ok</span><a href="#verify-all-is-ok" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>Create a <strong>busybox</strong> deployment and then list the pods:</p><div class="language-shell highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre><td class="rouge-code"><pre>kubectl run busybox <span class="nt">--image</span><span class="o">=</span>busybox:1.28 <span class="nt">--command</span> <span class="nt">--</span> <span class="nb">sleep </span>3600
kubectl get pods <span class="nt">-l</span> <span class="nv">run</span><span class="o">=</span>busybox
</pre></table></code></div></div><p><img data-src="/assets/img/2022/07/cloudshell_create_busybox.png" alt="CloudShell Create BusyBox Pod" data-proofer-ignore></p><p>Retrieve the full name of the <strong>busybox</strong> pod and perform a DNS lookup for the <strong>kubernetes</strong> service inside the <strong>busybox</strong> pod:</p><div class="language-shell highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre><td class="rouge-code"><pre><span class="nv">POD_NAME</span><span class="o">=</span><span class="si">$(</span>kubectl get pods <span class="nt">-l</span> <span class="nv">run</span><span class="o">=</span>busybox <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s2">"{.items[0].metadata.name}"</span><span class="si">)</span>
kubectl <span class="nb">exec</span> <span class="nt">-ti</span> <span class="nv">$POD_NAME</span> <span class="nt">--</span> nslookup kubernetes
</pre></table></code></div></div><p><img data-src="/assets/img/2022/07/cloudshell_busybox_dns_test.png" alt="CloudShell BusyBox DNS Test" data-proofer-ignore></p><h2 id="smoke-test"><span class="mr-2">Smoke Test</span><a href="#smoke-test" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><h3 id="data-encryption"><span class="mr-2">Data Encryption</span><a href="#data-encryption" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>Verify the ability to encrypt secret data at rest by creating a generic secret and then using <strong>hexdump</strong> to inspect the entry stored in <strong>etcd</strong>:</p><div class="language-shell highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
</pre><td class="rouge-code"><pre>kubectl create secret generic kubernetes-from-scratch <span class="se">\</span>
  <span class="nt">--from-literal</span><span class="o">=</span><span class="s2">"mykey=mydata"</span>
<span class="nv">external_ip</span><span class="o">=</span><span class="si">$(</span>aws ec2 describe-instances <span class="nt">--filters</span> <span class="se">\</span>
  <span class="s2">"Name=tag:Name,Values=controller-0"</span> <span class="se">\</span>
  <span class="s2">"Name=instance-state-name,Values=running"</span> <span class="se">\</span>
  <span class="nt">--output</span> text <span class="nt">--query</span> <span class="s1">'Reservations[].Instances[].PublicIpAddress'</span><span class="si">)</span>
ssh <span class="nt">-i</span> kubernetes.id_rsa ubuntu@<span class="k">${</span><span class="nv">external_ip</span><span class="k">}</span> <span class="se">\</span>
 <span class="s2">"sudo ETCDCTL_API=3 etcdctl get </span><span class="se">\</span><span class="s2">
  --endpoints=https://127.0.0.1:2379 </span><span class="se">\</span><span class="s2">
  --cacert=/etc/etcd/ca.pem </span><span class="se">\</span><span class="s2">
  --cert=/etc/etcd/kubernetes.pem </span><span class="se">\</span><span class="s2">
  --key=/etc/etcd/kubernetes-key.pem</span><span class="se">\</span><span class="s2">
  /registry/secrets/default/kubernetes-from-scratch | hexdump -C"</span>
</pre></table></code></div></div><p><img data-src="/assets/img/2022/07/cloudshell_secret_hexdump.png" alt="CloudShell Secrets HexDump" data-proofer-ignore></p><p>The etcd key should be prefixed with <strong>k8s:enc:aescbc:v1:key1</strong>, which indicates the aesbc provider was used to encrypt the data with the <strong>key1</strong> encryption key.</p><h3 id="creating-deployments"><span class="mr-2">Creating Deployments</span><a href="#creating-deployments" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>Deploy an <strong>nginx</strong> web server and list the pods:</p><div class="language-shell highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre><td class="rouge-code"><pre>kubectl create deployment nginx <span class="nt">--image</span><span class="o">=</span>nginx
kubectl get pods <span class="nt">-l</span> <span class="nv">app</span><span class="o">=</span>nginx
</pre></table></code></div></div><p><img data-src="/assets/img/2022/07/cloudshell_create_nginx.png" alt="CloudShell Create NGINX Deployment" data-proofer-ignore></p><h3 id="port-forwarding"><span class="mr-2">Port Forwarding</span><a href="#port-forwarding" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><div class="language-shell highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre><td class="rouge-code"><pre><span class="nv">POD_NAME</span><span class="o">=</span><span class="si">$(</span>kubectl get pods <span class="nt">-l</span> <span class="nv">app</span><span class="o">=</span>nginx <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s2">"{.items[0].metadata.name}"</span><span class="si">)</span>
kubectl port-forward <span class="nv">$POD_NAME</span> 8080:80
</pre></table></code></div></div><p><img data-src="/assets/img/2022/07/cloudshell_port_forwarding.png" alt="CloudShell Port Forwarding" data-proofer-ignore></p><p>In a new terminal window perform an HTTP request to the port forwarded endpoint:</p><div class="language-shell highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>curl <span class="nt">--head</span> http://127.0.0.1:8080
</pre></table></code></div></div><p><img data-src="/assets/img/2022/07/cloudshell_curl.png" alt="CloudShell cURL Port Forward NGINX Test" data-proofer-ignore></p><h3 id="logs"><span class="mr-2">Logs</span><a href="#logs" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>Print the <strong>nginx</strong> pod logs:</p><div class="language-shell highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>kubectl logs <span class="nv">$POD_NAME</span>
</pre></table></code></div></div><p><img data-src="/assets/img/2022/07/cloudshell_logs_output.png" alt="CloudShell Logs Output Test" data-proofer-ignore></p><h3 id="exec"><span class="mr-2">Exec</span><a href="#exec" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>Verify we are able to <a href="https://kubernetes.io/docs/tasks/debug-application-cluster/get-shell-running-container/#running-individual-commands-in-a-container">execute commands in a container</a>:</p><div class="language-shell highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>kubectl <span class="nb">exec</span> <span class="nt">-ti</span> <span class="nv">$POD_NAME</span> <span class="nt">--</span> nginx <span class="nt">-v</span>
</pre></table></code></div></div><p><img data-src="/assets/img/2022/07/cloudshell_exec.png" alt="CloudShell Exec Test" data-proofer-ignore></p><h3 id="services"><span class="mr-2">Services</span><a href="#services" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>Expose the <strong>nginx</strong> deployment using a <a href="https://kubernetes.io/docs/concepts/services-networking/service/#type-nodeport">NodePort</a> <a href="https://kubernetes.io/docs/concepts/services-networking/service/">service</a>:</p><div class="language-shell highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre><td class="rouge-code"><pre>kubectl expose deployment nginx <span class="nt">--port</span> 80 <span class="nt">--type</span> NodePort
<span class="nv">NODE_PORT</span><span class="o">=</span><span class="si">$(</span>kubectl get svc nginx <span class="se">\</span>
  <span class="nt">--output</span><span class="o">=</span><span class="nv">jsonpath</span><span class="o">=</span><span class="s1">'{range .spec.ports[0]}{.nodePort}'</span><span class="si">)</span>
</pre></table></code></div></div><p>Create a firewall rule that allows remote access to the <strong>nginx</strong> node port:</p><div class="language-shell highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre><td class="rouge-code"><pre>aws ec2 authorize-security-group-ingress <span class="se">\</span>
  <span class="nt">--group-id</span> <span class="k">${</span><span class="nv">SECURITY_GROUP_ID</span><span class="k">}</span> <span class="se">\</span>
  <span class="nt">--protocol</span> tcp <span class="se">\</span>
  <span class="nt">--port</span> <span class="k">${</span><span class="nv">NODE_PORT</span><span class="k">}</span> <span class="se">\</span>
  <span class="nt">--cidr</span> 0.0.0.0/0
</pre></table></code></div></div><p>Get the worker node name where the <strong>nginx</strong> pod is running, and retrieve the external IP address of a worker instance, finally make an HTTP request to the external IP address and the <strong>nginx</strong> node port:</p><div class="language-shell highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre><td class="rouge-code"><pre><span class="nv">INSTANCE_NAME</span><span class="o">=</span><span class="si">$(</span>kubectl get pod <span class="nv">$POD_NAME</span> <span class="nt">--output</span><span class="o">=</span><span class="nv">jsonpath</span><span class="o">=</span><span class="s1">'{.spec.nodeName}'</span><span class="si">)</span>
<span class="nv">EXTERNAL_IP</span><span class="o">=</span><span class="si">$(</span>aws ec2 describe-instances <span class="nt">--filters</span> <span class="se">\</span>
    <span class="s2">"Name=instance-state-name,Values=running"</span> <span class="se">\</span>
    <span class="s2">"Name=network-interface.private-dns-name,Values=</span><span class="k">${</span><span class="nv">INSTANCE_NAME</span><span class="k">}</span><span class="s2">.*.internal*"</span> <span class="se">\</span>
    <span class="nt">--output</span> text <span class="nt">--query</span> <span class="s1">'Reservations[].Instances[].PublicIpAddress'</span><span class="si">)</span>
curl <span class="nt">-I</span> http://<span class="k">${</span><span class="nv">EXTERNAL_IP</span><span class="k">}</span>:<span class="k">${</span><span class="nv">NODE_PORT</span><span class="k">}</span>
</pre></table></code></div></div><p><img data-src="/assets/img/2022/07/cloudshell_service_test.png" alt="CloudShell Service Test" data-proofer-ignore></p><h1 id="bonus-scanning-for-vulnerabilities-using-kube-hunter">Bonus: Scanning for Vulnerabilities using kube-hunter</h1><p><a href="https://github.com/aquasecurity/kube-hunter">kube-hunter</a> hunts for security weaknesses in Kubernetes clusters. The tool was developed to increase awareness and visibility for security issues in Kubernetes environments.</p><p>Deploy a job right into the cluster to scan for vulnerabilities:</p><div class="language-shell highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
</pre><td class="rouge-code"><pre><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh"> | kubectl apply --kubeconfig admin.kubeconfig -f -
apiVersion: batch/v1
kind: Job
metadata:
  name: kube-hunter
spec:
  template:
    metadata:
      labels:
        app: kube-hunter
    spec:
      containers:
        - name: kube-hunter
          image: aquasec/kube-hunter:0.6.8
          command: ["kube-hunter"]
          args: ["--pod"]
      restartPolicy: Never
</span><span class="no">EOF
</span></pre></table></code></div></div><p><img data-src="/assets/img/2022/07/cmd_list_pods.png" alt="KubeHunter List Pods" data-proofer-ignore></p><p>Let’s now check the logs and the output produced by <strong>kubehunter</strong>:</p><div class="language-shell highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
</pre><td class="rouge-code"><pre>kubectl logs <span class="nt">--kubeconfig</span> admin.kubeconfig kube-hunter-tws6b <span class="o">&gt;</span> logs.txt
<span class="nb">cat </span>logs.txt
2022-07-08 14:22:13,466 INFO kube_hunter.modules.report.collector Started hunting
2022-07-08 14:22:13,471 INFO kube_hunter.modules.report.collector Discovering Open Kubernetes Services
2022-07-08 14:22:13,478 INFO kube_hunter.modules.report.collector Found vulnerability <span class="s2">"CAP_NET_RAW Enabled"</span> <span class="k">in </span>Local to Pod <span class="o">(</span>kube-hunter-tws6b<span class="o">)</span>
2022-07-08 14:22:13,478 INFO kube_hunter.modules.report.collector Found vulnerability <span class="s2">"Read access to pod's service account token"</span> <span class="k">in </span>Local to Pod <span class="o">(</span>kube-hunter-tws6b<span class="o">)</span>
2022-07-08 14:22:13,479 INFO kube_hunter.modules.report.collector Found vulnerability <span class="s2">"Access to pod's secrets"</span> <span class="k">in </span>Local to Pod <span class="o">(</span>kube-hunter-tws6b<span class="o">)</span>
2022-07-08 14:22:13,515 INFO kube_hunter.modules.report.collector Found vulnerability <span class="s2">"AWS Metadata Exposure"</span> <span class="k">in </span>Local to Pod <span class="o">(</span>kube-hunter-tws6b<span class="o">)</span>
2022-07-08 14:22:13,678 INFO kube_hunter.modules.report.collector Found open service <span class="s2">"Kubelet API"</span> at 10.0.1.20:10250
2022-07-08 14:22:13,691 INFO kube_hunter.modules.report.collector Found open service <span class="s2">"Kubelet API"</span> at 10.0.1.21:10250
2022-07-08 14:22:13,694 INFO kube_hunter.modules.report.collector Found open service <span class="s2">"Etcd"</span> at 10.0.1.10:2379
2022-07-08 14:22:13,695 INFO kube_hunter.modules.report.collector Found open service <span class="s2">"Kubelet API"</span> at 10.0.1.22:10250
2022-07-08 14:22:13,750 INFO kube_hunter.modules.report.collector Found open service <span class="s2">"Etcd"</span> at 10.0.1.12:2379
2022-07-08 14:22:13,754 INFO kube_hunter.modules.report.collector Found open service <span class="s2">"Etcd"</span> at 10.0.1.11:2379
2022-07-08 14:22:13,817 INFO kube_hunter.modules.report.collector Found open service <span class="s2">"Kubelet API"</span> at 10.200.2.1:10250
2022-07-08 14:22:13,821 INFO kube_hunter.modules.report.collector Found open service <span class="s2">"Metrics Server"</span> at 10.0.1.10:6443
2022-07-08 14:22:13,825 INFO kube_hunter.modules.report.collector Found open service <span class="s2">"Metrics Server"</span> at 10.0.1.12:6443
2022-07-08 14:22:13,857 INFO kube_hunter.modules.report.collector Found open service <span class="s2">"Metrics Server"</span> at 10.0.1.11:6443
2022-07-08 14:22:21,218 INFO kube_hunter.modules.report.collector Found open service <span class="s2">"Metrics Server"</span> at 10.0.1.72:443
2022-07-08 14:22:21,406 INFO kube_hunter.modules.report.collector Found open service <span class="s2">"API Server"</span> at 10.32.0.1:443
2022-07-08 14:22:21,418 INFO kube_hunter.modules.report.collector Found vulnerability <span class="s2">"K8s Version Disclosure"</span> <span class="k">in </span>10.32.0.1:443
2022-07-08 14:22:21,424 INFO kube_hunter.modules.report.collector Found vulnerability <span class="s2">"Access to API using service account token"</span> <span class="k">in </span>10.32.0.1:443
Nodes
+-------------+------------+
| TYPE        | LOCATION   |
+-------------+------------+
| Node/Master | 10.200.2.1 |
+-------------+------------+
| Node/Master | 10.32.0.1  |
+-------------+------------+
| Node/Master | 10.0.1.72  |
+-------------+------------+
| Node/Master | 10.0.1.22  |
+-------------+------------+
| Node/Master | 10.0.1.21  |
+-------------+------------+
| Node/Master | 10.0.1.20  |
+-------------+------------+
| Node/Master | 10.0.1.12  |
+-------------+------------+
| Node/Master | 10.0.1.11  |
+-------------+------------+
| Node/Master | 10.0.1.10  |
+-------------+------------+
Detected Services
+----------------+------------------+----------------------+
| SERVICE        | LOCATION         | DESCRIPTION          |
+----------------+------------------+----------------------+
| Metrics Server | 10.0.1.72:443    | The Metrics server   |
|                |                  | is <span class="k">in </span>charge of      |
|                |                  | providing resource   |
|                |                  | usage metrics <span class="k">for</span>    |
|                |                  | pods and nodes to    |
|                |                  | the API server       |
+----------------+------------------+----------------------+
| Metrics Server | 10.0.1.12:6443   | The Metrics server   |
|                |                  | is <span class="k">in </span>charge of      |
|                |                  | providing resource   |
|                |                  | usage metrics <span class="k">for</span>    |
|                |                  | pods and nodes to    |
|                |                  | the API server       |
+----------------+------------------+----------------------+
| Metrics Server | 10.0.1.11:6443   | The Metrics server   |
|                |                  | is <span class="k">in </span>charge of      |
|                |                  | providing resource   |
|                |                  | usage metrics <span class="k">for</span>    |
|                |                  | pods and nodes to    |
|                |                  | the API server       |
+----------------+------------------+----------------------+
| Metrics Server | 10.0.1.10:6443   | The Metrics server   |
|                |                  | is <span class="k">in </span>charge of      |
|                |                  | providing resource   |
|                |                  | usage metrics <span class="k">for</span>    |
|                |                  | pods and nodes to    |
|                |                  | the API server       |
+----------------+------------------+----------------------+
| Kubelet API    | 10.200.2.1:10250 | The Kubelet is the   |
|                |                  | main component <span class="k">in</span>    |
|                |                  | every Node, all pod  |
|                |                  | operations goes      |
|                |                  | through the kubelet  |
+----------------+------------------+----------------------+
| Kubelet API    | 10.0.1.22:10250  | The Kubelet is the   |
|                |                  | main component <span class="k">in</span>    |
|                |                  | every Node, all pod  |
|                |                  | operations goes      |
|                |                  | through the kubelet  |
+----------------+------------------+----------------------+
| Kubelet API    | 10.0.1.21:10250  | The Kubelet is the   |
|                |                  | main component <span class="k">in</span>    |
|                |                  | every Node, all pod  |
|                |                  | operations goes      |
|                |                  | through the kubelet  |
+----------------+------------------+----------------------+
| Kubelet API    | 10.0.1.20:10250  | The Kubelet is the   |
|                |                  | main component <span class="k">in</span>    |
|                |                  | every Node, all pod  |
|                |                  | operations goes      |
|                |                  | through the kubelet  |
+----------------+------------------+----------------------+
| Etcd           | 10.0.1.12:2379   | Etcd is a DB that    |
|                |                  | stores cluster<span class="s1">'s     |
|                |                  | data, it contains    |
|                |                  | configuration and    |
|                |                  | current              |
|                |                  |     state            |
|                |                  | information, and     |
|                |                  | might contain        |
|                |                  | secrets              |
+----------------+------------------+----------------------+
| Etcd           | 10.0.1.11:2379   | Etcd is a DB that    |
|                |                  | stores cluster'</span>s     |
|                |                  | data, it contains    |
|                |                  | configuration and    |
|                |                  | current              |
|                |                  |     state            |
|                |                  | information, and     |
|                |                  | might contain        |
|                |                  | secrets              |
+----------------+------------------+----------------------+
| Etcd           | 10.0.1.10:2379   | Etcd is a DB that    |
|                |                  | stores cluster<span class="s1">'s     |
|                |                  | data, it contains    |
|                |                  | configuration and    |
|                |                  | current              |
|                |                  |     state            |
|                |                  | information, and     |
|                |                  | might contain        |
|                |                  | secrets              |
+----------------+------------------+----------------------+
| API Server     | 10.32.0.1:443    | The API server is in |
|                |                  | charge of all        |
|                |                  | operations on the    |
|                |                  | cluster.             |
+----------------+------------------+----------------------+
Vulnerabilities
For further information about a vulnerability, search its ID in:
https://avd.aquasec.com/
+--------+----------------------+----------------------+----------------------+----------------------+----------------------+
| ID     | LOCATION             | MITRE CATEGORY       | VULNERABILITY        | DESCRIPTION          | EVIDENCE             |
+--------+----------------------+----------------------+----------------------+----------------------+----------------------+
| None   | Local to Pod (kube-  | Lateral Movement //  | CAP_NET_RAW Enabled  | CAP_NET_RAW is       |                      |
|        | hunter-tws6b)        | ARP poisoning and IP |                      | enabled by default   |                      |
|        |                      | spoofing             |                      | for pods.            |                      |
|        |                      |                      |                      |     If an attacker   |                      |
|        |                      |                      |                      | manages to           |                      |
|        |                      |                      |                      | compromise a pod,    |                      |
|        |                      |                      |                      |     they could       |                      |
|        |                      |                      |                      | potentially take     |                      |
|        |                      |                      |                      | advantage of this    |                      |
|        |                      |                      |                      | capability to        |                      |
|        |                      |                      |                      | perform network      |                      |
|        |                      |                      |                      |     attacks on other |                      |
|        |                      |                      |                      | pods running on the  |                      |
|        |                      |                      |                      | same node            |                      |
+--------+----------------------+----------------------+----------------------+----------------------+----------------------+
| KHV002 | 10.32.0.1:443        | Initial Access //    | K8s Version          | The kubernetes       | v1.21.0              |
|        |                      | Exposed sensitive    | Disclosure           | version could be     |                      |
|        |                      | interfaces           |                      | obtained from the    |                      |
|        |                      |                      |                      | /version endpoint    |                      |
+--------+----------------------+----------------------+----------------------+----------------------+----------------------+
| KHV053 | Local to Pod (kube-  | Discovery //         | AWS Metadata         | Access to the AWS    | cidr: 10.0.1.0/24    |
|        | hunter-tws6b)        | Instance Metadata    | Exposure             | Metadata API exposes |                      |
|        |                      | API                  |                      | information about    |                      |
|        |                      |                      |                      | the machines         |                      |
|        |                      |                      |                      | associated with the  |                      |
|        |                      |                      |                      | cluster              |                      |
+--------+----------------------+----------------------+----------------------+----------------------+----------------------+
| KHV005 | 10.32.0.1:443        | Discovery // Access  | Access to API using  | The API Server port  | b'</span><span class="o">{</span><span class="s2">"kind"</span>:<span class="s2">"APIVersio |
|        |                      | the K8S API Server   | service account      | is accessible.       | ns"</span>,<span class="s2">"versions"</span>:[<span class="s2">"v1"</span> |
|        |                      |                      | token                |     Depending on     | <span class="o">]</span>,<span class="s2">"serverAddressByCl |
|        |                      |                      |                      | your RBAC settings   | ientCIDRs"</span>:[<span class="o">{</span><span class="s2">"client |
|        |                      |                      |                      | this could expose    | CIDR"</span>:<span class="s2">"0.0.0.0/0"</span>,<span class="s2">"s |
|        |                      |                      |                      | access to or control | ...                  |
|        |                      |                      |                      | of your cluster.     |                      |
+--------+----------------------+----------------------+----------------------+----------------------+----------------------+
| None   | Local to Pod (kube-  | Credential Access // | Access to pod's      | Accessing the pod's  | ['/var/run/secrets/k |
|        | hunter-tws6b)        | Access container     | secrets              | secrets within a     | ubernetes.io/service |
|        |                      | service account      |                      | compromised pod      | account/namespace',  |
|        |                      |                      |                      | might disclose       | '/var/run/secrets/ku |
|        |                      |                      |                      | valuable data to a   | bernetes.io/servicea |
|        |                      |                      |                      | potential attacker   | ...                  |
+--------+----------------------+----------------------+----------------------+----------------------+----------------------+
| KHV050 | Local to Pod (kube-  | Credential Access // | Read access to pod's | Accessing the pod    | eyJhbGciOiJSUzI1NiIs |
|        | hunter-tws6b)        | Access container     | service account      | service account      | ImtpZCI6IjItYnpCa1pK |
|        |                      | service account      | token                | token gives an       | aE1pTVpXZE1qSkhnRDA5 |
|        |                      |                      |                      | attacker the option  | YmdMZ3BLdmNxejV4VVYw |
|        |                      |                      |                      | to use the server    | OW12LVEifQ.eyJhdWQiO |
|        |                      |                      |                      | API                  | ...                  |
+--------+----------------------+----------------------+----------------------+----------------------+----------------------+
</span></pre></table></code></div></div><p>From the log output above you can see there is quite a bit of information around vulnerabilities and also some potential resolution steps. You can use this output to start patching your Kubernetes cluster to ensure you are getting the most security that you need.</p><h1 id="cleaning-up">Cleaning Up</h1><p><strong>Make sure you remove all of the resources we created</strong>, or else it will incure running costs if left running.</p><p>Delete all the <strong>worker</strong> instances, then afterwards delete <strong>controller</strong> instances, also delete our <strong>key-pair</strong>:</p><div class="language-shell highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
</pre><td class="rouge-code"><pre><span class="nb">echo</span> <span class="s2">"Issuing shutdown to worker nodes.. "</span> <span class="o">&amp;&amp;</span> <span class="se">\</span>
aws ec2 terminate-instances <span class="se">\</span>
  <span class="nt">--instance-ids</span> <span class="se">\</span>
    <span class="si">$(</span>aws ec2 describe-instances <span class="nt">--filters</span> <span class="se">\</span>
      <span class="s2">"Name=tag:Name,Values=worker-0,worker-1,worker-2"</span> <span class="se">\</span>
      <span class="s2">"Name=instance-state-name,Values=running"</span> <span class="se">\</span>
      <span class="nt">--output</span> text <span class="nt">--query</span> <span class="s1">'Reservations[].Instances[].InstanceId'</span><span class="si">)</span>

<span class="nb">echo</span> <span class="s2">"Waiting for worker nodes to finish terminating.. "</span> <span class="o">&amp;&amp;</span> <span class="se">\</span>
aws ec2 <span class="nb">wait </span>instance-terminated <span class="se">\</span>
  <span class="nt">--instance-ids</span> <span class="se">\</span>
    <span class="si">$(</span>aws ec2 describe-instances <span class="se">\</span>
      <span class="nt">--filter</span> <span class="s2">"Name=tag:Name,Values=worker-0,worker-1,worker-2"</span> <span class="se">\</span>
      <span class="nt">--output</span> text <span class="nt">--query</span> <span class="s1">'Reservations[].Instances[].InstanceId'</span><span class="si">)</span>

<span class="nb">echo</span> <span class="s2">"Issuing shutdown to master nodes.. "</span> <span class="o">&amp;&amp;</span> <span class="se">\</span>
aws ec2 terminate-instances <span class="se">\</span>
  <span class="nt">--instance-ids</span> <span class="se">\</span>
    <span class="si">$(</span>aws ec2 describe-instances <span class="nt">--filter</span> <span class="se">\</span>
      <span class="s2">"Name=tag:Name,Values=controller-0,controller-1,controller-2"</span> <span class="se">\</span>
      <span class="s2">"Name=instance-state-name,Values=running"</span> <span class="se">\</span>
      <span class="nt">--output</span> text <span class="nt">--query</span> <span class="s1">'Reservations[].Instances[].InstanceId'</span><span class="si">)</span>

<span class="nb">echo</span> <span class="s2">"Waiting for master nodes to finish terminating.. "</span> <span class="o">&amp;&amp;</span> <span class="se">\</span>
aws ec2 <span class="nb">wait </span>instance-terminated <span class="se">\</span>
  <span class="nt">--instance-ids</span> <span class="se">\</span>
    <span class="si">$(</span>aws ec2 describe-instances <span class="se">\</span>
      <span class="nt">--filter</span> <span class="s2">"Name=tag:Name,Values=controller-0,controller-1,controller-2"</span> <span class="se">\</span>
      <span class="nt">--output</span> text <span class="nt">--query</span> <span class="s1">'Reservations[].Instances[].InstanceId'</span><span class="si">)</span>

aws ec2 delete-key-pair <span class="nt">--key-name</span> kubernetes
</pre></table></code></div></div><p>Delete the external load balancer and network resources (if you have lost the values of any of the environment variables, then you can use AWS commands to look them up, or you can manually set them by looking at the resources in the AWS console):</p><div class="language-shell highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
</pre><td class="rouge-code"><pre>aws elbv2 delete-load-balancer <span class="nt">--load-balancer-arn</span> <span class="s2">"</span><span class="k">${</span><span class="nv">LOAD_BALANCER_ARN</span><span class="k">}</span><span class="s2">"</span>

aws elbv2 delete-target-group <span class="nt">--target-group-arn</span> <span class="s2">"</span><span class="k">${</span><span class="nv">TARGET_GROUP_ARN</span><span class="k">}</span><span class="s2">"</span>

aws ec2 delete-security-group <span class="nt">--group-id</span> <span class="s2">"</span><span class="k">${</span><span class="nv">SECURITY_GROUP_ID</span><span class="k">}</span><span class="s2">"</span>

<span class="nv">ROUTE_TABLE_ASSOCIATION_ID</span><span class="o">=</span><span class="s2">"</span><span class="si">$(</span>aws ec2 describe-route-tables <span class="se">\</span>
  <span class="nt">--route-table-ids</span> <span class="s2">"</span><span class="k">${</span><span class="nv">ROUTE_TABLE_ID</span><span class="k">}</span><span class="s2">"</span> <span class="se">\</span>
  <span class="nt">--output</span> text <span class="nt">--query</span> <span class="s1">'RouteTables[].Associations[].RouteTableAssociationId'</span><span class="si">)</span><span class="s2">"</span>

aws ec2 disassociate-route-table <span class="nt">--association-id</span> <span class="s2">"</span><span class="k">${</span><span class="nv">ROUTE_TABLE_ASSOCIATION_ID</span><span class="k">}</span><span class="s2">"</span>

aws ec2 delete-route-table <span class="nt">--route-table-id</span> <span class="s2">"</span><span class="k">${</span><span class="nv">ROUTE_TABLE_ID</span><span class="k">}</span><span class="s2">"</span>

<span class="nb">echo</span> <span class="s2">"Waiting a minute for all public address(es) to be unmapped.. "</span> <span class="o">&amp;&amp;</span> <span class="nb">sleep </span>60

aws ec2 detach-internet-gateway <span class="se">\</span>
  <span class="nt">--internet-gateway-id</span> <span class="s2">"</span><span class="k">${</span><span class="nv">INTERNET_GATEWAY_ID</span><span class="k">}</span><span class="s2">"</span> <span class="se">\</span>
  <span class="nt">--vpc-id</span> <span class="s2">"</span><span class="k">${</span><span class="nv">VPC_ID</span><span class="k">}</span><span class="s2">"</span>

aws ec2 delete-internet-gateway <span class="nt">--internet-gateway-id</span> <span class="s2">"</span><span class="k">${</span><span class="nv">INTERNET_GATEWAY_ID</span><span class="k">}</span><span class="s2">"</span>

aws ec2 delete-subnet <span class="nt">--subnet-id</span> <span class="s2">"</span><span class="k">${</span><span class="nv">SUBNET_ID</span><span class="k">}</span><span class="s2">"</span>

aws ec2 delete-vpc <span class="nt">--vpc-id</span> <span class="s2">"</span><span class="k">${</span><span class="nv">VPC_ID</span><span class="k">}</span><span class="s2">"</span>
</pre></table></code></div></div><p><strong>I cannot stress this enough, go through your AWS console resources and ensure that you have removed everything we have provisioned in this guide. You don’t want to come back a year later wondering why your credit card balance is hurting!!!</strong></p><p>Thanks for reading.</p><p>Medium Article: https://medium.com/geekculture/building-a-kubernetes-cluster-on-aws-from-scratch-7e1e8b0342c4</p></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/kubernetes/'>kubernetes</a>, <a href='/categories/aws/'>aws</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/kubernetes/" class="post-tag no-text-decoration" >kubernetes</a> <a href="/tags/aws/" class="post-tag no-text-decoration" >aws</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=Building+a+Kubernetes+cluster+on+AWS+from+scratch+-+Craig+Newton&url=https%3A%2F%2Fnewtondev.github.io%2Fposts%2Fbuilding-a-kubernetes-cluster-from-scratch%2F" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=Building+a+Kubernetes+cluster+on+AWS+from+scratch+-+Craig+Newton&u=https%3A%2F%2Fnewtondev.github.io%2Fposts%2Fbuilding-a-kubernetes-cluster-from-scratch%2F" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://t.me/share/url?url=https%3A%2F%2Fnewtondev.github.io%2Fposts%2Fbuilding-a-kubernetes-cluster-from-scratch%2F&text=Building+a+Kubernetes+cluster+on+AWS+from+scratch+-+Craig+Newton" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" data-title-succeed="Link copied successfully!"> </i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted"><div class="access"><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/kubernetes/">kubernetes</a> <a class="post-tag" href="/tags/aws/">aws</a> <a class="post-tag" href="/tags/azure/">azure</a> <a class="post-tag" href="/tags/cloud/">cloud</a> <a class="post-tag" href="/tags/cron/">cron</a> <a class="post-tag" href="/tags/docker/">docker</a> <a class="post-tag" href="/tags/efs/">efs</a> <a class="post-tag" href="/tags/eip/">eip</a> <a class="post-tag" href="/tags/gateways/">gateways</a> <a class="post-tag" href="/tags/gcp/">gcp</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"><div class="panel-heading pl-3 pt-2 mb-2">Contents</div><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div id="tail-wrapper" class="col-12 col-lg-11 col-xl-9 pl-3 pr-3 pr-xl-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/how-to-assign-an-eip-to-an-aws-nlb-in-kubernetes/"><div class="card-body"> <em class="small" data-ts="1559995200" data-df="ll" > Jun 8, 2019 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>How to assign an EIP to an AWS NLB in Kubernetes</h3><div class="text-muted small"><p> Out of the box Kubernetes has no support for assigning an EIP to an AWS Network Load Balancer. There is currently a pull request for this feature listed here: https://github.com/kubernetes/kubernet...</p></div></div></a></div><div class="card"> <a href="/posts/public-and-private-istio-ingress-gateways-on-aws/"><div class="card-body"> <em class="small" data-ts="1567166400" data-df="ll" > Aug 30, 2019 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Public and Private Istio Ingress Gateways on AWS</h3><div class="text-muted small"><p> By default, istio creates a service with a publicly accessible classic load balancer (ELB). However, there are times where we only want access from our internal network or a network we are connec...</p></div></div></a></div><div class="card"> <a href="/posts/kubernetes-on-aws-gotcha-when-using-efs/"><div class="card-body"> <em class="small" data-ts="1645531200" data-df="ll" > Feb 22, 2022 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Kubernetes on AWS — gotcha when using EFS</h3><div class="text-muted small"><p> I was happily migrating a lot of applications over to EKS when my pods went into an Init state. Panic set in and I was having a look around at what the problem could be (what is it this time, I s...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/how-i-passed-the-ine-certified-cloud-associate-icca-exam/" class="btn btn-outline-primary" prompt="Older"><p>How I passed the INE Certified Cloud Associate (ICCA) Exam</p></a><div class="btn btn-outline-primary disabled" prompt="Newer"><p>-</p></div></div></div></div><footer class="row pl-3 pr-3"><div class="col-12 d-flex justify-content-between align-items-center text-muted pl-0 pr-0"><div class="footer-left"><p class="mb-0"> © 2025 <a href="https://twitter.com/craignewtondev">Craig Newton</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/kubernetes/">kubernetes</a> <a class="post-tag" href="/tags/aws/">aws</a> <a class="post-tag" href="/tags/azure/">azure</a> <a class="post-tag" href="/tags/cloud/">cloud</a> <a class="post-tag" href="/tags/cron/">cron</a> <a class="post-tag" href="/tags/docker/">docker</a> <a class="post-tag" href="/tags/efs/">efs</a> <a class="post-tag" href="/tags/eip/">eip</a> <a class="post-tag" href="/tags/gateways/">gateways</a> <a class="post-tag" href="/tags/gcp/">gcp</a></div></div></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a><div id="notification" class="toast" role="alert" aria-live="assertive" aria-atomic="true" data-animation="true" data-autohide="false"><div class="toast-header"> <button type="button" class="ml-2 ml-auto close" data-dismiss="toast" aria-label="Close"> <span aria-hidden="true">&times;</span> </button></div><div class="toast-body text-center pt-0"><p class="pl-2 pr-2 mb-3">A new version of content is available.</p><button type="button" class="btn btn-primary" aria-label="Update"> Update </button></div></div><script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No results found.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/lozad/dist/lozad.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script src="https://cdn.jsdelivr.net/combine/npm/dayjs@1/dayjs.min.js,npm/dayjs@1/locale/en.min.js,npm/dayjs@1/plugin/relativeTime.min.js,npm/dayjs@1/plugin/localizedFormat.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.bundle.min.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id=G-Z9HKJNH86F"></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'G-Z9HKJNH86F'); }); </script>
