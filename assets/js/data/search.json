[ { "title": "Building a Kubernetes cluster on AWS from scratch", "url": "/posts/building-a-kubernetes-cluster-from-scratch/", "categories": "kubernetes, aws", "tags": "kubernetes, aws", "date": "2022-07-09 14:00:00 +0200", "snippet": "These days rolling out a Kubernetes cluster on any cloud provider is quite simple and easy, but not always cost effective. Us as DevOps/Ops Engineers are spoilt with the ease of configuration and not being able to see “behind the curtain”, especially on the master control plane. But there is always that nagging question as to how that works. Let’s peel away that curtain and go on a journey to build our own HA(Highly Available) Kubernetes cluster on AWS.This walkthrough is essential for people who support Kubernetes clusters in production and want to understand how it all fits together; and how all this runs on AWS. This work is based off of Kelsey Hightower’s Kubernetes The Hard Way Guide which was deployed to GCP (Google Cloud Platform).Below is a simple representation of what the infrastructure architecture, of what we want to achieve, would look like:We will need to provision the following compute resources in AWS: Networking — VPC, Subnet, Internet Gateway, Route Tables, Security Groups, Network Load Balancer Compute Instances — EC2 nodes for controllers and workers, SSH Key PairBefore we begin, we first need to ensure we have some prerequisites: Have an AWS account setup (hopefully with some free credit, but this cluster will not cost us much to provision as we will tear it down afterwards). Install the AWS CLI for interacting with your AWS account to provision resources; or you can use the AWS Cloud Shell which is a browser based alternative to setting all this up on your machine. Pick an AWS zone you want to deploy in, preferably closer to where you are; for me I am choosing eu-central-1 as that is my closest AWS region. You can also install and use tmux to simplify running the same commands on multiple instances. We will be generating quite a few PKI Infrastructure keys and generate TLS certificates as we want everything to be secure. Ensure that you have cfssl and cfssljon command line utilties installed. Because we will be using Kubernetes, we also need to make sure that we have our trusty kubectl client installed so we can perform actions on our Kubernetes cluster. Set a Default Compute Region and ZoneIn your terminal window or in your AWS CloudShell window run:AWS_REGION=eu-central-1aws configure set default.region $AWS_REGIONInstalling some client tools we needI am going to be using examples using linux here, but if you are using some other OS like Mac OS X, then refer to this page here.wget -q --timestamping \\ https://storage.googleapis.com/kubernetes-the-hard-way/cfssl/1.4.1/linux/cfssl \\ https://storage.googleapis.com/kubernetes-the-hard-way/cfssl/1.4.1/linux/cfssljsonchmod +x cfssl cfssljsonsudo mv cfssl cfssljson /usr/local/bin/cfssl versioncfssljson --versionwget https://storage.googleapis.com/kubernetes-release/release/v1.21.0/bin/linux/amd64/kubectlchmod +x kubectlsudo mv kubectl /usr/local/bin/kubectl version --clientProvisioning Compute InfrastructureBest practise with AWS dictates that we wrap our “project”/”product” into it’s own VPC (Virtual Private Cloud) with Subnets, Routing tables, Load Balancers and an Internet Gateway (of which only 1 Internet Gateway is allowed per VPC). This is not only a grouping mechanism, but also a layer of security.VPCLet’s set up a VPC called kubernetes-from-scratch that has DNS support and DNS hostname support enabled. Execute the following in your terminal session:VPC_ID=$(aws ec2 create-vpc --cidr-block 10.0.0.0/16 --output text --query 'Vpc.VpcId')aws ec2 create-tags --resources ${VPC_ID} --tags Key=Name,Value=kubernetes-from-scratchaws ec2 modify-vpc-attribute --vpc-id ${VPC_ID} --enable-dns-support '{\"Value\": true}'aws ec2 modify-vpc-attribute --vpc-id ${VPC_ID} --enable-dns-hostnames '{\"Value\": true}'Private SubnetWe need to be able to assign private IP addresses to our compute instances for both the control plane controllers, as well as our worker instances. Subnets in our VPC allow us to create a range of IP addresses that we can allocate to our instances which do not allow external access (unless through a proxy or load balancer):SUBNET_ID=$(aws ec2 create-subnet \\ --vpc-id ${VPC_ID} \\ --cidr-block 10.0.1.0/24 \\ --output text --query 'Subnet.SubnetId')aws ec2 create-tags --resources ${SUBNET_ID} --tags Key=Name,Value=kubernetes-pvtBy using this CIDR range 10.0.1.0/24 we have up to 256 hosts (actually less due to AWS reserving some of the IPs, the first 4 and the last 1, read more here).Internet GatewayOur instances need some way to connect and communicate with the internet since we are on a private network. This means we need to provision a gateway we can use to proxy our traffic through. Let’s setup one by running the following commands:INTERNET_GATEWAY_ID=$(aws ec2 create-internet-gateway --output text --query 'InternetGateway.InternetGatewayId')aws ec2 create-tags --resources ${INTERNET_GATEWAY_ID} --tags Key=Name,Value=kubernetes-igwaws ec2 attach-internet-gateway --internet-gateway-id ${INTERNET_GATEWAY_ID} --vpc-id ${VPC_ID}Route TableWe need to now define how we want to route our traffic from our instances in our network through our Internet Gateway. To do that we define a routing table for the traffic:ROUTE_TABLE_ID=$(aws ec2 create-route-table --vpc-id ${VPC_ID} --output text --query 'RouteTable.RouteTableId')aws ec2 create-tags --resources ${ROUTE_TABLE_ID} --tags Key=Name,Value=kubernetes-rtaws ec2 associate-route-table --route-table-id ${ROUTE_TABLE_ID} --subnet-id ${SUBNET_ID}aws ec2 create-route --route-table-id ${ROUTE_TABLE_ID} --destination-cidr-block 0.0.0.0/0 --gateway-id ${INTERNET_GATEWAY_ID}Our private subnet has now been associated with the Route Table and our routes have been setup for our Internet Gateway.Security GroupWe need a security group so that we can allow traffic between our instances, as well as access from our client software. We define rules for communication betweeen our controllers and our workers; SSH, Kubernetes API server, HTTPS and ICMP (for pings):SECURITY_GROUP_ID=$(aws ec2 create-security-group \\ --group-name kubernetes-from-scratch \\ --description \"Kubernetes from scratch - security group\" \\ --vpc-id ${VPC_ID} \\ --output text --query 'GroupId')aws ec2 create-tags --resources ${SECURITY_GROUP_ID} --tags Key=Name,Value=kubernetes-sgaws ec2 authorize-security-group-ingress --group-id ${SECURITY_GROUP_ID} --protocol all --cidr 10.0.0.0/16aws ec2 authorize-security-group-ingress --group-id ${SECURITY_GROUP_ID} --protocol all --cidr 10.200.0.0/16aws ec2 authorize-security-group-ingress --group-id ${SECURITY_GROUP_ID} --protocol tcp --port 22 --cidr 0.0.0.0/0aws ec2 authorize-security-group-ingress --group-id ${SECURITY_GROUP_ID} --protocol tcp --port 6443 --cidr 0.0.0.0/0aws ec2 authorize-security-group-ingress --group-id ${SECURITY_GROUP_ID} --protocol tcp --port 443 --cidr 0.0.0.0/0aws ec2 authorize-security-group-ingress --group-id ${SECURITY_GROUP_ID} --protocol icmp --port -1 --cidr 0.0.0.0/0Network Load BalancerWe need some way for us to access our Kubernetes API from the outside world . We create an Internet-facing Network Load Balancer and register our control plane controllers. This way we can have an HA (Highly Available) control plane. To create our load balancer execute the following:LOAD_BALANCER_ARN=$(aws elbv2 create-load-balancer \\ --name kubernetes-nlb \\ --subnets ${SUBNET_ID} \\ --scheme internet-facing \\ --type network \\ --output text --query 'LoadBalancers[].LoadBalancerArn')TARGET_GROUP_ARN=$(aws elbv2 create-target-group \\ --name kubernetes-tg \\ --protocol TCP \\ --port 6443 \\ --vpc-id ${VPC_ID} \\ --target-type ip \\ --output text --query 'TargetGroups[].TargetGroupArn')aws elbv2 register-targets --target-group-arn ${TARGET_GROUP_ARN} --targets Id=10.0.1.1{0,1,2}aws elbv2 create-listener \\ --load-balancer-arn ${LOAD_BALANCER_ARN} \\ --protocol TCP \\ --port 443 \\ --default-actions Type=forward,TargetGroupArn=${TARGET_GROUP_ARN} \\ --output text --query 'Listeners[].ListenerArn'We can get our public DNS address of our load balancer for use later on:KUBERNETES_PUBLIC_ADDRESS=$(aws elbv2 describe-load-balancers \\ --load-balancer-arns ${LOAD_BALANCER_ARN} \\ --output text --query 'LoadBalancers[].DNSName')Compute InstancesNow that we have setup all of our routing and supporting infrastructure, it has now come time to define our work horses (controllers and workers). We are going to be using Ubuntu 20.04 for our compute instances, so we will need to select that first:IMAGE_ID=$(aws ec2 describe-images --owners 099720109477 \\ --output json \\ --filters \\ 'Name=root-device-type,Values=ebs' \\ 'Name=architecture,Values=x86_64' \\ 'Name=name,Values=ubuntu/images/hvm-ssd/ubuntu-focal-20.04-amd64-server-*' \\ | jq -r '.Images|sort_by(.Name)[-1]|.ImageId')This will get the IMAGE_ID in our region for the Ubuntu 20.04 OS that we will be running on our nodes.We need to connect to our nodes so we can install software and manage the systems, so we need to create a key-pair so we can use it to securely connect to our instances. Let’s create our new key-pair:aws ec2 create-key-pair --key-name kubernetes --output text --query 'KeyMaterial' &gt; kubernetes.id_rsachmod 600 kubernetes.id_rsaRemember to save this file somewhere safe like in a keychain or something like Bitwarden.Next, let’s provision our Kubernetes controllers. We going to require 3 for HA (High Availability), and we will be using t3.micro instances in this step (feel free to experiment with different instance types, but bear in mind they do cost you money):for i in 0 1 2; do instance_id=$(aws ec2 run-instances \\ --associate-public-ip-address \\ --image-id ${IMAGE_ID} \\ --count 1 \\ --key-name kubernetes \\ --security-group-ids ${SECURITY_GROUP_ID} \\ --instance-type t3.micro \\ --private-ip-address 10.0.1.1${i} \\ --user-data \"name=controller-${i}\" \\ --subnet-id ${SUBNET_ID} \\ --block-device-mappings='{\"DeviceName\": \"/dev/sda1\", \"Ebs\": { \"VolumeSize\": 50 }, \"NoDevice\": \"\" }' \\ --output text --query 'Instances[].InstanceId') aws ec2 modify-instance-attribute --instance-id ${instance_id} --no-source-dest-check aws ec2 create-tags --resources ${instance_id} --tags \"Key=Name,Value=controller-${i}\" echo \"controller-${i} created \"doneTake special note of the key-name if you have named it different to what we have in the previous steps. We associate a public ip address as well to the instances.Now it is time to create the worker nodes:for i in 0 1 2; do instance_id=$(aws ec2 run-instances \\ --associate-public-ip-address \\ --image-id ${IMAGE_ID} \\ --count 1 \\ --key-name kubernetes \\ --security-group-ids ${SECURITY_GROUP_ID} \\ --instance-type t3.micro \\ --private-ip-address 10.0.1.2${i} \\ --user-data \"name=worker-${i}|pod-cidr=10.200.${i}.0/24\" \\ --subnet-id ${SUBNET_ID} \\ --block-device-mappings='{\"DeviceName\": \"/dev/sda1\", \"Ebs\": { \"VolumeSize\": 50 }, \"NoDevice\": \"\" }' \\ --output text --query 'Instances[].InstanceId') aws ec2 modify-instance-attribute --instance-id ${instance_id} --no-source-dest-check aws ec2 create-tags --resources ${instance_id} --tags \"Key=Name,Value=worker-${i}\" echo \"worker-${i} created\"doneAnd just like that we have our infrastructure in place, we now need to move on and install the necessary software and configuration on each node.Provisioning a CA and Generating TLS CertificatesSecurity is extremely important, and by using TLS Certificates we ensure that communication between the nodes in the system is secure.Let us proceed to create a folder called certs and then enter the folder:mkdir -p certscd certs/Certificate Authoritycat &gt; ca-config.json &lt;&lt;EOF{ \"signing\": { \"default\": { \"expiry\": \"8760h\" }, \"profiles\": { \"kubernetes\": { \"usages\": [\"signing\", \"key encipherment\", \"server auth\", \"client auth\"], \"expiry\": \"8760h\" } } }}EOFcat &gt; ca-csr.json &lt;&lt;EOF{ \"CN\": \"Kubernetes\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"NL\", \"L\": \"Netherlands\", \"O\": \"Kubernetes\", \"OU\": \"AMS\", \"ST\": \"Amsterdam\" } ]}EOFcfssl gencert -initca ca-csr.json | cfssljson -bare caClient and Server CertificatesWe need to generate client and server certificates for each Kubernetes component and a client certificate for the Kubernetes admin user.The Admin Client CertificateGenerate the admin client certificate and private key:cat &gt; admin-csr.json &lt;&lt;EOF{ \"CN\": \"admin\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"NL\", \"L\": \"Netherlands\", \"O\": \"system:masters\", \"OU\": \"Kubernetes from scratch\", \"ST\": \"Amsterdam\" } ]}EOFcfssl gencert \\ -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes \\ admin-csr.json | cfssljson -bare adminThe Kubelet Client CertificatesKubernetes uses a special-purpose authorization mode called Node Authorizer, that specifically authorizes API requests made by Kubelets. In order to be authorized by the Node Authorizer, Kubelets must use a credential that identifies them as being in the system:nodes group, with a username of system:node:&lt;nodeName&gt;. In this section you will create a certificate for each Kubernetes worker node that meets the Node Authorizer requirements.Generate a certificate and private key for each Kubernetes worker node:for i in 0 1 2; do instance=\"worker-${i}\" instance_hostname=\"ip-10-0-1-2${i}\" cat &gt; ${instance}-csr.json &lt;&lt;EOF{ \"CN\": \"system:node:${instance_hostname}\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"NL\", \"L\": \"Netherlands\", \"O\": \"system:nodes\", \"OU\": \"Kubernetes from scratch\", \"ST\": \"Amsterdam\" } ]}EOFexternal_ip=$(aws ec2 describe-instances --filters \\ \"Name=tag:Name,Values=${instance}\" \\ \"Name=instance-state-name,Values=running\" \\ --output text --query 'Reservations[].Instances[].PublicIpAddress')internal_ip=$(aws ec2 describe-instances --filters \\ \"Name=tag:Name,Values=${instance}\" \\ \"Name=instance-state-name,Values=running\" \\ --output text --query 'Reservations[].Instances[].PrivateIpAddress')cfssl gencert \\ -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -hostname=${instance_hostname},${external_ip},${internal_ip} \\ -profile=kubernetes \\ worker-${i}-csr.json | cfssljson -bare worker-${i}doneThe Controller Manager Client CertificateGenerate the kube-controller-manager client certificate and private key:cat &gt; kube-controller-manager-csr.json &lt;&lt;EOF{ \"CN\": \"system:kube-controller-manager\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"NL\", \"L\": \"Netherlands\", \"O\": \"system:kube-controller-manager\", \"OU\": \"Kubernetes from scratch\", \"ST\": \"Amsterdam\" } ]}EOFcfssl gencert \\ -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes \\ kube-controller-manager-csr.json | cfssljson -bare kube-controller-managerThe Kube Proxy Client CertificateGenerate the kube-proxy client certificate and private key:cat &gt; kube-proxy-csr.json &lt;&lt;EOF{ \"CN\": \"system:kube-proxy\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"NL\", \"L\": \"Netherlands\", \"O\": \"system:node-proxier\", \"OU\": \"Kubernetes from scratch\", \"ST\": \"Amsterdam\" } ]}EOFcfssl gencert \\ -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes \\ kube-proxy-csr.json | cfssljson -bare kube-proxyThe Scheduler Client CertificateGenerate the kube-scheduler client certificate and private key:cat &gt; kube-scheduler-csr.json &lt;&lt;EOF{ \"CN\": \"system:kube-scheduler\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"NL\", \"L\": \"Netherlands\", \"O\": \"system:kube-scheduler\", \"OU\": \"Kubernetes from scratch\", \"ST\": \"Amsterdam\" } ]}EOFcfssl gencert \\ -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes \\ kube-scheduler-csr.json | cfssljson -bare kube-schedulerThe Kubernetes API Server CertificateThe Kubernetes Public Address will be included in the list of subject alternative names for the Kubernetes API Server certificate. This will encure the certificate can be validated by remote clients.Generate the Kubernetes API Server certificate and private key:KUBERNETES_HOSTNAMES=kubernetes,kubernetes.default,kubernetes.default.svc,kubernetes.default.svc.cluster,kubernetes.svc.cluster.localcat &gt; kubernetes-csr.json &lt;&lt;EOF{ \"CN\": \"kubernetes\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"NL\", \"L\": \"Netherlands\", \"O\": \"Kubernetes\", \"OU\": \"Kubernetes from scratch\", \"ST\": \"Amsterdam\" } ]}EOFcfssl gencert \\ -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -hostname=10.32.0.1,10.0.1.10,10.0.1.11,10.0.1.12,${KUBERNETES_PUBLIC_ADDRESS},127.0.0.1,${KUBERNETES_HOSTNAMES} \\ -profile=kubernetes \\ kubernetes-csr.json | cfssljson -bare kubernetesThe Service Account Key PairThe Kubernetes Controller Manager leverages a key pair to generate and sign service account tokens as described in the managing service accounts documentation.Generate the service-account certificate and private key:cat &gt; service-account-csr.json &lt;&lt;EOF{ \"CN\": \"service-accounts\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"NL\", \"L\": \"Netherlands\", \"O\": \"Kubernetes\", \"OU\": \"Kubernetes from scratch\", \"ST\": \"Amsterdam\" } ]}EOFcfssl gencert \\ -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes \\ service-account-csr.json | cfssljson -bare service-accountDistribute the Client and Server CertificatesNow that we have this directory full of certificates, its time to send them to the nodes.First we will copy the certificates and private keys to each worker instance:for instance in worker-0 worker-1 worker-2; do external_ip=$(aws ec2 describe-instances --filters \\ \"Name=tag:Name,Values=${instance}\" \\ \"Name=instance-state-name,Values=running\" \\ --output text --query 'Reservations[].Instances[].PublicIpAddress')scp -i ../kubernetes.id_rsa ca.pem ${instance}-key.pem ${instance}.pem ubuntu@${external_ip}:~/doneTake note, we are in the certs/ folder, so to I added the ../kubernetes.id_rsa — which is one directory up from where we were in case you are getting permission denied errors.Next we copy the certificates and private keys to each controller instance:for instance in controller-0 controller-1 controller-2; do external_ip=$(aws ec2 describe-instances --filters \\ \"Name=tag:Name,Values=${instance}\" \\ \"Name=instance-state-name,Values=running\" \\ --output text --query 'Reservations[].Instances[].PublicIpAddress')scp -i ../kubernetes.id_rsa \\ ca.pem ca-key.pem kubernetes-key.pem kubernetes.pem \\ service-account-key.pem service-account.pem ubuntu@${external_ip}:~/doneGenerating Kubernetes Configuration Files for AuthenticationWe need to generate some kubeconfigs — Kubernetes configuration files — which enable Kubernetes clients to locate and authenticate to the Kubernetes API Servers.For this we will go back to our home folder and create a folder called configs and then change into that folder:cd ~/mkdir -p configscd configs/The kubelet Configuration FileGenerate a kubeconfig for each worker node:for instance in worker-0 worker-1 worker-2; do kubectl config set-cluster kubernetes-from-scratch \\ --certificate-authority=../certs/ca.pem \\ --embed-certs=true \\ --server=https://${KUBERNETES_PUBLIC_ADDRESS}:443 \\ --kubeconfig=${instance}.kubeconfigkubectl config set-credentials system:node:${instance} \\ --client-certificate=../certs/${instance}.pem \\ --client-key=../certs/${instance}-key.pem \\ --embed-certs=true \\ --kubeconfig=${instance}.kubeconfigkubectl config set-context default \\ --cluster=kubernetes-from-scratch \\ --user=system:node:${instance} \\ --kubeconfig=${instance}.kubeconfigkubectl config use-context default --kubeconfig=${instance}.kubeconfigdoneThe kube-proxy Kubernets Configuration FileGenerate a kubeconfig file for the kube-proxy service:kubectl config set-cluster kubernetes-from-scratch \\ --certificate-authority=../certs/ca.pem \\ --embed-certs=true \\ --server=https://${KUBERNETES_PUBLIC_ADDRESS}:443 \\ --kubeconfig=kube-proxy.kubeconfigkubectl config set-credentials system:kube-proxy \\ --client-certificate=../certs/kube-proxy.pem \\ --client-key=../certs/kube-proxy-key.pem \\ --embed-certs=true \\ --kubeconfig=kube-proxy.kubeconfigkubectl config set-context default \\ --cluster=kubernetes-from-scratch \\ --user=system:kube-proxy \\ --kubeconfig=kube-proxy.kubeconfigkubectl config use-context default --kubeconfig=kube-proxy.kubeconfigThe kube-controller-manager Kubernetes Configuration FileGenerate a kubeconfig file for the kube-controller-manager service:kubectl config set-cluster kubernetes-from-scratch \\ --certificate-authority=../certs/ca.pem \\ --embed-certs=true \\ --server=https://127.0.0.1:6443 \\ --kubeconfig=kube-controller-manager.kubeconfigkubectl config set-credentials system:kube-controller-manager \\ --client-certificate=../certs/kube-controller-manager.pem \\ --client-key=../certs/kube-controller-manager-key.pem \\ --embed-certs=true \\ --kubeconfig=kube-controller-manager.kubeconfigkubectl config set-context default \\ --cluster=kubernetes-from-scratch \\ --user=system:kube-controller-manager \\ --kubeconfig=kube-controller-manager.kubeconfigkubectl config use-context default --kubeconfig=kube-controller-manager.kubeconfigThe kube-scheduler Kubernetes Configuration FileGenerate a kubeconfig file for the kube-scheduler service:kubectl config set-cluster kubernetes-from-scratch \\ --certificate-authority=../certs/ca.pem \\ --embed-certs=true \\ --server=https://127.0.0.1:6443 \\ --kubeconfig=kube-scheduler.kubeconfigkubectl config set-credentials system:kube-scheduler \\ --client-certificate=../certs/kube-scheduler.pem \\ --client-key=../certs/kube-scheduler-key.pem \\ --embed-certs=true \\ --kubeconfig=kube-scheduler.kubeconfigkubectl config set-context default \\ --cluster=kubernetes-from-scratch \\ --user=system:kube-scheduler \\ --kubeconfig=kube-scheduler.kubeconfigkubectl config use-context default --kubeconfig=kube-scheduler.kubeconfigThe admin Kubernetes Configuration FileGenerate a kubeconfig file for the admin user:kubectl config set-cluster kubernetes-from-scratch \\ --certificate-authority=../certs/ca.pem \\ --embed-certs=true \\ --server=https://127.0.0.1:6443 \\ --kubeconfig=admin.kubeconfigkubectl config set-credentials admin \\ --client-certificate=../certs/admin.pem \\ --client-key=../certs/admin-key.pem \\ --embed-certs=true \\ --kubeconfig=admin.kubeconfigkubectl config set-context default \\ --cluster=kubernetes-from-scratch \\ --user=admin \\ --kubeconfig=admin.kubeconfigkubectl config use-context default --kubeconfig=admin.kubeconfigDistribute the Kubernetes Configuration FilesCopy the kubelet and kube-proxy kubeconfig files to each worker instance:for instance in worker-0 worker-1 worker-2; do external_ip=$(aws ec2 describe-instances --filters \\ \"Name=tag:Name,Values=${instance}\" \\ \"Name=instance-state-name,Values=running\" \\ --output text --query 'Reservations[].Instances[].PublicIpAddress')scp -i ../kubernetes.id_rsa \\ ${instance}.kubeconfig kube-proxy.kubeconfig ubuntu@${external_ip}:~/doneTake note, we are in the configs/ folder, so to I added the ../kubernetes.id_rsa — which is one directory up from where we were in case you are getting permission denied errors.Copy the kube-controller-manager and kube-scheduler kubeconfig files to each controller instance:for instance in controller-0 controller-1 controller-2; do external_ip=$(aws ec2 describe-instances --filters \\ \"Name=tag:Name,Values=${instance}\" \\ \"Name=instance-state-name,Values=running\" \\ --output text --query 'Reservations[].Instances[].PublicIpAddress') scp -i ../kubernetes.id_rsa \\ admin.kubeconfig kube-controller-manager.kubeconfig kube-scheduler.kubeconfig ubuntu@${external_ip}:~/doneTake note, we are in the configs/ folder, so to I added the ../kubernetes.id_rsa — which is one directory up from where we were in case you are getting permission denied errors.Generating the Data Encryption Config and KeyKubernetes stores a variety of data including cluster state, application configurations, and secrets. Kubernetes supports the ability to encrypt cluster data at rest.For this we need to generate an encryption key and an encryption config suitable for encrypting Kubernetes Secrets.For this we will go back to our home folder and create a folder called encryption and then change into that folder:cd ~/mkdir -p encryptioncd encryption/The Encryption KeyGenerate an encryption key:ENCRYPTION_KEY=$(head -c 32 /dev/urandom | base64)The Encryption ConfigCreate the encryption-config.yaml encryption config file:cat &gt; encryption-config.yaml &lt;&lt;EOFkind: EncryptionConfigapiVersion: v1resources: - resources: - secrets providers: - aescbc: keys: - name: key1 secret: ${ENCRYPTION_KEY} - identity: {}EOFCopy the encryption-config.yaml encryption config file to each controller instance:for instance in controller-0 controller-1 controller-2; do external_ip=$(aws ec2 describe-instances --filters \\ \"Name=tag:Name,Values=${instance}\" \\ \"Name=instance-state-name,Values=running\" \\ --output text --query 'Reservations[].Instances[].PublicIpAddress') scp -i ../kubernetes.id_rsa encryption-config.yaml ubuntu@${external_ip}:~/doneBootstrapping the etcd ClusterLet’s first return to our home directory:cd ~/Kubernetes components are stateless and store cluster state in etcd. We are going to provision a three node etcd cluster and configure it for HA (High Availability) and secure remote access.The next command will generate our SSH command line arguments to be able to connect to our controller instances:for instance in controller-0 controller-1 controller-2; do external_ip=$(aws ec2 describe-instances --filters \\ \"Name=tag:Name,Values=${instance}\" \\ \"Name=instance-state-name,Values=running\" \\ --output text --query 'Reservations[].Instances[].PublicIpAddress')echo ssh -i kubernetes.id_rsa ubuntu@$external_ipdoneYou can now use tmux to create multiple panes and connect to each instance. I am using AWS CloudShell, so I will be creating separate rows:Bootstrapping and etcd Cluster MemberAfter logging in to each controller node, we now need to download the official etcd release binaries from the etcd GitHub project (run this on each node):wget -q --timestamping \\ \"https://github.com/etcd-io/etcd/releases/download/v3.4.15/etcd-v3.4.15-linux-amd64.tar.gz\"tar -xvf etcd-v3.4.15-linux-amd64.tar.gzsudo mv etcd-v3.4.15-linux-amd64/etcd* /usr/local/bin/Configure the etcd ServerNext we create the necessary configuration folders and copy over the certificates and keys. We also get the internal IP address of the node to use in the configuration files. We also need to set a unique name within an etcd cluster. Remember to run this one each controller instance:sudo mkdir -p /etc/etcd /var/lib/etcdsudo chmod 700 /var/lib/etcdsudo cp ca.pem kubernetes-key.pem kubernetes.pem /etc/etcd/INTERNAL_IP=$(curl -s http://169.254.169.254/latest/meta-data/local-ipv4)ETCD_NAME=$(curl -s http://169.254.169.254/latest/user-data/ \\ | tr \"|\" \"\\n\" | grep \"^name\" | cut -d\"=\" -f2)echo \"${ETCD_NAME}\"We want to encure etcd is started on each controller at boot time, so we need to create an etcd.service systemd unit file, as well as enable and start the etcd service (remember to run this on each controller node):cat &lt;&lt;EOF | sudo tee /etc/systemd/system/etcd.service[Unit]Description=etcdDocumentation=https://github.com/coreos[Service]Type=notifyExecStart=/usr/local/bin/etcd \\\\ --name ${ETCD_NAME} \\\\ --cert-file=/etc/etcd/kubernetes.pem \\\\ --key-file=/etc/etcd/kubernetes-key.pem \\\\ --peer-cert-file=/etc/etcd/kubernetes.pem \\\\ --peer-key-file=/etc/etcd/kubernetes-key.pem \\\\ --trusted-ca-file=/etc/etcd/ca.pem \\\\ --peer-trusted-ca-file=/etc/etcd/ca.pem \\\\ --peer-client-cert-auth \\\\ --client-cert-auth \\\\ --initial-advertise-peer-urls https://${INTERNAL_IP}:2380 \\\\ --listen-peer-urls https://${INTERNAL_IP}:2380 \\\\ --listen-client-urls https://${INTERNAL_IP}:2379,https://127.0.0.1:2379 \\\\ --advertise-client-urls https://${INTERNAL_IP}:2379 \\\\ --initial-cluster-token etcd-cluster-0 \\\\ --initial-cluster controller-0=https://10.0.1.10:2380,controller-1=https://10.0.1.11:2380,controller-2=https://10.0.1.12:2380 \\\\ --initial-cluster-state new \\\\ --data-dir=/var/lib/etcdRestart=on-failureRestartSec=5[Install]WantedBy=multi-user.targetEOFsudo systemctl daemon-reloadsudo systemctl enable etcdsudo systemctl start etcdYou can check the status of the etcd service by running:sudo service etcd statusWe can also verify the etcd cluster members by running:sudo ETCDCTL_API=3 etcdctl member list \\ --endpoints=https://127.0.0.1:2379 \\ --cacert=/etc/etcd/ca.pem \\ --cert=/etc/etcd/kubernetes.pem \\ --key=/etc/etcd/kubernetes-key.pemBootstrapping the Kubernetes Control PlaneWe will be installing the following components on each of our controller nodes: Kubernetes API Server, Scheduler, and Controller Manager.Ensure that you have logged into each of the controller nodes via SSH, you can get the SSH command list by running:for instance in controller-0 controller-1 controller-2; do external_ip=$(aws ec2 describe-instances --filters \\ \"Name=tag:Name,Values=${instance}\" \\ \"Name=instance-state-name,Values=running\" \\ --output text --query 'Reservations[].Instances[].PublicIpAddress')echo ssh -i kubernetes.id_rsa ubuntu@$external_ipdoneProvisioning the Kubernetes Control PlaneWe will be creating our directory structure, as well as downloading and installing the binaries and copying over our certificates and keys that we need. We also determin the internal IP address of our node so we can use it in our configurations.Remember to run this on each one of our controller nodes:sudo mkdir -p /etc/kubernetes/configsudo mkdir -p /var/lib/kubernetes/sudo mv ca.pem ca-key.pem kubernetes-key.pem kubernetes.pem \\ service-account-key.pem service-account.pem \\ encryption-config.yaml /var/lib/kubernetes/wget -q --show-progress --https-only --timestamping \\ \"https://storage.googleapis.com/kubernetes-release/release/v1.21.0/bin/linux/amd64/kube-apiserver\" \\ \"https://storage.googleapis.com/kubernetes-release/release/v1.21.0/bin/linux/amd64/kube-controller-manager\" \\ \"https://storage.googleapis.com/kubernetes-release/release/v1.21.0/bin/linux/amd64/kube-scheduler\" \\ \"https://storage.googleapis.com/kubernetes-release/release/v1.21.0/bin/linux/amd64/kubectl\"chmod +x kube-apiserver kube-controller-manager kube-scheduler kubectlsudo mv kube-apiserver kube-controller-manager kube-scheduler kubectl /usr/local/bin/INTERNAL_IP=$(curl -s http://169.254.169.254/latest/meta-data/local-ipv4)We want to encure kube-apiserver service is started on each controller at boot time, so we need to create an kube-apiserver.service systemd unit file (remember to run this on each controller node):cat &lt;&lt;EOF | sudo tee /etc/systemd/system/kube-apiserver.service[Unit]Description=Kubernetes API ServerDocumentation=https://github.com/kubernetes/kubernetes[Service]ExecStart=/usr/local/bin/kube-apiserver \\\\ --advertise-address=${INTERNAL_IP} \\\\ --allow-privileged=true \\\\ --apiserver-count=3 \\\\ --audit-log-maxage=30 \\\\ --audit-log-maxbackup=3 \\\\ --audit-log-maxsize=100 \\\\ --audit-log-path=/var/log/audit.log \\\\ --authorization-mode=Node,RBAC \\\\ --bind-address=0.0.0.0 \\\\ --client-ca-file=/var/lib/kubernetes/ca.pem \\\\ --enable-admission-plugins=NamespaceLifecycle,NodeRestriction,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota \\\\ --etcd-cafile=/var/lib/kubernetes/ca.pem \\\\ --etcd-certfile=/var/lib/kubernetes/kubernetes.pem \\\\ --etcd-keyfile=/var/lib/kubernetes/kubernetes-key.pem \\\\ --etcd-servers=https://10.0.1.10:2379,https://10.0.1.11:2379,https://10.0.1.12:2379 \\\\ --event-ttl=1h \\\\ --encryption-provider-config=/var/lib/kubernetes/encryption-config.yaml \\\\ --kubelet-certificate-authority=/var/lib/kubernetes/ca.pem \\\\ --kubelet-client-certificate=/var/lib/kubernetes/kubernetes.pem \\\\ --kubelet-client-key=/var/lib/kubernetes/kubernetes-key.pem \\\\ --runtime-config='api/all=true' \\\\ --service-account-key-file=/var/lib/kubernetes/service-account.pem \\\\ --service-account-signing-key-file=/var/lib/kubernetes/service-account-key.pem \\\\ --service-account-issuer=https://${KUBERNETES_PUBLIC_ADDRESS}:443 \\\\ --service-cluster-ip-range=10.32.0.0/24 \\\\ --service-node-port-range=30000-32767 \\\\ --tls-cert-file=/var/lib/kubernetes/kubernetes.pem \\\\ --tls-private-key-file=/var/lib/kubernetes/kubernetes-key.pem \\\\ --v=2Restart=on-failureRestartSec=5[Install]WantedBy=multi-user.targetEOFConfigure the Kubernetes Controller ManagerMove the kube-controller-manager kubeconfig into place (remember to run this on each controller node):sudo mv kube-controller-manager.kubeconfig /var/lib/kubernetes/Create the kube-controller-manager.service systemd unit file (remember to run this on each controller node):cat &lt;&lt;EOF | sudo tee /etc/systemd/system/kube-controller-manager.service[Unit]Description=Kubernetes Controller ManagerDocumentation=https://github.com/kubernetes/kubernetes[Service]ExecStart=/usr/local/bin/kube-controller-manager \\\\ --bind-address=0.0.0.0 \\\\ --cluster-cidr=10.200.0.0/16 \\\\ --cluster-name=kubernetes \\\\ --cluster-signing-cert-file=/var/lib/kubernetes/ca.pem \\\\ --cluster-signing-key-file=/var/lib/kubernetes/ca-key.pem \\\\ --kubeconfig=/var/lib/kubernetes/kube-controller-manager.kubeconfig \\\\ --leader-elect=true \\\\ --root-ca-file=/var/lib/kubernetes/ca.pem \\\\ --service-account-private-key-file=/var/lib/kubernetes/service-account-key.pem \\\\ --service-cluster-ip-range=10.32.0.0/24 \\\\ --use-service-account-credentials=true \\\\ --v=2Restart=on-failureRestartSec=5[Install]WantedBy=multi-user.targetEOFConfigure the Kubernetes SchedulerMove the kube-scheduler kubeconfig into place (remember to run this on each controller node):sudo mv kube-scheduler.kubeconfig /var/lib/kubernetes/Create the kube-scheuduler.yaml configuration file:cat &lt;&lt;EOF | sudo tee /etc/kubernetes/config/kube-scheduler.yamlapiVersion: kubescheduler.config.k8s.io/v1beta1kind: KubeSchedulerConfigurationclientConnection: kubeconfig: \"/var/lib/kubernetes/kube-scheduler.kubeconfig\"leaderElection: leaderElect: trueEOFCreate the kube-scheduler.service systemd unit file:cat &lt;&lt;EOF | sudo tee /etc/systemd/system/kube-scheduler.service[Unit]Description=Kubernetes SchedulerDocumentation=https://github.com/kubernetes/kubernetes[Service]ExecStart=/usr/local/bin/kube-scheduler \\\\ --config=/etc/kubernetes/config/kube-scheduler.yaml \\\\ --v=2Restart=on-failureRestartSec=5[Install]WantedBy=multi-user.targetEOFEnable start-on-boot and Start the Controller ServicesRemember to run this on each controller node:sudo systemctl daemon-reloadsudo systemctl enable kube-apiserver kube-controller-manager kube-schedulersudo systemctl start kube-apiserver kube-controller-manager kube-schedulerAdd Host File EntriesIn order for kubectl exec commands to work, the controller nodes must each be able to resolve the worker hostnames. This is not set up by default in AWS. The workaround is to add manual host entries on each of the controller nodes:cat &lt;&lt;EOF | sudo tee -a /etc/hosts10.0.1.20 ip-10-0-1-2010.0.1.21 ip-10-0-1-2110.0.1.22 ip-10-0-1-22EOFIf this step is missed, the DNS Cluster Add-on testing will fail with an error like:Error from server: error dialing backend: dial tcp: lookup ip-10-0-1-22 on 127.0.0.53:53: server misbehavingVerify that you can access the Kubernetes cluster on the control plane nodeskubectl cluster-info --kubeconfig admin.kubeconfigRBAC For Kubelet AuthorizationWe need to setup roles and permissions for the Kubernetes API Server to access the Kubelet API on each worker node. Access to the Kubelet API is required for retrieving metrics, logs and executing commands in pods.The commands in this section will affect the entire cluster and only need to be run once from one of the controller nodes, the command below will print out the SSH command that you should use to connect to an instance:external_ip=$(aws ec2 describe-instances --filters \\ \"Name=tag:Name,Values=controller-0\" \\ \"Name=instance-state-name,Values=running\" \\ --output text --query 'Reservations[].Instances[].PublicIpAddress')ssh -i kubernetes.id_rsa ubuntu@${external_ip}Create the system:kube-apiserver-to-kubelet ClusterRole with permissions to acccess the Kubelet API and perform some common tasks associated with managing pods:cat &lt;&lt;EOF | kubectl apply --kubeconfig admin.kubeconfig -f -apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRolemetadata: annotations: rbac.authorization.kubernetes.io/autoupdate: \"true\" labels: kubernetes.io/bootstrapping: rbac-defaults name: system:kube-apiserver-to-kubeletrules: - apiGroups: - \"\" resources: - nodes/proxy - nodes/stats - nodes/log - nodes/spec - nodes/metrics verbs: - \"*\"EOFBind the system:kube-apiserver-to-kubelet ClusterRole to the kubernetes user:cat &lt;&lt;EOF | kubectl apply --kubeconfig admin.kubeconfig -f -apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata: name: system:kube-apiserver namespace: \"\"roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:kube-apiserver-to-kubeletsubjects: - apiGroup: rbac.authorization.k8s.io kind: User name: kubernetesEOFVerify the cluster public endpointLog out of the SSH connection and head back to the main terminal window as you need to execute AWS commands to verify the public endpoint remotely:KUBERNETES_PUBLIC_ADDRESS=$(aws elbv2 describe-load-balancers \\ --load-balancer-arns ${LOAD_BALANCER_ARN} \\ --output text --query 'LoadBalancers[].DNSName')curl --cacert certs/ca.pem https://${KUBERNETES_PUBLIC_ADDRESS}/versionBootstrapping the Kubernetes Worker NodesThe following components will be installed on each node: runc, container networking plugins, containerd, kubelet, and kube-proxy.The commands need to be run on each worker instance. Let’s generate our SSH command list:for instance in worker-0 worker-1 worker-2; do external_ip=$(aws ec2 describe-instances --filters \\ \"Name=tag:Name,Values=${instance}\" \\ \"Name=instance-state-name,Values=running\" \\ --output text --query 'Reservations[].Instances[].PublicIpAddress')echo ssh -i kubernetes.id_rsa ubuntu@$external_ipdoneSSH into each instance in a separate pane.Provision a Kubernetes Worker NodeInstall the OS dependencies, disable swap and download and install the worker binaries:sudo apt-get updatesudo apt-get -y install socat conntrack ipsetsudo swapon --showsudo swapoff -awget -q --show-progress --https-only --timestamping \\ https://github.com/kubernetes-sigs/cri-tools/releases/download/v1.21.0/crictl-v1.21.0-linux-amd64.tar.gz \\ https://github.com/opencontainers/runc/releases/download/v1.0.0-rc93/runc.amd64 \\ https://github.com/containernetworking/plugins/releases/download/v0.9.1/cni-plugins-linux-amd64-v0.9.1.tgz \\ https://github.com/containerd/containerd/releases/download/v1.4.4/containerd-1.4.4-linux-amd64.tar.gz \\ https://storage.googleapis.com/kubernetes-release/release/v1.21.0/bin/linux/amd64/kubectl \\ https://storage.googleapis.com/kubernetes-release/release/v1.21.0/bin/linux/amd64/kube-proxy \\ https://storage.googleapis.com/kubernetes-release/release/v1.21.0/bin/linux/amd64/kubeletsudo mkdir -p \\ /etc/cni/net.d \\ /opt/cni/bin \\ /var/lib/kubelet \\ /var/lib/kube-proxy \\ /var/lib/kubernetes \\ /var/run/kubernetesmkdir containerdtar -xvf crictl-v1.21.0-linux-amd64.tar.gztar -xvf containerd-1.4.4-linux-amd64.tar.gz -C containerdsudo tar -xvf cni-plugins-linux-amd64-v0.9.1.tgz -C /opt/cni/bin/sudo mv runc.amd64 runcchmod +x crictl kubectl kube-proxy kubelet runc sudo mv crictl kubectl kube-proxy kubelet runc /usr/local/bin/sudo mv containerd/bin/* /bin/Configure the CNI NetworkingRetrieve the Pod CIDR range for teh current compute instance. Create a network bridge and loopback configuration (remember to run on each worker node):POD_CIDR=$(curl -s http://169.254.169.254/latest/user-data/ \\ | tr \"|\" \"\\n\" | grep \"^pod-cidr\" | cut -d\"=\" -f2)echo \"${POD_CIDR}\"cat &lt;&lt;EOF | sudo tee /etc/cni/net.d/10-bridge.conf{ \"cniVersion\": \"0.4.0\", \"name\": \"bridge\", \"type\": \"bridge\", \"bridge\": \"cnio0\", \"isGateway\": true, \"ipMasq\": true, \"ipam\": { \"type\": \"host-local\", \"ranges\": [ [{\"subnet\": \"${POD_CIDR}\"}] ], \"routes\": [{\"dst\": \"0.0.0.0/0\"}] }}EOFcat &lt;&lt;EOF | sudo tee /etc/cni/net.d/99-loopback.conf{ \"cniVersion\": \"0.4.0\", \"name\": \"lo\", \"type\": \"loopback\"}EOFConfigure containerdCreate a containerd configuration file and create the containerd.service systemd unit file (remember to run on each worker node):sudo mkdir -p /etc/containerd/cat &lt;&lt; EOF | sudo tee /etc/containerd/config.toml[plugins] [plugins.cri.containerd] snapshotter = \"overlayfs\" [plugins.cri.containerd.default_runtime] runtime_type = \"io.containerd.runtime.v1.linux\" runtime_engine = \"/usr/local/bin/runc\" runtime_root = \"\"EOFcat &lt;&lt;EOF | sudo tee /etc/systemd/system/containerd.service[Unit]Description=containerd container runtimeDocumentation=https://containerd.ioAfter=network.target[Service]ExecStartPre=/sbin/modprobe overlayExecStart=/bin/containerdRestart=alwaysRestartSec=5Delegate=yesKillMode=processOOMScoreAdjust=-999LimitNOFILE=1048576LimitNPROC=infinityLimitCORE=infinity[Install]WantedBy=multi-user.targetEOFConfigure the KubeletMove and store the certificate and keys and create the necessary folders and configuration (remember to run on each worker node):WORKER_NAME=$(curl -s http://169.254.169.254/latest/user-data/ \\| tr \"|\" \"\\n\" | grep \"^name\" | cut -d\"=\" -f2)echo \"${WORKER_NAME}\"sudo mv ${WORKER_NAME}-key.pem ${WORKER_NAME}.pem /var/lib/kubelet/sudo mv ${WORKER_NAME}.kubeconfig /var/lib/kubelet/kubeconfigsudo mv ca.pem /var/lib/kubernetes/cat &lt;&lt;EOF | sudo tee /var/lib/kubelet/kubelet-config.yamlkind: KubeletConfigurationapiVersion: kubelet.config.k8s.io/v1beta1authentication: anonymous: enabled: false webhook: enabled: true x509: clientCAFile: \"/var/lib/kubernetes/ca.pem\"authorization: mode: WebhookclusterDomain: \"cluster.local\"clusterDNS: - \"10.32.0.10\"podCIDR: \"${POD_CIDR}\"resolvConf: \"/run/systemd/resolve/resolv.conf\"runtimeRequestTimeout: \"15m\"tlsCertFile: \"/var/lib/kubelet/${WORKER_NAME}.pem\"tlsPrivateKeyFile: \"/var/lib/kubelet/${WORKER_NAME}-key.pem\"EOFThe resolveConf configuration is used to avoid loops when using CoreDNS for service discovery on systems running systemd-resolved.Create the kubelet.service systemd unit file (remember to run on each worker node):cat &lt;&lt;EOF | sudo tee /etc/systemd/system/kubelet.service[Unit]Description=Kubernetes KubeletDocumentation=https://github.com/kubernetes/kubernetesAfter=containerd.serviceRequires=containerd.service[Service]ExecStart=/usr/local/bin/kubelet \\\\ --config=/var/lib/kubelet/kubelet-config.yaml \\\\ --container-runtime=remote \\\\ --container-runtime-endpoint=unix:///var/run/containerd/containerd.sock \\\\ --image-pull-progress-deadline=2m \\\\ --kubeconfig=/var/lib/kubelet/kubeconfig \\\\ --network-plugin=cni \\\\ --register-node=true \\\\ --v=2Restart=on-failureRestartSec=5[Install]WantedBy=multi-user.targetEOFConfigure the Kubernetes ProxyMove the kube-proxy kubeconfig into place, create the kube-proxy-config and finally create the kube-proxy.service systemd unit file (remember to run on each worker node):sudo mv kube-proxy.kubeconfig /var/lib/kube-proxy/kubeconfigcat &lt;&lt;EOF | sudo tee /var/lib/kube-proxy/kube-proxy-config.yamlkind: KubeProxyConfigurationapiVersion: kubeproxy.config.k8s.io/v1alpha1clientConnection: kubeconfig: \"/var/lib/kube-proxy/kubeconfig\"mode: \"iptables\"clusterCIDR: \"10.200.0.0/16\"EOFcat &lt;&lt;EOF | sudo tee /etc/systemd/system/kube-proxy.service[Unit]Description=Kubernetes Kube ProxyDocumentation=https://github.com/kubernetes/kubernetes[Service]ExecStart=/usr/local/bin/kube-proxy \\\\ --config=/var/lib/kube-proxy/kube-proxy-config.yamlRestart=on-failureRestartSec=5[Install]WantedBy=multi-user.targetEOFEnable start-at-boot and Start the Worker ServicesRemember to run on each worker node:sudo systemctl daemon-reloadsudo systemctl enable containerd kubelet kube-proxysudo systemctl start containerd kubelet kube-proxyVerify all is workingSwitch back to you terminal that has the AWS permissions (not the logged in worker node) and run:external_ip=$(aws ec2 describe-instances --filters \\ \"Name=tag:Name,Values=controller-0\" \\ \"Name=instance-state-name,Values=running\" \\ --output text --query 'Reservations[].Instances[].PublicIpAddress')ssh -i kubernetes.id_rsa ubuntu@${external_ip} kubectl get nodes --kubeconfig admin.kubeconfigWe can see the 3 worker nodes up and running and are Ready.Configuring kubectl for Remote AccessWe will generate a kubeconfig file for the kubectl command line utility based on the admin user credentials.Each kubeconfig requires a Kubernetes API Server to connect to. To support HA (High Availability), the DNS name of the external load balancer fronting the Kubernetes API servers will be used.Generate a kubeconfig file suitable for authenticating as the admin user:cd ~KUBERNETES_PUBLIC_ADDRESS=$(aws elbv2 describe-load-balancers \\--load-balancer-arns ${LOAD_BALANCER_ARN} \\--output text --query 'LoadBalancers[].DNSName')kubectl config set-cluster kubernetes-from-scratch \\ --certificate-authority=certs/ca.pem \\ --embed-certs=true \\ --server=https://${KUBERNETES_PUBLIC_ADDRESS}:443kubectl config set-credentials admin \\ --client-certificate=certs/admin.pem \\ --client-key=certs/admin-key.pemkubectl config set-context kubernetes-from-scratch \\ --cluster=kubernetes-from-scratch \\ --user=adminkubectl config use-context kubernetes-from-scratchVerify all is goodCheck the version of teh remote Kubernetes cluster:kubectl versionkubectl get nodeskubectl config viewProvisioning Pod Network RoutesPods scheduled to a node receive an IP address from the node’s Pod CIDR range. At this point pods can not communicate with other pods running on different nodes due to missing network routes.We will create a route for each worker node that maps the node’s Pod CIDR range to the node’s internal IP address.However in production workloads, this functionality will be provided by CNI plugins like flannel, calico, amazon-vpc-cni-k8s. Doing this by hand makes it easier to understand what those plugins do behind the scenes.Print the internal IP address and Pod CIDR range for each worker instance and create route table entries:for instance in worker-0 worker-1 worker-2; do instance_id_ip=\"$(aws ec2 describe-instances \\ --filters \"Name=tag:Name,Values=${instance}\" \\ --output text --query 'Reservations[].Instances[].[InstanceId,PrivateIpAddress]')\" instance_id=\"$(echo \"${instance_id_ip}\" | cut -f1)\" instance_ip=\"$(echo \"${instance_id_ip}\" | cut -f2)\" pod_cidr=\"$(aws ec2 describe-instance-attribute \\ --instance-id \"${instance_id}\" \\ --attribute userData \\ --output text --query 'UserData.Value' \\ | base64 --decode | tr \"|\" \"\\n\" | grep \"^pod-cidr\" | cut -d'=' -f2)\" echo \"${instance_ip} ${pod_cidr}\"aws ec2 create-route \\ --route-table-id \"${ROUTE_TABLE_ID}\" \\ --destination-cidr-block \"${pod_cidr}\" \\ --instance-id \"${instance_id}\"doneValidate RoutesValidate network routes for each worker instance:aws ec2 describe-route-tables \\ --route-table-ids \"${ROUTE_TABLE_ID}\" \\ --query 'RouteTables[].Routes'Deploying the DNS Cluster Add-onWe will now deploy the DNS add-on which provides DNS based service discovery, backed by CoreDNS, to applications running inside the Kubernetes cluster.Deploy the coredns cluster add-on:kubectl apply -f https://storage.googleapis.com/kubernetes-the-hard-way/coredns-1.8.yamlList pods created by the core-dns deployment:kubectl get pods -l k8s-app=kube-dns -n kube-systemVerify all is okCreate a busybox deployment and then list the pods:kubectl run busybox --image=busybox:1.28 --command -- sleep 3600kubectl get pods -l run=busyboxRetrieve the full name of the busybox pod and perform a DNS lookup for the kubernetes service inside the busybox pod:POD_NAME=$(kubectl get pods -l run=busybox -o jsonpath=\"{.items[0].metadata.name}\")kubectl exec -ti $POD_NAME -- nslookup kubernetesSmoke TestData EncryptionVerify the ability to encrypt secret data at rest by creating a generic secret and then using hexdump to inspect the entry stored in etcd:kubectl create secret generic kubernetes-from-scratch \\ --from-literal=\"mykey=mydata\"external_ip=$(aws ec2 describe-instances --filters \\ \"Name=tag:Name,Values=controller-0\" \\ \"Name=instance-state-name,Values=running\" \\ --output text --query 'Reservations[].Instances[].PublicIpAddress')ssh -i kubernetes.id_rsa ubuntu@${external_ip} \\ \"sudo ETCDCTL_API=3 etcdctl get \\ --endpoints=https://127.0.0.1:2379 \\ --cacert=/etc/etcd/ca.pem \\ --cert=/etc/etcd/kubernetes.pem \\ --key=/etc/etcd/kubernetes-key.pem\\ /registry/secrets/default/kubernetes-from-scratch | hexdump -C\"The etcd key should be prefixed with k8s:enc:aescbc:v1:key1, which indicates the aesbc provider was used to encrypt the data with the key1 encryption key.Creating DeploymentsDeploy an nginx web server and list the pods:kubectl create deployment nginx --image=nginxkubectl get pods -l app=nginxPort ForwardingPOD_NAME=$(kubectl get pods -l app=nginx -o jsonpath=\"{.items[0].metadata.name}\")kubectl port-forward $POD_NAME 8080:80In a new terminal window perform an HTTP request to the port forwarded endpoint:curl --head http://127.0.0.1:8080LogsPrint the nginx pod logs:kubectl logs $POD_NAMEExecVerify we are able to execute commands in a container:kubectl exec -ti $POD_NAME -- nginx -vServicesExpose the nginx deployment using a NodePort service:kubectl expose deployment nginx --port 80 --type NodePortNODE_PORT=$(kubectl get svc nginx \\ --output=jsonpath='{range .spec.ports[0]}{.nodePort}')Create a firewall rule that allows remote access to the nginx node port:aws ec2 authorize-security-group-ingress \\ --group-id ${SECURITY_GROUP_ID} \\ --protocol tcp \\ --port ${NODE_PORT} \\ --cidr 0.0.0.0/0Get the worker node name where the nginx pod is running, and retrieve the external IP address of a worker instance, finally make an HTTP request to the external IP address and the nginx node port:INSTANCE_NAME=$(kubectl get pod $POD_NAME --output=jsonpath='{.spec.nodeName}')EXTERNAL_IP=$(aws ec2 describe-instances --filters \\ \"Name=instance-state-name,Values=running\" \\ \"Name=network-interface.private-dns-name,Values=${INSTANCE_NAME}.*.internal*\" \\ --output text --query 'Reservations[].Instances[].PublicIpAddress')curl -I http://${EXTERNAL_IP}:${NODE_PORT}Bonus: Scanning for Vulnerabilities using kube-hunterkube-hunter hunts for security weaknesses in Kubernetes clusters. The tool was developed to increase awareness and visibility for security issues in Kubernetes environments.Deploy a job right into the cluster to scan for vulnerabilities:cat &lt;&lt;EOF | kubectl apply --kubeconfig admin.kubeconfig -f -apiVersion: batch/v1kind: Jobmetadata: name: kube-hunterspec: template: metadata: labels: app: kube-hunter spec: containers: - name: kube-hunter image: aquasec/kube-hunter:0.6.8 command: [\"kube-hunter\"] args: [\"--pod\"] restartPolicy: NeverEOFLet’s now check the logs and the output produced by kubehunter:kubectl logs --kubeconfig admin.kubeconfig kube-hunter-tws6b &gt; logs.txtcat logs.txt2022-07-08 14:22:13,466 INFO kube_hunter.modules.report.collector Started hunting2022-07-08 14:22:13,471 INFO kube_hunter.modules.report.collector Discovering Open Kubernetes Services2022-07-08 14:22:13,478 INFO kube_hunter.modules.report.collector Found vulnerability \"CAP_NET_RAW Enabled\" in Local to Pod (kube-hunter-tws6b)2022-07-08 14:22:13,478 INFO kube_hunter.modules.report.collector Found vulnerability \"Read access to pod's service account token\" in Local to Pod (kube-hunter-tws6b)2022-07-08 14:22:13,479 INFO kube_hunter.modules.report.collector Found vulnerability \"Access to pod's secrets\" in Local to Pod (kube-hunter-tws6b)2022-07-08 14:22:13,515 INFO kube_hunter.modules.report.collector Found vulnerability \"AWS Metadata Exposure\" in Local to Pod (kube-hunter-tws6b)2022-07-08 14:22:13,678 INFO kube_hunter.modules.report.collector Found open service \"Kubelet API\" at 10.0.1.20:102502022-07-08 14:22:13,691 INFO kube_hunter.modules.report.collector Found open service \"Kubelet API\" at 10.0.1.21:102502022-07-08 14:22:13,694 INFO kube_hunter.modules.report.collector Found open service \"Etcd\" at 10.0.1.10:23792022-07-08 14:22:13,695 INFO kube_hunter.modules.report.collector Found open service \"Kubelet API\" at 10.0.1.22:102502022-07-08 14:22:13,750 INFO kube_hunter.modules.report.collector Found open service \"Etcd\" at 10.0.1.12:23792022-07-08 14:22:13,754 INFO kube_hunter.modules.report.collector Found open service \"Etcd\" at 10.0.1.11:23792022-07-08 14:22:13,817 INFO kube_hunter.modules.report.collector Found open service \"Kubelet API\" at 10.200.2.1:102502022-07-08 14:22:13,821 INFO kube_hunter.modules.report.collector Found open service \"Metrics Server\" at 10.0.1.10:64432022-07-08 14:22:13,825 INFO kube_hunter.modules.report.collector Found open service \"Metrics Server\" at 10.0.1.12:64432022-07-08 14:22:13,857 INFO kube_hunter.modules.report.collector Found open service \"Metrics Server\" at 10.0.1.11:64432022-07-08 14:22:21,218 INFO kube_hunter.modules.report.collector Found open service \"Metrics Server\" at 10.0.1.72:4432022-07-08 14:22:21,406 INFO kube_hunter.modules.report.collector Found open service \"API Server\" at 10.32.0.1:4432022-07-08 14:22:21,418 INFO kube_hunter.modules.report.collector Found vulnerability \"K8s Version Disclosure\" in 10.32.0.1:4432022-07-08 14:22:21,424 INFO kube_hunter.modules.report.collector Found vulnerability \"Access to API using service account token\" in 10.32.0.1:443Nodes+-------------+------------+| TYPE | LOCATION |+-------------+------------+| Node/Master | 10.200.2.1 |+-------------+------------+| Node/Master | 10.32.0.1 |+-------------+------------+| Node/Master | 10.0.1.72 |+-------------+------------+| Node/Master | 10.0.1.22 |+-------------+------------+| Node/Master | 10.0.1.21 |+-------------+------------+| Node/Master | 10.0.1.20 |+-------------+------------+| Node/Master | 10.0.1.12 |+-------------+------------+| Node/Master | 10.0.1.11 |+-------------+------------+| Node/Master | 10.0.1.10 |+-------------+------------+Detected Services+----------------+------------------+----------------------+| SERVICE | LOCATION | DESCRIPTION |+----------------+------------------+----------------------+| Metrics Server | 10.0.1.72:443 | The Metrics server || | | is in charge of || | | providing resource || | | usage metrics for || | | pods and nodes to || | | the API server |+----------------+------------------+----------------------+| Metrics Server | 10.0.1.12:6443 | The Metrics server || | | is in charge of || | | providing resource || | | usage metrics for || | | pods and nodes to || | | the API server |+----------------+------------------+----------------------+| Metrics Server | 10.0.1.11:6443 | The Metrics server || | | is in charge of || | | providing resource || | | usage metrics for || | | pods and nodes to || | | the API server |+----------------+------------------+----------------------+| Metrics Server | 10.0.1.10:6443 | The Metrics server || | | is in charge of || | | providing resource || | | usage metrics for || | | pods and nodes to || | | the API server |+----------------+------------------+----------------------+| Kubelet API | 10.200.2.1:10250 | The Kubelet is the || | | main component in || | | every Node, all pod || | | operations goes || | | through the kubelet |+----------------+------------------+----------------------+| Kubelet API | 10.0.1.22:10250 | The Kubelet is the || | | main component in || | | every Node, all pod || | | operations goes || | | through the kubelet |+----------------+------------------+----------------------+| Kubelet API | 10.0.1.21:10250 | The Kubelet is the || | | main component in || | | every Node, all pod || | | operations goes || | | through the kubelet |+----------------+------------------+----------------------+| Kubelet API | 10.0.1.20:10250 | The Kubelet is the || | | main component in || | | every Node, all pod || | | operations goes || | | through the kubelet |+----------------+------------------+----------------------+| Etcd | 10.0.1.12:2379 | Etcd is a DB that || | | stores cluster's || | | data, it contains || | | configuration and || | | current || | | state || | | information, and || | | might contain || | | secrets |+----------------+------------------+----------------------+| Etcd | 10.0.1.11:2379 | Etcd is a DB that || | | stores cluster's || | | data, it contains || | | configuration and || | | current || | | state || | | information, and || | | might contain || | | secrets |+----------------+------------------+----------------------+| Etcd | 10.0.1.10:2379 | Etcd is a DB that || | | stores cluster's || | | data, it contains || | | configuration and || | | current || | | state || | | information, and || | | might contain || | | secrets |+----------------+------------------+----------------------+| API Server | 10.32.0.1:443 | The API server is in || | | charge of all || | | operations on the || | | cluster. |+----------------+------------------+----------------------+VulnerabilitiesFor further information about a vulnerability, search its ID in:https://avd.aquasec.com/+--------+----------------------+----------------------+----------------------+----------------------+----------------------+| ID | LOCATION | MITRE CATEGORY | VULNERABILITY | DESCRIPTION | EVIDENCE |+--------+----------------------+----------------------+----------------------+----------------------+----------------------+| None | Local to Pod (kube- | Lateral Movement // | CAP_NET_RAW Enabled | CAP_NET_RAW is | || | hunter-tws6b) | ARP poisoning and IP | | enabled by default | || | | spoofing | | for pods. | || | | | | If an attacker | || | | | | manages to | || | | | | compromise a pod, | || | | | | they could | || | | | | potentially take | || | | | | advantage of this | || | | | | capability to | || | | | | perform network | || | | | | attacks on other | || | | | | pods running on the | || | | | | same node | |+--------+----------------------+----------------------+----------------------+----------------------+----------------------+| KHV002 | 10.32.0.1:443 | Initial Access // | K8s Version | The kubernetes | v1.21.0 || | | Exposed sensitive | Disclosure | version could be | || | | interfaces | | obtained from the | || | | | | /version endpoint | |+--------+----------------------+----------------------+----------------------+----------------------+----------------------+| KHV053 | Local to Pod (kube- | Discovery // | AWS Metadata | Access to the AWS | cidr: 10.0.1.0/24 || | hunter-tws6b) | Instance Metadata | Exposure | Metadata API exposes | || | | API | | information about | || | | | | the machines | || | | | | associated with the | || | | | | cluster | |+--------+----------------------+----------------------+----------------------+----------------------+----------------------+| KHV005 | 10.32.0.1:443 | Discovery // Access | Access to API using | The API Server port | b'{\"kind\":\"APIVersio || | | the K8S API Server | service account | is accessible. | ns\",\"versions\":[\"v1\" || | | | token | Depending on | ],\"serverAddressByCl || | | | | your RBAC settings | ientCIDRs\":[{\"client || | | | | this could expose | CIDR\":\"0.0.0.0/0\",\"s || | | | | access to or control | ... || | | | | of your cluster. | |+--------+----------------------+----------------------+----------------------+----------------------+----------------------+| None | Local to Pod (kube- | Credential Access // | Access to pod's | Accessing the pod's | ['/var/run/secrets/k || | hunter-tws6b) | Access container | secrets | secrets within a | ubernetes.io/service || | | service account | | compromised pod | account/namespace', || | | | | might disclose | '/var/run/secrets/ku || | | | | valuable data to a | bernetes.io/servicea || | | | | potential attacker | ... |+--------+----------------------+----------------------+----------------------+----------------------+----------------------+| KHV050 | Local to Pod (kube- | Credential Access // | Read access to pod's | Accessing the pod | eyJhbGciOiJSUzI1NiIs || | hunter-tws6b) | Access container | service account | service account | ImtpZCI6IjItYnpCa1pK || | | service account | token | token gives an | aE1pTVpXZE1qSkhnRDA5 || | | | | attacker the option | YmdMZ3BLdmNxejV4VVYw || | | | | to use the server | OW12LVEifQ.eyJhdWQiO || | | | | API | ... |+--------+----------------------+----------------------+----------------------+----------------------+----------------------+From the log output above you can see there is quite a bit of information around vulnerabilities and also some potential resolution steps. You can use this output to start patching your Kubernetes cluster to ensure you are getting the most security that you need.Cleaning UpMake sure you remove all of the resources we created, or else it will incure running costs if left running.Delete all the worker instances, then afterwards delete controller instances, also delete our key-pair:echo \"Issuing shutdown to worker nodes.. \" &amp;&amp; \\aws ec2 terminate-instances \\ --instance-ids \\ $(aws ec2 describe-instances --filters \\ \"Name=tag:Name,Values=worker-0,worker-1,worker-2\" \\ \"Name=instance-state-name,Values=running\" \\ --output text --query 'Reservations[].Instances[].InstanceId')echo \"Waiting for worker nodes to finish terminating.. \" &amp;&amp; \\aws ec2 wait instance-terminated \\ --instance-ids \\ $(aws ec2 describe-instances \\ --filter \"Name=tag:Name,Values=worker-0,worker-1,worker-2\" \\ --output text --query 'Reservations[].Instances[].InstanceId')echo \"Issuing shutdown to master nodes.. \" &amp;&amp; \\aws ec2 terminate-instances \\ --instance-ids \\ $(aws ec2 describe-instances --filter \\ \"Name=tag:Name,Values=controller-0,controller-1,controller-2\" \\ \"Name=instance-state-name,Values=running\" \\ --output text --query 'Reservations[].Instances[].InstanceId')echo \"Waiting for master nodes to finish terminating.. \" &amp;&amp; \\aws ec2 wait instance-terminated \\ --instance-ids \\ $(aws ec2 describe-instances \\ --filter \"Name=tag:Name,Values=controller-0,controller-1,controller-2\" \\ --output text --query 'Reservations[].Instances[].InstanceId')aws ec2 delete-key-pair --key-name kubernetesDelete the external load balancer and network resources (if you have lost the values of any of the environment variables, then you can use AWS commands to look them up, or you can manually set them by looking at the resources in the AWS console):aws elbv2 delete-load-balancer --load-balancer-arn \"${LOAD_BALANCER_ARN}\"aws elbv2 delete-target-group --target-group-arn \"${TARGET_GROUP_ARN}\"aws ec2 delete-security-group --group-id \"${SECURITY_GROUP_ID}\"ROUTE_TABLE_ASSOCIATION_ID=\"$(aws ec2 describe-route-tables \\ --route-table-ids \"${ROUTE_TABLE_ID}\" \\ --output text --query 'RouteTables[].Associations[].RouteTableAssociationId')\"aws ec2 disassociate-route-table --association-id \"${ROUTE_TABLE_ASSOCIATION_ID}\"aws ec2 delete-route-table --route-table-id \"${ROUTE_TABLE_ID}\"echo \"Waiting a minute for all public address(es) to be unmapped.. \" &amp;&amp; sleep 60aws ec2 detach-internet-gateway \\ --internet-gateway-id \"${INTERNET_GATEWAY_ID}\" \\ --vpc-id \"${VPC_ID}\"aws ec2 delete-internet-gateway --internet-gateway-id \"${INTERNET_GATEWAY_ID}\"aws ec2 delete-subnet --subnet-id \"${SUBNET_ID}\"aws ec2 delete-vpc --vpc-id \"${VPC_ID}\"I cannot stress this enough, go through your AWS console resources and ensure that you have removed everything we have provisioned in this guide. You don’t want to come back a year later wondering why your credit card balance is hurting!!!Thanks for reading.Medium Article: https://medium.com/geekculture/building-a-kubernetes-cluster-on-aws-from-scratch-7e1e8b0342c4" }, { "title": "How I passed the INE Certified Cloud Associate (ICCA) Exam", "url": "/posts/how-i-passed-the-ine-certified-cloud-associate-icca-exam/", "categories": "certifications", "tags": "cloud, gcp, aws, azure", "date": "2022-02-22 13:00:00 +0100", "snippet": "The ICCA certification courseware introduces you to the foundational and fundamental aspects of the cloud, and not just one specific provider but a high level approach to working with the methodology across a range of providers. Everything from provisioning cloud resources, the shared responsibility model, all the way through to security and compliance was covered by this course.The course does however focus on three of the more prominent cloud providers, that being: AWS, Microsoft Azure and Google Cloud (GCP).The learning material is extremely well structured and the instructor Tracy Wallace explains each topic carefully and is a really great speaker. The course is broken down into three sub courses namely: Cloud Foundations Cloud Management Concepts Fundamentals of Cloud Identity, Security, and ComplianceThe duration of the course is 10 hours and 13 minutes. There are questions in-between the sections which test your knowledge of the topic before moving on. There are also lab exercises for you to perform, however they do not provide the cloud accounts for you to practise these labs, you will need to sign up for a free account on each of the cloud providers and then use their free tier to perform the operations (unlike other platforms like acloud.guru, but I am sure they will add that functionality at some point in time).Once I got through all the course material I decided to look at the mentioned resources as well as going through some of the security and compliance documentation for each cloud provider on their respective web sites.I took all the questions from the course and added them to a spreadsheet and kept reviewing the notes I had taken from all the videos and worked through it 2 hours per evening for about 10 evenings until I felt confident that I had covered everything that was in the course.The date I decided to write the exam had come and I fired up the exam software and was presented with a really streamlined testing process. The exam time is set for 1 and a half hours. It was so easy to go through the questions and when I was done with the questions it was time for the hands on lab. Luckilly this time the cloud credentials are provided to you and you need to perform the hands on lab tasks. Don’t forget that when you complete the lab you will need to submit it and keep your instances running on the cloud provider so you can be graded. After the lab I went back to some of the questions I marked (about 4) for review as they were slightly tricky. When I was happy with my answers I submitted for marking and within seconds a screen popped up to say congratulations you have passed. I scored 93% !!! I got 3 questions wrong, luckily the minimum score required is 70%. It must have been those questions I reviewed :) This was a very smooth process and I had no issues.I am extremely suprised that this is the first certification from INE and that the testing framework is structured well and operates well, they have done an absolutely amazing job.Obviously I cannot go into too much detail about the questions asked but I can highlight some things you need to be aware of. For me the videos only helped with about 80% of the questions asked. The other 20% of questions suprised me a little bit, but I have been working with cloud for a while now. In addition to going through the video material I would suggest you read the high level documentation about each cloud provider (AWS, Azure, GCP) and spend quite a bit of time on their Security offerings and know which platforms adhere to certain compliance requirements. There were quite a few questions on security and compliance.For the labs, I cannot tell you exactly what tasks to perform, but I can tell you all the tasks were described in the video material accompanying the course, so practise, practise, practise.The cost of the exam is $99 and you get 1 retake (even immediate).Read up for more information about the certification here: https://info.ine.com/icca-certification/Disclaimer: I am in no way affiliated with INE at all, this was just my personal experience that I thought would be helpful to share.Medium Article: https://craignewtondev.medium.com/how-i-passed-the-ine-certified-cloud-associate-icca-exam-d46b65664a63" }, { "title": "Kubernetes on AWS — gotcha when using EFS", "url": "/posts/kubernetes-on-aws-gotcha-when-using-efs/", "categories": "kubernetes, aws", "tags": "kubernetes, aws, efs", "date": "2022-02-22 13:00:00 +0100", "snippet": "I was happily migrating a lot of applications over to EKS when my pods went into an Init state. Panic set in and I was having a look around at what the problem could be (what is it this time, I said to myself).For this specific Kubernetes cluster I was using a single EFS volume for all my deployments into quite a few separate namespaces. The problem I eventually faced was that when you create a persistent volume, it registers as an Access Point on an EFS mount … and turns out there is a hard limit of 120 Access Points for each file system. So when I hit this limit, my new pods failed to provision persistent volumes. PANIC STATIONS!The way I got around this limitation was to create additional EFS volumes and deploy multiple EFS storage classes in Kubernetes tied to these new volumes and balance out the allocation of new application deployments to those new storage classes. For now that seems to be working stable, but I will keep you posted if there are any changes or updates. The other limit on EFS to keep a keen eye on is “Number of mount targets for each VPC” which is 400.Have you encountered this issue? What do you think is the best way to solve the problem? Did this post help you in any way? Please feel free to comment below.Ensure you read and understand the limits imposed on EFS, the hard limits stop you dead in your tracks.Following are the quotas on Amazon EFS resources for each customer account in an AWS Region: Number of access points for each file system = 120 Number of connections for each file system = 25,000 Number of mount targets for each file system in an Availability Zone = 1 Number of mount targets for each VPC = 400 Number of security groups for each mount target = 5 Number of tags for each file system = 50 Number of VPCs for each file system = 1More here: https://docs.aws.amazon.com/efs/latest/ug/limits.htmlAlso ensure when you are using Amazon EFS CSI driver that the limits are also imposed here: https://docs.aws.amazon.com/eks/latest/userguide/efs-csi.html" }, { "title": "How to run a Kubernetes cron job manually as a one time job", "url": "/posts/how-to-run-a-kubernetes-cron-job-manually-as-a-one-time-job/", "categories": "kubernetes", "tags": "kubernetes, cron", "date": "2020-05-28 14:00:00 +0200", "snippet": "I recently had to create a cron job in Kubernetes that would run a data archive script that would archive the immutable MySQL table data for transactions for the previous day and upload the result to AWS S3.The job failed today as there was a network connectivity issue to the database and my code tried 3 times and then gave up for today. I then needed to have the job execute so I could have it process yesterdays data and archive it. So I did what I normally do, I take the YAML for the cronjob and modify it to become a standard type: Job and then I just apply the YAML to run the job and once completed I just run kubectl delete -f my-onetime-job.yaml.It works but seems a bit tedious. Recently I discovered how to do this without needing a YAML file, I could just run it from the command line directly.Here is the command I used:kubectl create job --from=cronjob/s3-transactions-data-archiver s3-transactions-data-archiver-otjWhere with placeholders:kubectl create job --from=cronjob/&lt;name-of-cron-job&gt; &lt;name-of-job&gt;This will create a job and run it, you can verify it’s completion by running the following command and inspecting the output:kubectl get jobsThe output will show you the jobs you currently have running and my job for s3-transactions-data-archiver-otj was listed there as completed.To cleanup after all we now need to do is delete the job by running the command:kubectl delete job s3-transactions-data-archiver-otjWhere with placeholders:kubectl delete job &lt;name of job&gt;This is a great way to quickly run a failed cron job, or even if you create your cron job and it does not immediately run, you can trigger this manual run.Hope this helps you and thank you for reading this snippet.Medium Article: https://medium.com/swlh/how-to-run-a-kubernetes-cron-job-manually-as-a-one-time-job-5622f1d6a3b0" }, { "title": "Public and Private Istio Ingress Gateways on AWS", "url": "/posts/public-and-private-istio-ingress-gateways-on-aws/", "categories": "kubernetes, aws", "tags": "kubernetes, aws, istio, gateways", "date": "2019-08-30 14:00:00 +0200", "snippet": "By default, istio creates a service with a publicly accessible classic load balancer (ELB). However, there are times where we only want access from our internal network or a network we are connected to via a secure VPN.To achieve this we need a copy of our current ingressgateway service and deployment configuration.The command below will output our current configuration to a file:kubectl get svc istio-ingressgateway -n istio-system -o yaml &gt; istio-pvt-ingressgateway.yamlOpen the file in your favourite text editor and get ready to make some changes to the file. You can remove all the auto-generated fields. What you will need to add to create an NLB is the annotation service.beta.kubernetes.io/aws-load-balancer-type: \"nlb\". If however if you prefer the classic load balancer (ELB), then just remove this annotation. The next annotation we need to add is service.beta.kubernetes.io/aws-load-balancer-internal: \"true\". This annotation will ensure that this load balancer is only available to other services within your VPC. This is great for microservices that do not need to be exposed to public traffic, or for access on your company intranet connected via VPN to your VPC.What you will need to change in the file is the following: change the name to anything you want: I changed it from istio-ingressgateway to istio-pvt-ingressgateway change the app label to anything you want: I changed it from istio-ingressgateway to istio-pvt-ingressgateway change the istio label to anything you want: I changed it from ingressgateway to pvt-ingressgateway in the selector configuration section update the app and istio label to match the values you defined in the metadata labels section remove all the nodePort values from the ports configuration so that the newly created service allocated the ports automatically now is your chance to add on any additional ports that you want to use apiVersion: v1kind: Servicemetadata: annotations: service.beta.kubernetes.io/aws-load-balancer-type: \"nlb\" service.beta.kubernetes.io/aws-load-balancer-internal: \"true\" labels: app: istio-pvt-ingressgateway chart: gateways heritage: Tiller istio: pvt-ingressgateway release: istio name: istio-pvt-ingressgateway namespace: istio-systemspec: ports: - name: status-port port: 15020 protocol: TCP targetPort: 15020 - name: http2 port: 80 protocol: TCP targetPort: 80 - name: https port: 443 protocol: TCP targetPort: 443 - name: smpp port: 2775 protocol: TCP targetPort: 2775 - name: tcp port: 31400 protocol: TCP targetPort: 31400 - name: https-kiali port: 15029 protocol: TCP targetPort: 15029 - name: https-prometheus port: 15030 protocol: TCP targetPort: 15030 - name: https-grafana port: 15031 protocol: TCP targetPort: 15031 - name: https-tracing port: 15032 protocol: TCP targetPort: 15032 - name: tls port: 15443 protocol: TCP targetPort: 15443 selector: app: istio-pvt-ingressgateway istio: pvt-ingressgateway release: istio type: LoadBalancerNow save and apply that file:kubectl apply -f istio-pvt-ingressgateway.yamlWait for your load balancer to become operational. After a few minutes, you can check the status of the load balancer and creation:kubectl get svc -n istio-system | grep istio-pvt-ingressgatewayYou should see the DNS name of your load balancer and you can log into your AWS and verify that the load balancer created is indeed an NLB where Type is network and that the Scheme is internal.Now feel free to create any domain/subdomain in Route53 and point it to your load balancer DNS as an A record alias.The next step would be to create a copy of the istio-ingressgateway deployment and change it to facilitate the new service we have created:kubectl get deployment/istio-ingressgateway -n istio-system -o yaml &gt; istio-pvt-ingressgateway-deployment.yamlOpen up the newly created file in a text editor and remove the standard fields that are automatically created (clean up the file a bit, you don’t have to). We then need to change the name to something a bit more custom: I changed mine to match the service which is name: istio-pvt-ingressgateway We need to change the app labels as well to app: istio-pvt-ingressgateway We need to change the istio labels to istio: pvt-ingressgateway Feel free to add on any additional ports you need to be defined. apiVersion: extensions/v1beta1kind: Deploymentmetadata: labels: app: istio-pvt-ingressgateway chart: gateways heritage: Tiller istio: pvt-ingressgateway release: istio name: istio-pvt-ingressgateway namespace: istio-systemspec: replicas: 1 selector: matchLabels: app: istio-pvt-ingressgateway istio: pvt-ingressgateway template: metadata: annotations: sidecar.istio.io/inject: \"false\" labels: app: istio-pvt-ingressgateway chart: gateways heritage: Tiller istio: pvt-ingressgateway release: istio spec: affinity: nodeAffinity: preferredDuringSchedulingIgnoredDuringExecution: - preference: matchExpressions: - key: beta.kubernetes.io/arch operator: In values: - amd64 weight: 2 - preference: matchExpressions: - key: beta.kubernetes.io/arch operator: In values: - ppc64le weight: 2 - preference: matchExpressions: - key: beta.kubernetes.io/arch operator: In values: - s390x weight: 2 requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: beta.kubernetes.io/arch operator: In values: - amd64 - ppc64le - s390x - key: kubelet.kubernetes.io/role operator: In values: - management containers: - args: - proxy - router - --domain - $(POD_NAMESPACE).svc.cluster.local - --log_output_level=default:info - --drainDuration - 45s - --parentShutdownDuration - 1m0s - --connectTimeout - 10s - --serviceCluster - istio-ingressgateway - --zipkinAddress - zipkin:9411 - --proxyAdminPort - \"15000\" - --statusPort - \"15020\" - --controlPlaneAuthPolicy - NONE - --discoveryAddress - istio-pilot:15010 env: - name: NODE_NAME valueFrom: fieldRef: apiVersion: v1 fieldPath: spec.nodeName - name: POD_NAME valueFrom: fieldRef: apiVersion: v1 fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: apiVersion: v1 fieldPath: metadata.namespace - name: INSTANCE_IP valueFrom: fieldRef: apiVersion: v1 fieldPath: status.podIP - name: HOST_IP valueFrom: fieldRef: apiVersion: v1 fieldPath: status.hostIP - name: ISTIO_META_POD_NAME valueFrom: fieldRef: apiVersion: v1 fieldPath: metadata.name - name: ISTIO_META_CONFIG_NAMESPACE valueFrom: fieldRef: apiVersion: v1 fieldPath: metadata.namespace - name: ISTIO_META_ROUTER_MODE value: sni-dnat image: gcr.io/istio-release/proxyv2:master-latest-daily imagePullPolicy: IfNotPresent name: istio-proxy ports: - containerPort: 15020 protocol: TCP - containerPort: 80 protocol: TCP - containerPort: 443 protocol: TCP - containerPort: 31400 protocol: TCP - containerPort: 15029 protocol: TCP - containerPort: 15030 protocol: TCP - containerPort: 15031 protocol: TCP - containerPort: 15032 protocol: TCP - containerPort: 15443 protocol: TCP - containerPort: 15090 name: http-envoy-prom protocol: TCP readinessProbe: failureThreshold: 30 httpGet: path: /healthz/ready port: 15020 scheme: HTTP initialDelaySeconds: 1 periodSeconds: 2 successThreshold: 1 timeoutSeconds: 1 resources: limits: cpu: \"2\" memory: 1Gi requests: cpu: 100m memory: 128Mi volumeMounts: - mountPath: /etc/certs name: istio-certs readOnly: true - mountPath: /etc/istio/ingressgateway-certs name: ingressgateway-certs readOnly: true - mountPath: /etc/istio/ingressgateway-ca-certs name: ingressgateway-ca-certs readOnly: true serviceAccount: istio-ingressgateway-service-account serviceAccountName: istio-ingressgateway-service-account tolerations: - effect: NoSchedule key: dedicated operator: Equal value: management volumes: - name: istio-certs secret: defaultMode: 420 optional: true secretName: istio.istio-ingressgateway-service-account - name: ingressgateway-certs secret: defaultMode: 420 optional: true secretName: istio-ingressgateway-certs - name: ingressgateway-ca-certs secret: defaultMode: 420 optional: true secretName: istio-ingressgateway-ca-certsNow we want to apply that deployment:kubectl apply -f istio-pvt-ingressgateway-deployment.yamlYou can now check to ensure that the pod is running:kubectl get pods -n istio-system | grep istio-pvt-ingressgatewayNext, create an istio gateway configuration and ensure that the selector is set to what we created earlier on in the private gateway service. In my case it was istio: pvt-ingressgateway. This selector will use that label to be used with our service configuration.apiVersion: networking.istio.io/v1alpha3kind: Gatewaymetadata: name: myapp-gatewayspec: selector: istio: pvt-ingressgateway servers: - port: number: 80 name: http protocol: HTTP hosts: - \"myapp.mydomain.com\"Apply the file:kubectl apply -f gateway.yamlNow we can just follow the normal convention of deploying the rest of the istio configuration and kubernetes configuration for our application.Bonus — rest of the configuration for an istio based applicationOnce we have a gateway configuration setup we now need a virtual service. The virtual service acts as a firewall to your application. In the config, you can define some really amazing routing rules and whitelisting/blacklisting.Here is an example config for a pretty standard HTTP application:apiVersion: networking.istio.io/v1alpha3kind: VirtualServicemetadata: name: myappspec: hosts: - \"myapp.mydomain.com\" gateways: - myapp-gateway http: - match: - authority: exact: \"myapp.mydomain.com\" route: - destination: port: number: 80 host: myappApply the file:kubectl apply -f virtualservice.yamlNext, we need a standard Kubernetes service:apiVersion: v1kind: Servicemetadata: labels: app: myapp name: myappspec: ports: - name: http port: 80 protocol: TCP targetPort: 80 selector: app: myapp type: NodePortApply the file:kubectl apply -f service.yamlAnd then we need our standard Kubernetes deployment:apiVersion: apps/v1kind: Deploymentmetadata: name: myappspec: selector: matchLabels: app: myapp replicas: 2 template: metadata: labels: app: myapp spec: containers: - name: nginx image: nginx:1.7.9 ports: - containerPort: 80Now before we apply the deployment, because we are using istio, we need to remember to inject the envoy sidecar, unless of course, you have added the auto-injection to your namespace. If not, remember to run the following:kubectl apply -f &lt;(istioctl kube-inject -f deployment.yaml)Wait for a few seconds (maybe minute or two), then try to access your service using your URL (from within your VPC, preferably on the VPN). Mine was http://myapp.mydomain.comIf all went well then you should now be able to access your service from within your network (VPC) and no prying eyes allowed from outside your network.Please leave comments if this was helpful, or if you have questions, or if you have a better way to implement this.Thank you for reading.Medium Article: https://medium.com/swlh/public-and-private-istio-ingress-gateways-on-aws-f968783d62fe" }, { "title": "How to fix — Kubernetes namespace deleting stuck in Terminating state", "url": "/posts/how-to-fix-kubernetes-namespace-deleting-stuck-in-terminating-state/", "categories": "kubernetes", "tags": "kubernetes, troubleshooting", "date": "2019-07-12 14:00:00 +0200", "snippet": "So AWS launched their hosted Kubernetes called EKS (Elastic Kubernetes Service) last year and we merrily jumped onboard and deployed most of our services in the months since then. Everything was looking great and working amazing until I started doing some cleanup on the platform. I am sure that it was possibly my fault for adding deployments, removing deployments, moving nodes around, terminating nodes, etc. over a period of time, I pretty much scrambled its marbles.I removed everything from a namespace I had created called “logging” as I was moving all the resource to a different namespace called “monitoring”. I then proceeded to delete the namespace using kubectl delete namespace logging. Everything looked great and when I listed the namespaces and it was showing the state of that namespace as Terminating. I proceeded with my day to day DevOps routine and came back later to find the namespace still stuck in the Terminating state. I ignored it and went and had a great weekend.Coming back on Monday, that same namespace was still there, stuck on Terminating. Frustrated I turned to Google and started looking if anyone had the same issue. There were quite a few results, tried most of them to no avail until I stumbled on this issue: https://github.com/kubernetes/kubernetes/issues/60807. I tried most of the recommendations also to no avail until I started mixing and matching some of the commands, and then finally, I got rid of that namespace that was stuck in the Terminating state. I know, I could have left it there and not bothered with it, but my OCD kicked in and I wanted it gone ;)So it turns out I had to remove the finalizer for kubernetes. But the catch was not to just apply the change using kubectl apply -f, it had to go via the cluster API for it to work.Here are the instructions I used to delete that namespace:Step 1: Dump the descriptor as JSON to a filekubectl get namespace logging -o json &gt; logging.jsonOpen the file for editing:{ \"apiVersion\": \"v1\", \"kind\": \"Namespace\", \"metadata\": { \"creationTimestamp\": \"2019-05-14T13:55:20Z\", \"labels\": { \"name\": \"logging\" }, \"name\": \"logging\", \"resourceVersion\": \"29571918\", \"selfLink\": \"/api/v1/namespaces/logging\", \"uid\": \"e9516a8b-764f-11e9-9621-0a9c41ba9af6\" }, \"spec\": { \"finalizers\": [ \"kubernetes\" ] }, \"status\": { \"phase\": \"Terminating\" }}Remove kubernetes from the finalizers array:{ \"apiVersion\": \"v1\", \"kind\": \"Namespace\", \"metadata\": { \"creationTimestamp\": \"2019-05-14T13:55:20Z\", \"labels\": { \"name\": \"logging\" }, \"name\": \"logging\", \"resourceVersion\": \"29571918\", \"selfLink\": \"/api/v1/namespaces/logging\", \"uid\": \"e9516a8b-764f-11e9-9621-0a9c41ba9af6\" }, \"spec\": { \"finalizers\": [ ] }, \"status\": { \"phase\": \"Terminating\" }}Step 2: Executing our cleanup commandNow that we have that setup we can instruct our cluster to get rid of that annoying namespace:kubectl replace --raw \"/api/v1/namespaces/logging/finalize\" -f ./logging.jsonWhere: /api/v1/namespaces/&lt;your_namespace_here&gt;/finalizeAfter running that command, the namespace should now be absent from your namespaces list.The key thing to note here is the resource you are modifying, in our case, it is for namespaces, it could be pods, deployments, services, etc. This same method can be applied to those resources stuck in Terminating state." }, { "title": "Kubernetes with Minikube and switching Docker environments", "url": "/posts/kubernetes-with-minikube-and-switching-docker-environments/", "categories": "kubernetes, docker", "tags": "kubernetes, minikube, docker", "date": "2019-06-08 16:00:00 +0200", "snippet": "The popularity of Kubernetes as a container orchestration service has been growing at an amazing rate. Developers are flocking to the platform and taking advantage of the increase in the speed of their development deployments.Minikube is a tool that helps you run Kubernetes locally. It runs as a single-node Kubernetes cluster on your machine so you can use it to run your software which will use the same/or similar configuration to run in production.With Kubernetes you run Docker containers within your pods. These containers are started by running Docker images. These images can be pulled from public repositories. But what happens when you are busy developing your software locally and need quicker iterations and not have to push the image changes to a public repository so that it can be pulled from that public repository just to start up your container on a Kubernetes pod running locally.Because Minikube runs in a VM on your machine, it has Docker and a local repository installed. So all you really need to do is build your docker images locally and tag them inside of the Minikube VM.Minikube has no way to access your local Docker environment or repository. Luckily there is an easy way to switch context between your Minikube and local environments.You can find out more information about installing and starting Minikube here.To switch your Docker context to using Minikube you run the following command from your terminal:#!/usr/bin/env basheval $(minikube docker-env)Now all your Docker commands will be executed in the Minikube VM. Now all you need to do is build your image and tag it to be available when deploying to your Minikube Kubernetes instance.#!/usr/bin/env bash# Build docker image with no-cache flagdocker build --no-cache -t my-super-app .# Tag it as a versiondocker tag my-super-app:latest my-super-app:1.0.0Now you can deploy the image to your local Minikube environment.Add the following to a file called deployment-my-super-app.yaml:apiVersion: apps/v1kind: Deploymentmetadata: name: my-super-appspec: selector: matchLabels: app: my-super-app template: metadata: labels: app: my-super-app spec: containers: - name: app image: my-super-app:1.0.0 imagePullPolicy: Never ports: - containerPort: 80#!/usr/bin/env bashkubectl apply -f ./deployment-my-super-app.yamlThe important thing here is that you set the imagePullPolicy: Never ; This assumes that the image is available locally and will make no attempt to pull the image from a remote location.Your pod should be running happily. When you make a code change and are ready to test out the changes in the container, you can just rebuild the image using the Docker build with no cache option. Then you can either just delete the deployment and apply it again; or just delete the pod so it will create a new one for you; or you can scale the deployment to 0 and then back up to 1, or your desired amount of replicas.#!/usr/bin/env bashkubectl scale --replicas=0 deployment/my-super-appkubectl scale --replicas=1 deployment/my-super-appOnce you are done with your development and you would like to switch back to your original context, you can do this by running the following:#!/usr/bin/env basheval $(docker-machine env -u)You local Docker context would now be restored and you can happily run your local docker images again.BonusMinikube can be quite resource intensive on your machine. You can tweak the resource usage settings in VirtualBox, or you can use some simple commands to tweak your environment. Sometimes you might want to dump everything from your Minikube environment and start over. Resetting your Minikube environment and changing resources can be done with the following commands:#!/usr/bin/env bashset -exminikube config set cpus 4minikube config set memory 4096minikube config viewminikube delete || trueminikube start --vm-driver ${1-\"virtualbox\"}Medium Article: https://craignewtondev.medium.com/kubernetes-with-minikube-and-switching-docker-environments-4d965d00eb26" }, { "title": "How to assign an EIP to an AWS NLB in Kubernetes", "url": "/posts/how-to-assign-an-eip-to-an-aws-nlb-in-kubernetes/", "categories": "kubernetes, aws", "tags": "kubernetes, aws, eip, nlb", "date": "2019-06-08 14:00:00 +0200", "snippet": "Out of the box Kubernetes has no support for assigning an EIP to an AWS Network Load Balancer. There is currently a pull request for this feature listed here: https://github.com/kubernetes/kubernetes/issues/63959#issuecomment-484203661This feature request has unfortunately not been approved yet.Not to worry though I have put together a simple application you can run as a CronJob on your current Kubernetes cluster that will clone the targets from your Kubernetes Service LoadBalancer to an AWS Network Load Balancer that you create and have an EIP assigned to.Docker hub repository: https://hub.docker.com/r/newtondev/k8s-aws-nlb-target-clonerSource code repository: https://github.com/newtondev/k8s-aws-nlb-target-clonerMedium Article: https://craignewtondev.medium.com/how-to-assign-an-eip-to-an-aws-nlb-in-kubernetes-17c2b5894728" } ]
